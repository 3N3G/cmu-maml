{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vBTJxs8IYd3",
        "outputId": "48f7fd33-d641-485a-e553-f4e2cd5a823c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HowToTrainYourMAMLPytorch'...\n",
            "remote: Enumerating objects: 36634, done.\u001b[K\n",
            "remote: Counting objects: 100% (261/261), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 36634 (delta 159), reused 210 (delta 129), pack-reused 36373 (from 1)\u001b[K\n",
            "Receiving objects: 100% (36634/36634), 18.95 MiB | 12.24 MiB/s, done.\n",
            "Resolving deltas: 100% (2204/2204), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JqpuygWUiM_i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# from meta_neural_network_architectures import VGGReLUNormNetwork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KzOo4o8Tiwhg"
      },
      "outputs": [],
      "source": [
        "def set_torch_seed(seed):\n",
        "    \"\"\"\n",
        "    Sets the pytorch seeds for current experiment run\n",
        "    :param seed: The seed (int)\n",
        "    :return: A random number generator to use\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed=seed)\n",
        "    torch_seed = rng.randint(0, 999999)\n",
        "    torch.manual_seed(seed=torch_seed)\n",
        "\n",
        "    return rng"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get mini-imagenet"
      ],
      "metadata": {
        "id": "Iq_IztyOad4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "!kaggle datasets download zcyzhchyu/mini-imagenet"
      ],
      "metadata": {
        "id": "RFIFtdihZNL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774225af-5b9d-4992-f72c-a1672919e7a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/zcyzhchyu/mini-imagenet\n",
            "License(s): CC0-1.0\n",
            "Downloading mini-imagenet.zip to /content\n",
            "100% 2.85G/2.86G [00:36<00:00, 119MB/s]\n",
            "100% 2.86G/2.86G [00:36<00:00, 84.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/mini-imagenet.zip -d /content/HowToTrainYourMAMLPytorch/"
      ],
      "metadata": {
        "id": "3CN6EZnvZbbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size\n",
        "!mv /content/HowToTrainYourMAMLPytorch/images.tar /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size\n",
        "!mv /content/HowToTrainYourMAMLPytorch/train.csv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size\n",
        "!mv /content/HowToTrainYourMAMLPytorch/val.csv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size\n",
        "!mv /content/HowToTrainYourMAMLPytorch/test.csv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size"
      ],
      "metadata": {
        "id": "_gJBmd0UhK3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images/\n",
        "!mv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images.tar /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images/\n",
        "!tar -xvf /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images/images.tar -C /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images/"
      ],
      "metadata": {
        "id": "3skC8Ev8ePob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6521d1-53a6-4479-c46d-67640682aed8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "./n0679411000000879.jpg\n",
            "./n0679411000000880.jpg\n",
            "./n0679411000000881.jpg\n",
            "./n0679411000000882.jpg\n",
            "./n0679411000000885.jpg\n",
            "./n0679411000000894.jpg\n",
            "./n0679411000000895.jpg\n",
            "./n0679411000000897.jpg\n",
            "./n0679411000000898.jpg\n",
            "./n0679411000000899.jpg\n",
            "./n0679411000000900.jpg\n",
            "./n0679411000000901.jpg\n",
            "./n0679411000000902.jpg\n",
            "./n0679411000000904.jpg\n",
            "./n0679411000000905.jpg\n",
            "./n0679411000000909.jpg\n",
            "./n0679411000000912.jpg\n",
            "./n0679411000000913.jpg\n",
            "./n0679411000000916.jpg\n",
            "./n0679411000000918.jpg\n",
            "./n0679411000000919.jpg\n",
            "./n0679411000000923.jpg\n",
            "./n0679411000000926.jpg\n",
            "./n0679411000000929.jpg\n",
            "./n0679411000000931.jpg\n",
            "./n0679411000000932.jpg\n",
            "./n0679411000000933.jpg\n",
            "./n0679411000000935.jpg\n",
            "./n0679411000000936.jpg\n",
            "./n0679411000000938.jpg\n",
            "./n0679411000000939.jpg\n",
            "./n0679411000000940.jpg\n",
            "./n0679411000000941.jpg\n",
            "./n0679411000000942.jpg\n",
            "./n0679411000000944.jpg\n",
            "./n0679411000000948.jpg\n",
            "./n0679411000000949.jpg\n",
            "./n0679411000000950.jpg\n",
            "./n0679411000000955.jpg\n",
            "./n0679411000000963.jpg\n",
            "./n0679411000000964.jpg\n",
            "./n0679411000000966.jpg\n",
            "./n0679411000000967.jpg\n",
            "./n0679411000000969.jpg\n",
            "./n0679411000000972.jpg\n",
            "./n0679411000000976.jpg\n",
            "./n0679411000000978.jpg\n",
            "./n0679411000000981.jpg\n",
            "./n0679411000000983.jpg\n",
            "./n0679411000000985.jpg\n",
            "./n0679411000000989.jpg\n",
            "./n0679411000000992.jpg\n",
            "./n0679411000000993.jpg\n",
            "./n0679411000000995.jpg\n",
            "./n0679411000001000.jpg\n",
            "./n0679411000001003.jpg\n",
            "./n0679411000001005.jpg\n",
            "./n0679411000001007.jpg\n",
            "./n0679411000001008.jpg\n",
            "./n0679411000001010.jpg\n",
            "./n0679411000001011.jpg\n",
            "./n0679411000001015.jpg\n",
            "./n0679411000001017.jpg\n",
            "./n0679411000001020.jpg\n",
            "./n0679411000001031.jpg\n",
            "./n0679411000001032.jpg\n",
            "./n0679411000001035.jpg\n",
            "./n0679411000001036.jpg\n",
            "./n0679411000001037.jpg\n",
            "./n0679411000001038.jpg\n",
            "./n0679411000001039.jpg\n",
            "./n0679411000001040.jpg\n",
            "./n0679411000001044.jpg\n",
            "./n0679411000001050.jpg\n",
            "./n0679411000001051.jpg\n",
            "./n0679411000001052.jpg\n",
            "./n0679411000001053.jpg\n",
            "./n0679411000001055.jpg\n",
            "./n0679411000001061.jpg\n",
            "./n0679411000001064.jpg\n",
            "./n0679411000001065.jpg\n",
            "./n0679411000001067.jpg\n",
            "./n0679411000001068.jpg\n",
            "./n0679411000001069.jpg\n",
            "./n0679411000001071.jpg\n",
            "./n0679411000001073.jpg\n",
            "./n0679411000001074.jpg\n",
            "./n0679411000001077.jpg\n",
            "./n0679411000001078.jpg\n",
            "./n0679411000001079.jpg\n",
            "./n0679411000001080.jpg\n",
            "./n0679411000001082.jpg\n",
            "./n0679411000001083.jpg\n",
            "./n0679411000001085.jpg\n",
            "./n0679411000001087.jpg\n",
            "./n0679411000001090.jpg\n",
            "./n0679411000001092.jpg\n",
            "./n0679411000001093.jpg\n",
            "./n0679411000001094.jpg\n",
            "./n0679411000001095.jpg\n",
            "./n0679411000001096.jpg\n",
            "./n0679411000001099.jpg\n",
            "./n0679411000001102.jpg\n",
            "./n0679411000001103.jpg\n",
            "./n0679411000001105.jpg\n",
            "./n0679411000001107.jpg\n",
            "./n0679411000001108.jpg\n",
            "./n0679411000001109.jpg\n",
            "./n0679411000001110.jpg\n",
            "./n0679411000001111.jpg\n",
            "./n0679411000001113.jpg\n",
            "./n0679411000001116.jpg\n",
            "./n0679411000001121.jpg\n",
            "./n0679411000001132.jpg\n",
            "./n0679411000001134.jpg\n",
            "./n0679411000001137.jpg\n",
            "./n0679411000001138.jpg\n",
            "./n0679411000001140.jpg\n",
            "./n0679411000001144.jpg\n",
            "./n0679411000001145.jpg\n",
            "./n0679411000001146.jpg\n",
            "./n0679411000001148.jpg\n",
            "./n0679411000001149.jpg\n",
            "./n0679411000001151.jpg\n",
            "./n0679411000001153.jpg\n",
            "./n0679411000001155.jpg\n",
            "./n0679411000001158.jpg\n",
            "./n0679411000001159.jpg\n",
            "./n0679411000001161.jpg\n",
            "./n0679411000001164.jpg\n",
            "./n0679411000001167.jpg\n",
            "./n0679411000001169.jpg\n",
            "./n0679411000001170.jpg\n",
            "./n0679411000001171.jpg\n",
            "./n0679411000001173.jpg\n",
            "./n0679411000001175.jpg\n",
            "./n0679411000001176.jpg\n",
            "./n0679411000001177.jpg\n",
            "./n0679411000001180.jpg\n",
            "./n0679411000001185.jpg\n",
            "./n0679411000001186.jpg\n",
            "./n0679411000001188.jpg\n",
            "./n0679411000001191.jpg\n",
            "./n0679411000001194.jpg\n",
            "./n0679411000001195.jpg\n",
            "./n0679411000001197.jpg\n",
            "./n0679411000001198.jpg\n",
            "./n0679411000001199.jpg\n",
            "./n0679411000001201.jpg\n",
            "./n0679411000001202.jpg\n",
            "./n0679411000001203.jpg\n",
            "./n0679411000001205.jpg\n",
            "./n0679411000001206.jpg\n",
            "./n0679411000001208.jpg\n",
            "./n0679411000001209.jpg\n",
            "./n0679411000001211.jpg\n",
            "./n0679411000001212.jpg\n",
            "./n0679411000001215.jpg\n",
            "./n0679411000001218.jpg\n",
            "./n0679411000001219.jpg\n",
            "./n0679411000001220.jpg\n",
            "./n0679411000001221.jpg\n",
            "./n0679411000001222.jpg\n",
            "./n0679411000001223.jpg\n",
            "./n0679411000001224.jpg\n",
            "./n0679411000001225.jpg\n",
            "./n0679411000001226.jpg\n",
            "./n0679411000001227.jpg\n",
            "./n0679411000001234.jpg\n",
            "./n0679411000001238.jpg\n",
            "./n0679411000001239.jpg\n",
            "./n0679411000001242.jpg\n",
            "./n0679411000001243.jpg\n",
            "./n0679411000001245.jpg\n",
            "./n0679411000001247.jpg\n",
            "./n0679411000001250.jpg\n",
            "./n0679411000001254.jpg\n",
            "./n0679411000001256.jpg\n",
            "./n0679411000001257.jpg\n",
            "./n0679411000001262.jpg\n",
            "./n0679411000001264.jpg\n",
            "./n0679411000001265.jpg\n",
            "./n0679411000001266.jpg\n",
            "./n0679411000001267.jpg\n",
            "./n0679411000001268.jpg\n",
            "./n0679411000001272.jpg\n",
            "./n0679411000001276.jpg\n",
            "./n0679411000001278.jpg\n",
            "./n0679411000001282.jpg\n",
            "./n0679411000001284.jpg\n",
            "./n0679411000001286.jpg\n",
            "./n0679411000001288.jpg\n",
            "./n0679411000001289.jpg\n",
            "./n0679411000001291.jpg\n",
            "./n0679411000001294.jpg\n",
            "./n0679411000001295.jpg\n",
            "./n0679411000001296.jpg\n",
            "./n0679411000001297.jpg\n",
            "./n0679411000001299.jpg\n",
            "./n0679411000001300.jpg\n",
            "./n0758411000000003.jpg\n",
            "./n0758411000000004.jpg\n",
            "./n0758411000000007.jpg\n",
            "./n0758411000000010.jpg\n",
            "./n0758411000000013.jpg\n",
            "./n0758411000000015.jpg\n",
            "./n0758411000000021.jpg\n",
            "./n0758411000000022.jpg\n",
            "./n0758411000000023.jpg\n",
            "./n0758411000000025.jpg\n",
            "./n0758411000000026.jpg\n",
            "./n0758411000000029.jpg\n",
            "./n0758411000000030.jpg\n",
            "./n0758411000000031.jpg\n",
            "./n0758411000000034.jpg\n",
            "./n0758411000000035.jpg\n",
            "./n0758411000000038.jpg\n",
            "./n0758411000000045.jpg\n",
            "./n0758411000000046.jpg\n",
            "./n0758411000000048.jpg\n",
            "./n0758411000000051.jpg\n",
            "./n0758411000000052.jpg\n",
            "./n0758411000000054.jpg\n",
            "./n0758411000000055.jpg\n",
            "./n0758411000000059.jpg\n",
            "./n0758411000000061.jpg\n",
            "./n0758411000000066.jpg\n",
            "./n0758411000000067.jpg\n",
            "./n0758411000000071.jpg\n",
            "./n0758411000000072.jpg\n",
            "./n0758411000000076.jpg\n",
            "./n0758411000000079.jpg\n",
            "./n0758411000000080.jpg\n",
            "./n0758411000000081.jpg\n",
            "./n0758411000000086.jpg\n",
            "./n0758411000000087.jpg\n",
            "./n0758411000000088.jpg\n",
            "./n0758411000000089.jpg\n",
            "./n0758411000000090.jpg\n",
            "./n0758411000000098.jpg\n",
            "./n0758411000000102.jpg\n",
            "./n0758411000000103.jpg\n",
            "./n0758411000000104.jpg\n",
            "./n0758411000000105.jpg\n",
            "./n0758411000000108.jpg\n",
            "./n0758411000000109.jpg\n",
            "./n0758411000000110.jpg\n",
            "./n0758411000000112.jpg\n",
            "./n0758411000000115.jpg\n",
            "./n0758411000000118.jpg\n",
            "./n0758411000000120.jpg\n",
            "./n0758411000000122.jpg\n",
            "./n0758411000000123.jpg\n",
            "./n0758411000000124.jpg\n",
            "./n0758411000000125.jpg\n",
            "./n0758411000000128.jpg\n",
            "./n0758411000000130.jpg\n",
            "./n0758411000000131.jpg\n",
            "./n0758411000000133.jpg\n",
            "./n0758411000000136.jpg\n",
            "./n0758411000000137.jpg\n",
            "./n0758411000000139.jpg\n",
            "./n0758411000000140.jpg\n",
            "./n0758411000000143.jpg\n",
            "./n0758411000000144.jpg\n",
            "./n0758411000000145.jpg\n",
            "./n0758411000000146.jpg\n",
            "./n0758411000000147.jpg\n",
            "./n0758411000000148.jpg\n",
            "./n0758411000000149.jpg\n",
            "./n0758411000000150.jpg\n",
            "./n0758411000000151.jpg\n",
            "./n0758411000000152.jpg\n",
            "./n0758411000000154.jpg\n",
            "./n0758411000000159.jpg\n",
            "./n0758411000000161.jpg\n",
            "./n0758411000000162.jpg\n",
            "./n0758411000000168.jpg\n",
            "./n0758411000000169.jpg\n",
            "./n0758411000000170.jpg\n",
            "./n0758411000000171.jpg\n",
            "./n0758411000000172.jpg\n",
            "./n0758411000000174.jpg\n",
            "./n0758411000000177.jpg\n",
            "./n0758411000000180.jpg\n",
            "./n0758411000000181.jpg\n",
            "./n0758411000000182.jpg\n",
            "./n0758411000000183.jpg\n",
            "./n0758411000000184.jpg\n",
            "./n0758411000000186.jpg\n",
            "./n0758411000000187.jpg\n",
            "./n0758411000000190.jpg\n",
            "./n0758411000000191.jpg\n",
            "./n0758411000000193.jpg\n",
            "./n0758411000000198.jpg\n",
            "./n0758411000000205.jpg\n",
            "./n0758411000000206.jpg\n",
            "./n0758411000000208.jpg\n",
            "./n0758411000000212.jpg\n",
            "./n0758411000000213.jpg\n",
            "./n0758411000000214.jpg\n",
            "./n0758411000000215.jpg\n",
            "./n0758411000000216.jpg\n",
            "./n0758411000000220.jpg\n",
            "./n0758411000000223.jpg\n",
            "./n0758411000000225.jpg\n",
            "./n0758411000000226.jpg\n",
            "./n0758411000000229.jpg\n",
            "./n0758411000000232.jpg\n",
            "./n0758411000000234.jpg\n",
            "./n0758411000000235.jpg\n",
            "./n0758411000000237.jpg\n",
            "./n0758411000000238.jpg\n",
            "./n0758411000000239.jpg\n",
            "./n0758411000000243.jpg\n",
            "./n0758411000000244.jpg\n",
            "./n0758411000000247.jpg\n",
            "./n0758411000000250.jpg\n",
            "./n0758411000000252.jpg\n",
            "./n0758411000000254.jpg\n",
            "./n0758411000000258.jpg\n",
            "./n0758411000000262.jpg\n",
            "./n0758411000000268.jpg\n",
            "./n0758411000000269.jpg\n",
            "./n0758411000000270.jpg\n",
            "./n0758411000000271.jpg\n",
            "./n0758411000000272.jpg\n",
            "./n0758411000000276.jpg\n",
            "./n0758411000000277.jpg\n",
            "./n0758411000000278.jpg\n",
            "./n0758411000000280.jpg\n",
            "./n0758411000000282.jpg\n",
            "./n0758411000000283.jpg\n",
            "./n0758411000000284.jpg\n",
            "./n0758411000000285.jpg\n",
            "./n0758411000000287.jpg\n",
            "./n0758411000000289.jpg\n",
            "./n0758411000000290.jpg\n",
            "./n0758411000000292.jpg\n",
            "./n0758411000000293.jpg\n",
            "./n0758411000000294.jpg\n",
            "./n0758411000000295.jpg\n",
            "./n0758411000000299.jpg\n",
            "./n0758411000000301.jpg\n",
            "./n0758411000000306.jpg\n",
            "./n0758411000000307.jpg\n",
            "./n0758411000000309.jpg\n",
            "./n0758411000000313.jpg\n",
            "./n0758411000000314.jpg\n",
            "./n0758411000000315.jpg\n",
            "./n0758411000000316.jpg\n",
            "./n0758411000000317.jpg\n",
            "./n0758411000000319.jpg\n",
            "./n0758411000000323.jpg\n",
            "./n0758411000000325.jpg\n",
            "./n0758411000000327.jpg\n",
            "./n0758411000000328.jpg\n",
            "./n0758411000000333.jpg\n",
            "./n0758411000000334.jpg\n",
            "./n0758411000000335.jpg\n",
            "./n0758411000000339.jpg\n",
            "./n0758411000000340.jpg\n",
            "./n0758411000000343.jpg\n",
            "./n0758411000000344.jpg\n",
            "./n0758411000000350.jpg\n",
            "./n0758411000000351.jpg\n",
            "./n0758411000000356.jpg\n",
            "./n0758411000000357.jpg\n",
            "./n0758411000000358.jpg\n",
            "./n0758411000000359.jpg\n",
            "./n0758411000000360.jpg\n",
            "./n0758411000000361.jpg\n",
            "./n0758411000000363.jpg\n",
            "./n0758411000000364.jpg\n",
            "./n0758411000000368.jpg\n",
            "./n0758411000000369.jpg\n",
            "./n0758411000000370.jpg\n",
            "./n0758411000000374.jpg\n",
            "./n0758411000000377.jpg\n",
            "./n0758411000000378.jpg\n",
            "./n0758411000000381.jpg\n",
            "./n0758411000000382.jpg\n",
            "./n0758411000000384.jpg\n",
            "./n0758411000000386.jpg\n",
            "./n0758411000000387.jpg\n",
            "./n0758411000000390.jpg\n",
            "./n0758411000000393.jpg\n",
            "./n0758411000000397.jpg\n",
            "./n0758411000000399.jpg\n",
            "./n0758411000000400.jpg\n",
            "./n0758411000000403.jpg\n",
            "./n0758411000000407.jpg\n",
            "./n0758411000000409.jpg\n",
            "./n0758411000000410.jpg\n",
            "./n0758411000000411.jpg\n",
            "./n0758411000000412.jpg\n",
            "./n0758411000000413.jpg\n",
            "./n0758411000000414.jpg\n",
            "./n0758411000000416.jpg\n",
            "./n0758411000000417.jpg\n",
            "./n0758411000000424.jpg\n",
            "./n0758411000000425.jpg\n",
            "./n0758411000000427.jpg\n",
            "./n0758411000000435.jpg\n",
            "./n0758411000000436.jpg\n",
            "./n0758411000000437.jpg\n",
            "./n0758411000000439.jpg\n",
            "./n0758411000000441.jpg\n",
            "./n0758411000000442.jpg\n",
            "./n0758411000000444.jpg\n",
            "./n0758411000000447.jpg\n",
            "./n0758411000000451.jpg\n",
            "./n0758411000000452.jpg\n",
            "./n0758411000000453.jpg\n",
            "./n0758411000000455.jpg\n",
            "./n0758411000000456.jpg\n",
            "./n0758411000000458.jpg\n",
            "./n0758411000000462.jpg\n",
            "./n0758411000000464.jpg\n",
            "./n0758411000000465.jpg\n",
            "./n0758411000000466.jpg\n",
            "./n0758411000000467.jpg\n",
            "./n0758411000000468.jpg\n",
            "./n0758411000000469.jpg\n",
            "./n0758411000000471.jpg\n",
            "./n0758411000000472.jpg\n",
            "./n0758411000000473.jpg\n",
            "./n0758411000000474.jpg\n",
            "./n0758411000000475.jpg\n",
            "./n0758411000000477.jpg\n",
            "./n0758411000000479.jpg\n",
            "./n0758411000000483.jpg\n",
            "./n0758411000000484.jpg\n",
            "./n0758411000000490.jpg\n",
            "./n0758411000000492.jpg\n",
            "./n0758411000000494.jpg\n",
            "./n0758411000000497.jpg\n",
            "./n0758411000000499.jpg\n",
            "./n0758411000000503.jpg\n",
            "./n0758411000000504.jpg\n",
            "./n0758411000000508.jpg\n",
            "./n0758411000000514.jpg\n",
            "./n0758411000000517.jpg\n",
            "./n0758411000000519.jpg\n",
            "./n0758411000000522.jpg\n",
            "./n0758411000000523.jpg\n",
            "./n0758411000000525.jpg\n",
            "./n0758411000000526.jpg\n",
            "./n0758411000000527.jpg\n",
            "./n0758411000000529.jpg\n",
            "./n0758411000000533.jpg\n",
            "./n0758411000000535.jpg\n",
            "./n0758411000000536.jpg\n",
            "./n0758411000000539.jpg\n",
            "./n0758411000000542.jpg\n",
            "./n0758411000000543.jpg\n",
            "./n0758411000000545.jpg\n",
            "./n0758411000000546.jpg\n",
            "./n0758411000000549.jpg\n",
            "./n0758411000000553.jpg\n",
            "./n0758411000000557.jpg\n",
            "./n0758411000000559.jpg\n",
            "./n0758411000000560.jpg\n",
            "./n0758411000000561.jpg\n",
            "./n0758411000000564.jpg\n",
            "./n0758411000000566.jpg\n",
            "./n0758411000000567.jpg\n",
            "./n0758411000000568.jpg\n",
            "./n0758411000000573.jpg\n",
            "./n0758411000000574.jpg\n",
            "./n0758411000000575.jpg\n",
            "./n0758411000000577.jpg\n",
            "./n0758411000000578.jpg\n",
            "./n0758411000000581.jpg\n",
            "./n0758411000000585.jpg\n",
            "./n0758411000000588.jpg\n",
            "./n0758411000000590.jpg\n",
            "./n0758411000000592.jpg\n",
            "./n0758411000000594.jpg\n",
            "./n0758411000000595.jpg\n",
            "./n0758411000000597.jpg\n",
            "./n0758411000000598.jpg\n",
            "./n0758411000000599.jpg\n",
            "./n0758411000000600.jpg\n",
            "./n0758411000000601.jpg\n",
            "./n0758411000000602.jpg\n",
            "./n0758411000000605.jpg\n",
            "./n0758411000000609.jpg\n",
            "./n0758411000000610.jpg\n",
            "./n0758411000000612.jpg\n",
            "./n0758411000000614.jpg\n",
            "./n0758411000000615.jpg\n",
            "./n0758411000000618.jpg\n",
            "./n0758411000000620.jpg\n",
            "./n0758411000000622.jpg\n",
            "./n0758411000000624.jpg\n",
            "./n0758411000000626.jpg\n",
            "./n0758411000000627.jpg\n",
            "./n0758411000000628.jpg\n",
            "./n0758411000000631.jpg\n",
            "./n0758411000000632.jpg\n",
            "./n0758411000000633.jpg\n",
            "./n0758411000000635.jpg\n",
            "./n0758411000000636.jpg\n",
            "./n0758411000000639.jpg\n",
            "./n0758411000000642.jpg\n",
            "./n0758411000000645.jpg\n",
            "./n0758411000000646.jpg\n",
            "./n0758411000000648.jpg\n",
            "./n0758411000000649.jpg\n",
            "./n0758411000000650.jpg\n",
            "./n0758411000000651.jpg\n",
            "./n0758411000000652.jpg\n",
            "./n0758411000000653.jpg\n",
            "./n0758411000000654.jpg\n",
            "./n0758411000000655.jpg\n",
            "./n0758411000000656.jpg\n",
            "./n0758411000000658.jpg\n",
            "./n0758411000000660.jpg\n",
            "./n0758411000000661.jpg\n",
            "./n0758411000000662.jpg\n",
            "./n0758411000000666.jpg\n",
            "./n0758411000000670.jpg\n",
            "./n0758411000000672.jpg\n",
            "./n0758411000000675.jpg\n",
            "./n0758411000000681.jpg\n",
            "./n0758411000000683.jpg\n",
            "./n0758411000000684.jpg\n",
            "./n0758411000000686.jpg\n",
            "./n0758411000000693.jpg\n",
            "./n0758411000000694.jpg\n",
            "./n0758411000000695.jpg\n",
            "./n0758411000000696.jpg\n",
            "./n0758411000000698.jpg\n",
            "./n0758411000000699.jpg\n",
            "./n0758411000000701.jpg\n",
            "./n0758411000000704.jpg\n",
            "./n0758411000000706.jpg\n",
            "./n0758411000000707.jpg\n",
            "./n0758411000000709.jpg\n",
            "./n0758411000000710.jpg\n",
            "./n0758411000000711.jpg\n",
            "./n0758411000000712.jpg\n",
            "./n0758411000000713.jpg\n",
            "./n0758411000000715.jpg\n",
            "./n0758411000000716.jpg\n",
            "./n0758411000000717.jpg\n",
            "./n0758411000000720.jpg\n",
            "./n0758411000000722.jpg\n",
            "./n0758411000000723.jpg\n",
            "./n0758411000000724.jpg\n",
            "./n0758411000000727.jpg\n",
            "./n0758411000000728.jpg\n",
            "./n0758411000000731.jpg\n",
            "./n0758411000000732.jpg\n",
            "./n0758411000000734.jpg\n",
            "./n0758411000000737.jpg\n",
            "./n0758411000000738.jpg\n",
            "./n0758411000000739.jpg\n",
            "./n0758411000000741.jpg\n",
            "./n0758411000000742.jpg\n",
            "./n0758411000000743.jpg\n",
            "./n0758411000000748.jpg\n",
            "./n0758411000000750.jpg\n",
            "./n0758411000000751.jpg\n",
            "./n0758411000000756.jpg\n",
            "./n0758411000000759.jpg\n",
            "./n0758411000000761.jpg\n",
            "./n0758411000000763.jpg\n",
            "./n0758411000000764.jpg\n",
            "./n0758411000000765.jpg\n",
            "./n0758411000000767.jpg\n",
            "./n0758411000000768.jpg\n",
            "./n0758411000000769.jpg\n",
            "./n0758411000000770.jpg\n",
            "./n0758411000000771.jpg\n",
            "./n0758411000000773.jpg\n",
            "./n0758411000000777.jpg\n",
            "./n0758411000000780.jpg\n",
            "./n0758411000000781.jpg\n",
            "./n0758411000000785.jpg\n",
            "./n0758411000000786.jpg\n",
            "./n0758411000000788.jpg\n",
            "./n0758411000000790.jpg\n",
            "./n0758411000000791.jpg\n",
            "./n0758411000000792.jpg\n",
            "./n0758411000000794.jpg\n",
            "./n0758411000000800.jpg\n",
            "./n0758411000000801.jpg\n",
            "./n0758411000000806.jpg\n",
            "./n0758411000000807.jpg\n",
            "./n0758411000000808.jpg\n",
            "./n0758411000000809.jpg\n",
            "./n0758411000000811.jpg\n",
            "./n0758411000000813.jpg\n",
            "./n0758411000000814.jpg\n",
            "./n0758411000000815.jpg\n",
            "./n0758411000000819.jpg\n",
            "./n0758411000000825.jpg\n",
            "./n0758411000000826.jpg\n",
            "./n0758411000000828.jpg\n",
            "./n0758411000000829.jpg\n",
            "./n0758411000000830.jpg\n",
            "./n0758411000000831.jpg\n",
            "./n0758411000000834.jpg\n",
            "./n0758411000000835.jpg\n",
            "./n0758411000000836.jpg\n",
            "./n0758411000000840.jpg\n",
            "./n0758411000000843.jpg\n",
            "./n0758411000000844.jpg\n",
            "./n0758411000000845.jpg\n",
            "./n0758411000000850.jpg\n",
            "./n0758411000000852.jpg\n",
            "./n0758411000000855.jpg\n",
            "./n0758411000000856.jpg\n",
            "./n0758411000000858.jpg\n",
            "./n0758411000000859.jpg\n",
            "./n0758411000000860.jpg\n",
            "./n0758411000000861.jpg\n",
            "./n0758411000000862.jpg\n",
            "./n0758411000000863.jpg\n",
            "./n0758411000000867.jpg\n",
            "./n0758411000000868.jpg\n",
            "./n0758411000000874.jpg\n",
            "./n0758411000000875.jpg\n",
            "./n0758411000000877.jpg\n",
            "./n0758411000000878.jpg\n",
            "./n0758411000000879.jpg\n",
            "./n0758411000000880.jpg\n",
            "./n0758411000000884.jpg\n",
            "./n0758411000000885.jpg\n",
            "./n0758411000000886.jpg\n",
            "./n0758411000000890.jpg\n",
            "./n0758411000000893.jpg\n",
            "./n0758411000000894.jpg\n",
            "./n0758411000000896.jpg\n",
            "./n0758411000000898.jpg\n",
            "./n0758411000000899.jpg\n",
            "./n0758411000000901.jpg\n",
            "./n0758411000000902.jpg\n",
            "./n0758411000000903.jpg\n",
            "./n0758411000000906.jpg\n",
            "./n0758411000000907.jpg\n",
            "./n0758411000000908.jpg\n",
            "./n0758411000000912.jpg\n",
            "./n0758411000000913.jpg\n",
            "./n0758411000000915.jpg\n",
            "./n0758411000000919.jpg\n",
            "./n0758411000000920.jpg\n",
            "./n0758411000000921.jpg\n",
            "./n0758411000000922.jpg\n",
            "./n0758411000000923.jpg\n",
            "./n0758411000000925.jpg\n",
            "./n0758411000000927.jpg\n",
            "./n0758411000000929.jpg\n",
            "./n0758411000000930.jpg\n",
            "./n0758411000000933.jpg\n",
            "./n0758411000000935.jpg\n",
            "./n0758411000000937.jpg\n",
            "./n0758411000000938.jpg\n",
            "./n0758411000000941.jpg\n",
            "./n0758411000000942.jpg\n",
            "./n0758411000000945.jpg\n",
            "./n0758411000000946.jpg\n",
            "./n0758411000000948.jpg\n",
            "./n0758411000000949.jpg\n",
            "./n0758411000000950.jpg\n",
            "./n0758411000000952.jpg\n",
            "./n0758411000000956.jpg\n",
            "./n0758411000000957.jpg\n",
            "./n0758411000000959.jpg\n",
            "./n0758411000000962.jpg\n",
            "./n0758411000000963.jpg\n",
            "./n0758411000000964.jpg\n",
            "./n0758411000000965.jpg\n",
            "./n0758411000000966.jpg\n",
            "./n0758411000000969.jpg\n",
            "./n0758411000000972.jpg\n",
            "./n0758411000000973.jpg\n",
            "./n0758411000000979.jpg\n",
            "./n0758411000000980.jpg\n",
            "./n0758411000000982.jpg\n",
            "./n0758411000000990.jpg\n",
            "./n0758411000000991.jpg\n",
            "./n0758411000000993.jpg\n",
            "./n0758411000000994.jpg\n",
            "./n0758411000000995.jpg\n",
            "./n0758411000000997.jpg\n",
            "./n0758411000000998.jpg\n",
            "./n0758411000000999.jpg\n",
            "./n0758411000001000.jpg\n",
            "./n0758411000001002.jpg\n",
            "./n0758411000001003.jpg\n",
            "./n0758411000001004.jpg\n",
            "./n0758411000001005.jpg\n",
            "./n0758411000001015.jpg\n",
            "./n0758411000001017.jpg\n",
            "./n0758411000001019.jpg\n",
            "./n0758411000001022.jpg\n",
            "./n0758411000001025.jpg\n",
            "./n0758411000001028.jpg\n",
            "./n0758411000001029.jpg\n",
            "./n0758411000001031.jpg\n",
            "./n0758411000001032.jpg\n",
            "./n0758411000001033.jpg\n",
            "./n0758411000001035.jpg\n",
            "./n0758411000001039.jpg\n",
            "./n0758411000001040.jpg\n",
            "./n0758411000001041.jpg\n",
            "./n0758411000001042.jpg\n",
            "./n0758411000001046.jpg\n",
            "./n0758411000001047.jpg\n",
            "./n0758411000001049.jpg\n",
            "./n0758411000001051.jpg\n",
            "./n0758411000001055.jpg\n",
            "./n0758411000001059.jpg\n",
            "./n0758411000001060.jpg\n",
            "./n0758411000001061.jpg\n",
            "./n0758411000001062.jpg\n",
            "./n0758411000001063.jpg\n",
            "./n0758411000001066.jpg\n",
            "./n0758411000001068.jpg\n",
            "./n0758411000001069.jpg\n",
            "./n0758411000001070.jpg\n",
            "./n0758411000001073.jpg\n",
            "./n0758411000001074.jpg\n",
            "./n0758411000001076.jpg\n",
            "./n0758411000001080.jpg\n",
            "./n0758411000001083.jpg\n",
            "./n0758411000001084.jpg\n",
            "./n0758411000001087.jpg\n",
            "./n0758411000001088.jpg\n",
            "./n0758411000001090.jpg\n",
            "./n0758411000001091.jpg\n",
            "./n0758411000001092.jpg\n",
            "./n0758411000001093.jpg\n",
            "./n0758411000001096.jpg\n",
            "./n0758411000001098.jpg\n",
            "./n0758411000001099.jpg\n",
            "./n0758411000001100.jpg\n",
            "./n0758411000001102.jpg\n",
            "./n0758411000001104.jpg\n",
            "./n0758411000001105.jpg\n",
            "./n0758411000001106.jpg\n",
            "./n0758411000001107.jpg\n",
            "./n0758411000001109.jpg\n",
            "./n0758411000001110.jpg\n",
            "./n0758411000001111.jpg\n",
            "./n0758411000001112.jpg\n",
            "./n0758411000001114.jpg\n",
            "./n0758411000001116.jpg\n",
            "./n0758411000001120.jpg\n",
            "./n0758411000001121.jpg\n",
            "./n0758411000001123.jpg\n",
            "./n0758411000001125.jpg\n",
            "./n0758411000001126.jpg\n",
            "./n0758411000001127.jpg\n",
            "./n0758411000001128.jpg\n",
            "./n0758411000001130.jpg\n",
            "./n0758411000001132.jpg\n",
            "./n0758411000001134.jpg\n",
            "./n0758411000001135.jpg\n",
            "./n0758411000001136.jpg\n",
            "./n0758411000001137.jpg\n",
            "./n0758411000001140.jpg\n",
            "./n0758411000001142.jpg\n",
            "./n0758411000001144.jpg\n",
            "./n0758411000001146.jpg\n",
            "./n0758411000001147.jpg\n",
            "./n0758411000001148.jpg\n",
            "./n0758411000001149.jpg\n",
            "./n0758411000001151.jpg\n",
            "./n0758411000001154.jpg\n",
            "./n0758411000001156.jpg\n",
            "./n0758411000001157.jpg\n",
            "./n0758411000001160.jpg\n",
            "./n0758411000001161.jpg\n",
            "./n0758411000001164.jpg\n",
            "./n0758411000001165.jpg\n",
            "./n0758411000001166.jpg\n",
            "./n0758411000001170.jpg\n",
            "./n0758411000001173.jpg\n",
            "./n0758411000001174.jpg\n",
            "./n0758411000001175.jpg\n",
            "./n0758411000001176.jpg\n",
            "./n0758411000001178.jpg\n",
            "./n0758411000001180.jpg\n",
            "./n0758411000001181.jpg\n",
            "./n0758411000001182.jpg\n",
            "./n0758411000001184.jpg\n",
            "./n0758411000001186.jpg\n",
            "./n0758411000001188.jpg\n",
            "./n0758411000001190.jpg\n",
            "./n0758411000001191.jpg\n",
            "./n0758411000001192.jpg\n",
            "./n0758411000001193.jpg\n",
            "./n0758411000001195.jpg\n",
            "./n0758411000001197.jpg\n",
            "./n0758411000001202.jpg\n",
            "./n0758411000001205.jpg\n",
            "./n0761348000000001.jpg\n",
            "./n0761348000000002.jpg\n",
            "./n0761348000000005.jpg\n",
            "./n0761348000000007.jpg\n",
            "./n0761348000000009.jpg\n",
            "./n0761348000000010.jpg\n",
            "./n0761348000000011.jpg\n",
            "./n0761348000000013.jpg\n",
            "./n0761348000000015.jpg\n",
            "./n0761348000000019.jpg\n",
            "./n0761348000000022.jpg\n",
            "./n0761348000000025.jpg\n",
            "./n0761348000000027.jpg\n",
            "./n0761348000000031.jpg\n",
            "./n0761348000000037.jpg\n",
            "./n0761348000000042.jpg\n",
            "./n0761348000000044.jpg\n",
            "./n0761348000000045.jpg\n",
            "./n0761348000000046.jpg\n",
            "./n0761348000000047.jpg\n",
            "./n0761348000000049.jpg\n",
            "./n0761348000000050.jpg\n",
            "./n0761348000000054.jpg\n",
            "./n0761348000000057.jpg\n",
            "./n0761348000000061.jpg\n",
            "./n0761348000000062.jpg\n",
            "./n0761348000000064.jpg\n",
            "./n0761348000000072.jpg\n",
            "./n0761348000000073.jpg\n",
            "./n0761348000000074.jpg\n",
            "./n0761348000000075.jpg\n",
            "./n0761348000000076.jpg\n",
            "./n0761348000000079.jpg\n",
            "./n0761348000000080.jpg\n",
            "./n0761348000000082.jpg\n",
            "./n0761348000000083.jpg\n",
            "./n0761348000000084.jpg\n",
            "./n0761348000000085.jpg\n",
            "./n0761348000000090.jpg\n",
            "./n0761348000000091.jpg\n",
            "./n0761348000000092.jpg\n",
            "./n0761348000000095.jpg\n",
            "./n0761348000000097.jpg\n",
            "./n0761348000000099.jpg\n",
            "./n0761348000000110.jpg\n",
            "./n0761348000000112.jpg\n",
            "./n0761348000000113.jpg\n",
            "./n0761348000000114.jpg\n",
            "./n0761348000000115.jpg\n",
            "./n0761348000000117.jpg\n",
            "./n0761348000000121.jpg\n",
            "./n0761348000000124.jpg\n",
            "./n0761348000000125.jpg\n",
            "./n0761348000000126.jpg\n",
            "./n0761348000000127.jpg\n",
            "./n0761348000000130.jpg\n",
            "./n0761348000000134.jpg\n",
            "./n0761348000000139.jpg\n",
            "./n0761348000000140.jpg\n",
            "./n0761348000000141.jpg\n",
            "./n0761348000000143.jpg\n",
            "./n0761348000000144.jpg\n",
            "./n0761348000000145.jpg\n",
            "./n0761348000000147.jpg\n",
            "./n0761348000000149.jpg\n",
            "./n0761348000000152.jpg\n",
            "./n0761348000000153.jpg\n",
            "./n0761348000000157.jpg\n",
            "./n0761348000000158.jpg\n",
            "./n0761348000000159.jpg\n",
            "./n0761348000000162.jpg\n",
            "./n0761348000000163.jpg\n",
            "./n0761348000000164.jpg\n",
            "./n0761348000000165.jpg\n",
            "./n0761348000000166.jpg\n",
            "./n0761348000000168.jpg\n",
            "./n0761348000000169.jpg\n",
            "./n0761348000000171.jpg\n",
            "./n0761348000000176.jpg\n",
            "./n0761348000000178.jpg\n",
            "./n0761348000000179.jpg\n",
            "./n0761348000000180.jpg\n",
            "./n0761348000000183.jpg\n",
            "./n0761348000000185.jpg\n",
            "./n0761348000000186.jpg\n",
            "./n0761348000000188.jpg\n",
            "./n0761348000000189.jpg\n",
            "./n0761348000000190.jpg\n",
            "./n0761348000000191.jpg\n",
            "./n0761348000000193.jpg\n",
            "./n0761348000000197.jpg\n",
            "./n0761348000000198.jpg\n",
            "./n0761348000000199.jpg\n",
            "./n0761348000000200.jpg\n",
            "./n0761348000000201.jpg\n",
            "./n0761348000000205.jpg\n",
            "./n0761348000000206.jpg\n",
            "./n0761348000000208.jpg\n",
            "./n0761348000000209.jpg\n",
            "./n0761348000000211.jpg\n",
            "./n0761348000000213.jpg\n",
            "./n0761348000000214.jpg\n",
            "./n0761348000000217.jpg\n",
            "./n0761348000000219.jpg\n",
            "./n0761348000000222.jpg\n",
            "./n0761348000000224.jpg\n",
            "./n0761348000000225.jpg\n",
            "./n0761348000000227.jpg\n",
            "./n0761348000000228.jpg\n",
            "./n0761348000000229.jpg\n",
            "./n0761348000000230.jpg\n",
            "./n0761348000000231.jpg\n",
            "./n0761348000000239.jpg\n",
            "./n0761348000000240.jpg\n",
            "./n0761348000000242.jpg\n",
            "./n0761348000000245.jpg\n",
            "./n0761348000000249.jpg\n",
            "./n0761348000000253.jpg\n",
            "./n0761348000000254.jpg\n",
            "./n0761348000000256.jpg\n",
            "./n0761348000000257.jpg\n",
            "./n0761348000000258.jpg\n",
            "./n0761348000000262.jpg\n",
            "./n0761348000000265.jpg\n",
            "./n0761348000000266.jpg\n",
            "./n0761348000000267.jpg\n",
            "./n0761348000000269.jpg\n",
            "./n0761348000000272.jpg\n",
            "./n0761348000000274.jpg\n",
            "./n0761348000000275.jpg\n",
            "./n0761348000000276.jpg\n",
            "./n0761348000000277.jpg\n",
            "./n0761348000000278.jpg\n",
            "./n0761348000000279.jpg\n",
            "./n0761348000000280.jpg\n",
            "./n0761348000000282.jpg\n",
            "./n0761348000000283.jpg\n",
            "./n0761348000000286.jpg\n",
            "./n0761348000000287.jpg\n",
            "./n0761348000000289.jpg\n",
            "./n0761348000000291.jpg\n",
            "./n0761348000000292.jpg\n",
            "./n0761348000000293.jpg\n",
            "./n0761348000000296.jpg\n",
            "./n0761348000000297.jpg\n",
            "./n0761348000000298.jpg\n",
            "./n0761348000000299.jpg\n",
            "./n0761348000000300.jpg\n",
            "./n0761348000000301.jpg\n",
            "./n0761348000000304.jpg\n",
            "./n0761348000000306.jpg\n",
            "./n0761348000000307.jpg\n",
            "./n0761348000000308.jpg\n",
            "./n0761348000000310.jpg\n",
            "./n0761348000000312.jpg\n",
            "./n0761348000000313.jpg\n",
            "./n0761348000000314.jpg\n",
            "./n0761348000000315.jpg\n",
            "./n0761348000000317.jpg\n",
            "./n0761348000000322.jpg\n",
            "./n0761348000000323.jpg\n",
            "./n0761348000000327.jpg\n",
            "./n0761348000000328.jpg\n",
            "./n0761348000000330.jpg\n",
            "./n0761348000000331.jpg\n",
            "./n0761348000000333.jpg\n",
            "./n0761348000000336.jpg\n",
            "./n0761348000000337.jpg\n",
            "./n0761348000000343.jpg\n",
            "./n0761348000000344.jpg\n",
            "./n0761348000000349.jpg\n",
            "./n0761348000000350.jpg\n",
            "./n0761348000000351.jpg\n",
            "./n0761348000000353.jpg\n",
            "./n0761348000000355.jpg\n",
            "./n0761348000000363.jpg\n",
            "./n0761348000000364.jpg\n",
            "./n0761348000000365.jpg\n",
            "./n0761348000000371.jpg\n",
            "./n0761348000000374.jpg\n",
            "./n0761348000000377.jpg\n",
            "./n0761348000000380.jpg\n",
            "./n0761348000000382.jpg\n",
            "./n0761348000000383.jpg\n",
            "./n0761348000000386.jpg\n",
            "./n0761348000000388.jpg\n",
            "./n0761348000000393.jpg\n",
            "./n0761348000000394.jpg\n",
            "./n0761348000000396.jpg\n",
            "./n0761348000000400.jpg\n",
            "./n0761348000000401.jpg\n",
            "./n0761348000000403.jpg\n",
            "./n0761348000000406.jpg\n",
            "./n0761348000000407.jpg\n",
            "./n0761348000000408.jpg\n",
            "./n0761348000000411.jpg\n",
            "./n0761348000000413.jpg\n",
            "./n0761348000000415.jpg\n",
            "./n0761348000000417.jpg\n",
            "./n0761348000000419.jpg\n",
            "./n0761348000000421.jpg\n",
            "./n0761348000000424.jpg\n",
            "./n0761348000000427.jpg\n",
            "./n0761348000000428.jpg\n",
            "./n0761348000000429.jpg\n",
            "./n0761348000000432.jpg\n",
            "./n0761348000000433.jpg\n",
            "./n0761348000000434.jpg\n",
            "./n0761348000000436.jpg\n",
            "./n0761348000000440.jpg\n",
            "./n0761348000000441.jpg\n",
            "./n0761348000000443.jpg\n",
            "./n0761348000000444.jpg\n",
            "./n0761348000000445.jpg\n",
            "./n0761348000000447.jpg\n",
            "./n0761348000000448.jpg\n",
            "./n0761348000000450.jpg\n",
            "./n0761348000000451.jpg\n",
            "./n0761348000000452.jpg\n",
            "./n0761348000000457.jpg\n",
            "./n0761348000000461.jpg\n",
            "./n0761348000000462.jpg\n",
            "./n0761348000000464.jpg\n",
            "./n0761348000000465.jpg\n",
            "./n0761348000000467.jpg\n",
            "./n0761348000000468.jpg\n",
            "./n0761348000000469.jpg\n",
            "./n0761348000000470.jpg\n",
            "./n0761348000000472.jpg\n",
            "./n0761348000000473.jpg\n",
            "./n0761348000000475.jpg\n",
            "./n0761348000000477.jpg\n",
            "./n0761348000000478.jpg\n",
            "./n0761348000000480.jpg\n",
            "./n0761348000000482.jpg\n",
            "./n0761348000000486.jpg\n",
            "./n0761348000000487.jpg\n",
            "./n0761348000000489.jpg\n",
            "./n0761348000000493.jpg\n",
            "./n0761348000000494.jpg\n",
            "./n0761348000000495.jpg\n",
            "./n0761348000000497.jpg\n",
            "./n0761348000000498.jpg\n",
            "./n0761348000000502.jpg\n",
            "./n0761348000000509.jpg\n",
            "./n0761348000000511.jpg\n",
            "./n0761348000000513.jpg\n",
            "./n0761348000000518.jpg\n",
            "./n0761348000000520.jpg\n",
            "./n0761348000000521.jpg\n",
            "./n0761348000000531.jpg\n",
            "./n0761348000000532.jpg\n",
            "./n0761348000000535.jpg\n",
            "./n0761348000000538.jpg\n",
            "./n0761348000000541.jpg\n",
            "./n0761348000000543.jpg\n",
            "./n0761348000000545.jpg\n",
            "./n0761348000000547.jpg\n",
            "./n0761348000000549.jpg\n",
            "./n0761348000000550.jpg\n",
            "./n0761348000000553.jpg\n",
            "./n0761348000000554.jpg\n",
            "./n0761348000000556.jpg\n",
            "./n0761348000000557.jpg\n",
            "./n0761348000000560.jpg\n",
            "./n0761348000000563.jpg\n",
            "./n0761348000000564.jpg\n",
            "./n0761348000000566.jpg\n",
            "./n0761348000000568.jpg\n",
            "./n0761348000000570.jpg\n",
            "./n0761348000000571.jpg\n",
            "./n0761348000000574.jpg\n",
            "./n0761348000000577.jpg\n",
            "./n0761348000000579.jpg\n",
            "./n0761348000000580.jpg\n",
            "./n0761348000000581.jpg\n",
            "./n0761348000000582.jpg\n",
            "./n0761348000000583.jpg\n",
            "./n0761348000000586.jpg\n",
            "./n0761348000000587.jpg\n",
            "./n0761348000000588.jpg\n",
            "./n0761348000000590.jpg\n",
            "./n0761348000000592.jpg\n",
            "./n0761348000000594.jpg\n",
            "./n0761348000000595.jpg\n",
            "./n0761348000000596.jpg\n",
            "./n0761348000000597.jpg\n",
            "./n0761348000000602.jpg\n",
            "./n0761348000000605.jpg\n",
            "./n0761348000000607.jpg\n",
            "./n0761348000000611.jpg\n",
            "./n0761348000000612.jpg\n",
            "./n0761348000000615.jpg\n",
            "./n0761348000000616.jpg\n",
            "./n0761348000000618.jpg\n",
            "./n0761348000000620.jpg\n",
            "./n0761348000000622.jpg\n",
            "./n0761348000000624.jpg\n",
            "./n0761348000000627.jpg\n",
            "./n0761348000000630.jpg\n",
            "./n0761348000000632.jpg\n",
            "./n0761348000000633.jpg\n",
            "./n0761348000000635.jpg\n",
            "./n0761348000000641.jpg\n",
            "./n0761348000000642.jpg\n",
            "./n0761348000000645.jpg\n",
            "./n0761348000000649.jpg\n",
            "./n0761348000000650.jpg\n",
            "./n0761348000000651.jpg\n",
            "./n0761348000000652.jpg\n",
            "./n0761348000000656.jpg\n",
            "./n0761348000000657.jpg\n",
            "./n0761348000000659.jpg\n",
            "./n0761348000000662.jpg\n",
            "./n0761348000000663.jpg\n",
            "./n0761348000000664.jpg\n",
            "./n0761348000000665.jpg\n",
            "./n0761348000000666.jpg\n",
            "./n0761348000000667.jpg\n",
            "./n0761348000000668.jpg\n",
            "./n0761348000000669.jpg\n",
            "./n0761348000000671.jpg\n",
            "./n0761348000000672.jpg\n",
            "./n0761348000000677.jpg\n",
            "./n0761348000000681.jpg\n",
            "./n0761348000000683.jpg\n",
            "./n0761348000000684.jpg\n",
            "./n0761348000000686.jpg\n",
            "./n0761348000000689.jpg\n",
            "./n0761348000000690.jpg\n",
            "./n0761348000000691.jpg\n",
            "./n0761348000000692.jpg\n",
            "./n0761348000000693.jpg\n",
            "./n0761348000000696.jpg\n",
            "./n0761348000000697.jpg\n",
            "./n0761348000000698.jpg\n",
            "./n0761348000000700.jpg\n",
            "./n0761348000000705.jpg\n",
            "./n0761348000000707.jpg\n",
            "./n0761348000000709.jpg\n",
            "./n0761348000000710.jpg\n",
            "./n0761348000000713.jpg\n",
            "./n0761348000000714.jpg\n",
            "./n0761348000000715.jpg\n",
            "./n0761348000000716.jpg\n",
            "./n0761348000000717.jpg\n",
            "./n0761348000000721.jpg\n",
            "./n0761348000000723.jpg\n",
            "./n0761348000000724.jpg\n",
            "./n0761348000000730.jpg\n",
            "./n0761348000000733.jpg\n",
            "./n0761348000000735.jpg\n",
            "./n0761348000000737.jpg\n",
            "./n0761348000000741.jpg\n",
            "./n0761348000000742.jpg\n",
            "./n0761348000000743.jpg\n",
            "./n0761348000000744.jpg\n",
            "./n0761348000000746.jpg\n",
            "./n0761348000000748.jpg\n",
            "./n0761348000000754.jpg\n",
            "./n0761348000000755.jpg\n",
            "./n0761348000000757.jpg\n",
            "./n0761348000000760.jpg\n",
            "./n0761348000000765.jpg\n",
            "./n0761348000000766.jpg\n",
            "./n0761348000000768.jpg\n",
            "./n0761348000000769.jpg\n",
            "./n0761348000000770.jpg\n",
            "./n0761348000000772.jpg\n",
            "./n0761348000000775.jpg\n",
            "./n0761348000000778.jpg\n",
            "./n0761348000000779.jpg\n",
            "./n0761348000000783.jpg\n",
            "./n0761348000000784.jpg\n",
            "./n0761348000000789.jpg\n",
            "./n0761348000000792.jpg\n",
            "./n0761348000000794.jpg\n",
            "./n0761348000000800.jpg\n",
            "./n0761348000000801.jpg\n",
            "./n0761348000000803.jpg\n",
            "./n0761348000000807.jpg\n",
            "./n0761348000000808.jpg\n",
            "./n0761348000000811.jpg\n",
            "./n0761348000000818.jpg\n",
            "./n0761348000000821.jpg\n",
            "./n0761348000000824.jpg\n",
            "./n0761348000000825.jpg\n",
            "./n0761348000000827.jpg\n",
            "./n0761348000000829.jpg\n",
            "./n0761348000000832.jpg\n",
            "./n0761348000000833.jpg\n",
            "./n0761348000000835.jpg\n",
            "./n0761348000000838.jpg\n",
            "./n0761348000000840.jpg\n",
            "./n0761348000000843.jpg\n",
            "./n0761348000000844.jpg\n",
            "./n0761348000000845.jpg\n",
            "./n0761348000000849.jpg\n",
            "./n0761348000000850.jpg\n",
            "./n0761348000000852.jpg\n",
            "./n0761348000000856.jpg\n",
            "./n0761348000000859.jpg\n",
            "./n0761348000000860.jpg\n",
            "./n0761348000000861.jpg\n",
            "./n0761348000000862.jpg\n",
            "./n0761348000000863.jpg\n",
            "./n0761348000000864.jpg\n",
            "./n0761348000000867.jpg\n",
            "./n0761348000000868.jpg\n",
            "./n0761348000000869.jpg\n",
            "./n0761348000000871.jpg\n",
            "./n0761348000000873.jpg\n",
            "./n0761348000000875.jpg\n",
            "./n0761348000000881.jpg\n",
            "./n0761348000000882.jpg\n",
            "./n0761348000000888.jpg\n",
            "./n0761348000000889.jpg\n",
            "./n0761348000000890.jpg\n",
            "./n0761348000000892.jpg\n",
            "./n0761348000000894.jpg\n",
            "./n0761348000000895.jpg\n",
            "./n0761348000000896.jpg\n",
            "./n0761348000000897.jpg\n",
            "./n0761348000000902.jpg\n",
            "./n0761348000000904.jpg\n",
            "./n0761348000000908.jpg\n",
            "./n0761348000000910.jpg\n",
            "./n0761348000000914.jpg\n",
            "./n0761348000000915.jpg\n",
            "./n0761348000000916.jpg\n",
            "./n0761348000000918.jpg\n",
            "./n0761348000000919.jpg\n",
            "./n0761348000000920.jpg\n",
            "./n0761348000000923.jpg\n",
            "./n0761348000000925.jpg\n",
            "./n0761348000000926.jpg\n",
            "./n0761348000000927.jpg\n",
            "./n0761348000000933.jpg\n",
            "./n0761348000000935.jpg\n",
            "./n0761348000000936.jpg\n",
            "./n0761348000000941.jpg\n",
            "./n0761348000000943.jpg\n",
            "./n0761348000000946.jpg\n",
            "./n0761348000000949.jpg\n",
            "./n0761348000000950.jpg\n",
            "./n0761348000000951.jpg\n",
            "./n0761348000000954.jpg\n",
            "./n0761348000000955.jpg\n",
            "./n0761348000000959.jpg\n",
            "./n0761348000000965.jpg\n",
            "./n0761348000000969.jpg\n",
            "./n0761348000000971.jpg\n",
            "./n0761348000000973.jpg\n",
            "./n0761348000000974.jpg\n",
            "./n0761348000000976.jpg\n",
            "./n0761348000000978.jpg\n",
            "./n0761348000000979.jpg\n",
            "./n0761348000000981.jpg\n",
            "./n0761348000000982.jpg\n",
            "./n0761348000000988.jpg\n",
            "./n0761348000000991.jpg\n",
            "./n0761348000000995.jpg\n",
            "./n0761348000000997.jpg\n",
            "./n0761348000000998.jpg\n",
            "./n0761348000000999.jpg\n",
            "./n0761348000001004.jpg\n",
            "./n0761348000001009.jpg\n",
            "./n0761348000001014.jpg\n",
            "./n0761348000001018.jpg\n",
            "./n0761348000001019.jpg\n",
            "./n0761348000001020.jpg\n",
            "./n0761348000001022.jpg\n",
            "./n0761348000001025.jpg\n",
            "./n0761348000001028.jpg\n",
            "./n0761348000001029.jpg\n",
            "./n0761348000001033.jpg\n",
            "./n0761348000001034.jpg\n",
            "./n0761348000001035.jpg\n",
            "./n0761348000001039.jpg\n",
            "./n0761348000001046.jpg\n",
            "./n0761348000001051.jpg\n",
            "./n0761348000001053.jpg\n",
            "./n0761348000001055.jpg\n",
            "./n0761348000001056.jpg\n",
            "./n0761348000001059.jpg\n",
            "./n0761348000001060.jpg\n",
            "./n0761348000001061.jpg\n",
            "./n0761348000001064.jpg\n",
            "./n0761348000001067.jpg\n",
            "./n0761348000001068.jpg\n",
            "./n0761348000001072.jpg\n",
            "./n0761348000001073.jpg\n",
            "./n0761348000001076.jpg\n",
            "./n0761348000001077.jpg\n",
            "./n0761348000001080.jpg\n",
            "./n0761348000001082.jpg\n",
            "./n0761348000001083.jpg\n",
            "./n0761348000001088.jpg\n",
            "./n0761348000001089.jpg\n",
            "./n0761348000001090.jpg\n",
            "./n0761348000001092.jpg\n",
            "./n0761348000001095.jpg\n",
            "./n0761348000001096.jpg\n",
            "./n0761348000001097.jpg\n",
            "./n0761348000001098.jpg\n",
            "./n0761348000001099.jpg\n",
            "./n0761348000001100.jpg\n",
            "./n0761348000001104.jpg\n",
            "./n0761348000001105.jpg\n",
            "./n0761348000001106.jpg\n",
            "./n0761348000001111.jpg\n",
            "./n0761348000001112.jpg\n",
            "./n0761348000001114.jpg\n",
            "./n0761348000001115.jpg\n",
            "./n0761348000001118.jpg\n",
            "./n0761348000001119.jpg\n",
            "./n0761348000001121.jpg\n",
            "./n0761348000001122.jpg\n",
            "./n0761348000001124.jpg\n",
            "./n0761348000001125.jpg\n",
            "./n0761348000001128.jpg\n",
            "./n0761348000001130.jpg\n",
            "./n0761348000001132.jpg\n",
            "./n0761348000001134.jpg\n",
            "./n0761348000001136.jpg\n",
            "./n0761348000001137.jpg\n",
            "./n0761348000001142.jpg\n",
            "./n0761348000001144.jpg\n",
            "./n0761348000001145.jpg\n",
            "./n0761348000001150.jpg\n",
            "./n0761348000001151.jpg\n",
            "./n0761348000001152.jpg\n",
            "./n0761348000001153.jpg\n",
            "./n0761348000001157.jpg\n",
            "./n0761348000001159.jpg\n",
            "./n0761348000001160.jpg\n",
            "./n0761348000001162.jpg\n",
            "./n0761348000001163.jpg\n",
            "./n0761348000001164.jpg\n",
            "./n0761348000001167.jpg\n",
            "./n0761348000001168.jpg\n",
            "./n0761348000001176.jpg\n",
            "./n0761348000001177.jpg\n",
            "./n0761348000001178.jpg\n",
            "./n0761348000001180.jpg\n",
            "./n0761348000001182.jpg\n",
            "./n0761348000001183.jpg\n",
            "./n0761348000001187.jpg\n",
            "./n0761348000001189.jpg\n",
            "./n0761348000001190.jpg\n",
            "./n0761348000001191.jpg\n",
            "./n0761348000001192.jpg\n",
            "./n0761348000001193.jpg\n",
            "./n0761348000001197.jpg\n",
            "./n0761348000001198.jpg\n",
            "./n0761348000001199.jpg\n",
            "./n0761348000001200.jpg\n",
            "./n0761348000001202.jpg\n",
            "./n0761348000001204.jpg\n",
            "./n0761348000001205.jpg\n",
            "./n0761348000001207.jpg\n",
            "./n0761348000001215.jpg\n",
            "./n0761348000001221.jpg\n",
            "./n0761348000001225.jpg\n",
            "./n0761348000001229.jpg\n",
            "./n0761348000001234.jpg\n",
            "./n0761348000001237.jpg\n",
            "./n0761348000001238.jpg\n",
            "./n0761348000001239.jpg\n",
            "./n0761348000001240.jpg\n",
            "./n0761348000001242.jpg\n",
            "./n0761348000001245.jpg\n",
            "./n0761348000001247.jpg\n",
            "./n0761348000001249.jpg\n",
            "./n0761348000001252.jpg\n",
            "./n0761348000001254.jpg\n",
            "./n0761348000001260.jpg\n",
            "./n0761348000001264.jpg\n",
            "./n0761348000001265.jpg\n",
            "./n0761348000001268.jpg\n",
            "./n0761348000001271.jpg\n",
            "./n0761348000001272.jpg\n",
            "./n0761348000001273.jpg\n",
            "./n0761348000001274.jpg\n",
            "./n0761348000001276.jpg\n",
            "./n0761348000001277.jpg\n",
            "./n0761348000001278.jpg\n",
            "./n0761348000001279.jpg\n",
            "./n0761348000001281.jpg\n",
            "./n0761348000001282.jpg\n",
            "./n0761348000001284.jpg\n",
            "./n0761348000001288.jpg\n",
            "./n0761348000001289.jpg\n",
            "./n0761348000001290.jpg\n",
            "./n0761348000001292.jpg\n",
            "./n0761348000001293.jpg\n",
            "./n0761348000001294.jpg\n",
            "./n0761348000001296.jpg\n",
            "./n0761348000001297.jpg\n",
            "./n0761348000001298.jpg\n",
            "./n0769753700000001.jpg\n",
            "./n0769753700000006.jpg\n",
            "./n0769753700000008.jpg\n",
            "./n0769753700000010.jpg\n",
            "./n0769753700000011.jpg\n",
            "./n0769753700000013.jpg\n",
            "./n0769753700000014.jpg\n",
            "./n0769753700000015.jpg\n",
            "./n0769753700000017.jpg\n",
            "./n0769753700000018.jpg\n",
            "./n0769753700000022.jpg\n",
            "./n0769753700000027.jpg\n",
            "./n0769753700000029.jpg\n",
            "./n0769753700000031.jpg\n",
            "./n0769753700000033.jpg\n",
            "./n0769753700000036.jpg\n",
            "./n0769753700000045.jpg\n",
            "./n0769753700000046.jpg\n",
            "./n0769753700000047.jpg\n",
            "./n0769753700000048.jpg\n",
            "./n0769753700000055.jpg\n",
            "./n0769753700000058.jpg\n",
            "./n0769753700000060.jpg\n",
            "./n0769753700000061.jpg\n",
            "./n0769753700000064.jpg\n",
            "./n0769753700000068.jpg\n",
            "./n0769753700000070.jpg\n",
            "./n0769753700000074.jpg\n",
            "./n0769753700000075.jpg\n",
            "./n0769753700000079.jpg\n",
            "./n0769753700000081.jpg\n",
            "./n0769753700000083.jpg\n",
            "./n0769753700000084.jpg\n",
            "./n0769753700000087.jpg\n",
            "./n0769753700000088.jpg\n",
            "./n0769753700000091.jpg\n",
            "./n0769753700000092.jpg\n",
            "./n0769753700000097.jpg\n",
            "./n0769753700000099.jpg\n",
            "./n0769753700000100.jpg\n",
            "./n0769753700000101.jpg\n",
            "./n0769753700000104.jpg\n",
            "./n0769753700000105.jpg\n",
            "./n0769753700000106.jpg\n",
            "./n0769753700000109.jpg\n",
            "./n0769753700000114.jpg\n",
            "./n0769753700000115.jpg\n",
            "./n0769753700000120.jpg\n",
            "./n0769753700000125.jpg\n",
            "./n0769753700000126.jpg\n",
            "./n0769753700000131.jpg\n",
            "./n0769753700000132.jpg\n",
            "./n0769753700000133.jpg\n",
            "./n0769753700000137.jpg\n",
            "./n0769753700000138.jpg\n",
            "./n0769753700000141.jpg\n",
            "./n0769753700000142.jpg\n",
            "./n0769753700000143.jpg\n",
            "./n0769753700000144.jpg\n",
            "./n0769753700000145.jpg\n",
            "./n0769753700000148.jpg\n",
            "./n0769753700000151.jpg\n",
            "./n0769753700000154.jpg\n",
            "./n0769753700000155.jpg\n",
            "./n0769753700000156.jpg\n",
            "./n0769753700000157.jpg\n",
            "./n0769753700000158.jpg\n",
            "./n0769753700000159.jpg\n",
            "./n0769753700000162.jpg\n",
            "./n0769753700000163.jpg\n",
            "./n0769753700000166.jpg\n",
            "./n0769753700000169.jpg\n",
            "./n0769753700000171.jpg\n",
            "./n0769753700000174.jpg\n",
            "./n0769753700000175.jpg\n",
            "./n0769753700000176.jpg\n",
            "./n0769753700000177.jpg\n",
            "./n0769753700000179.jpg\n",
            "./n0769753700000181.jpg\n",
            "./n0769753700000183.jpg\n",
            "./n0769753700000184.jpg\n",
            "./n0769753700000185.jpg\n",
            "./n0769753700000194.jpg\n",
            "./n0769753700000197.jpg\n",
            "./n0769753700000199.jpg\n",
            "./n0769753700000202.jpg\n",
            "./n0769753700000209.jpg\n",
            "./n0769753700000211.jpg\n",
            "./n0769753700000212.jpg\n",
            "./n0769753700000214.jpg\n",
            "./n0769753700000216.jpg\n",
            "./n0769753700000217.jpg\n",
            "./n0769753700000218.jpg\n",
            "./n0769753700000220.jpg\n",
            "./n0769753700000221.jpg\n",
            "./n0769753700000223.jpg\n",
            "./n0769753700000226.jpg\n",
            "./n0769753700000227.jpg\n",
            "./n0769753700000229.jpg\n",
            "./n0769753700000231.jpg\n",
            "./n0769753700000232.jpg\n",
            "./n0769753700000233.jpg\n",
            "./n0769753700000235.jpg\n",
            "./n0769753700000237.jpg\n",
            "./n0769753700000240.jpg\n",
            "./n0769753700000242.jpg\n",
            "./n0769753700000245.jpg\n",
            "./n0769753700000249.jpg\n",
            "./n0769753700000250.jpg\n",
            "./n0769753700000251.jpg\n",
            "./n0769753700000257.jpg\n",
            "./n0769753700000258.jpg\n",
            "./n0769753700000259.jpg\n",
            "./n0769753700000261.jpg\n",
            "./n0769753700000262.jpg\n",
            "./n0769753700000263.jpg\n",
            "./n0769753700000267.jpg\n",
            "./n0769753700000271.jpg\n",
            "./n0769753700000272.jpg\n",
            "./n0769753700000273.jpg\n",
            "./n0769753700000275.jpg\n",
            "./n0769753700000278.jpg\n",
            "./n0769753700000280.jpg\n",
            "./n0769753700000282.jpg\n",
            "./n0769753700000283.jpg\n",
            "./n0769753700000287.jpg\n",
            "./n0769753700000288.jpg\n",
            "./n0769753700000293.jpg\n",
            "./n0769753700000294.jpg\n",
            "./n0769753700000296.jpg\n",
            "./n0769753700000301.jpg\n",
            "./n0769753700000302.jpg\n",
            "./n0769753700000303.jpg\n",
            "./n0769753700000304.jpg\n",
            "./n0769753700000309.jpg\n",
            "./n0769753700000311.jpg\n",
            "./n0769753700000312.jpg\n",
            "./n0769753700000313.jpg\n",
            "./n0769753700000316.jpg\n",
            "./n0769753700000320.jpg\n",
            "./n0769753700000321.jpg\n",
            "./n0769753700000322.jpg\n",
            "./n0769753700000323.jpg\n",
            "./n0769753700000324.jpg\n",
            "./n0769753700000326.jpg\n",
            "./n0769753700000327.jpg\n",
            "./n0769753700000328.jpg\n",
            "./n0769753700000331.jpg\n",
            "./n0769753700000335.jpg\n",
            "./n0769753700000337.jpg\n",
            "./n0769753700000339.jpg\n",
            "./n0769753700000341.jpg\n",
            "./n0769753700000342.jpg\n",
            "./n0769753700000343.jpg\n",
            "./n0769753700000344.jpg\n",
            "./n0769753700000345.jpg\n",
            "./n0769753700000347.jpg\n",
            "./n0769753700000349.jpg\n",
            "./n0769753700000351.jpg\n",
            "./n0769753700000355.jpg\n",
            "./n0769753700000357.jpg\n",
            "./n0769753700000363.jpg\n",
            "./n0769753700000364.jpg\n",
            "./n0769753700000366.jpg\n",
            "./n0769753700000369.jpg\n",
            "./n0769753700000370.jpg\n",
            "./n0769753700000371.jpg\n",
            "./n0769753700000374.jpg\n",
            "./n0769753700000377.jpg\n",
            "./n0769753700000382.jpg\n",
            "./n0769753700000385.jpg\n",
            "./n0769753700000386.jpg\n",
            "./n0769753700000387.jpg\n",
            "./n0769753700000391.jpg\n",
            "./n0769753700000393.jpg\n",
            "./n0769753700000394.jpg\n",
            "./n0769753700000395.jpg\n",
            "./n0769753700000396.jpg\n",
            "./n0769753700000397.jpg\n",
            "./n0769753700000398.jpg\n",
            "./n0769753700000399.jpg\n",
            "./n0769753700000400.jpg\n",
            "./n0769753700000405.jpg\n",
            "./n0769753700000406.jpg\n",
            "./n0769753700000409.jpg\n",
            "./n0769753700000413.jpg\n",
            "./n0769753700000417.jpg\n",
            "./n0769753700000419.jpg\n",
            "./n0769753700000421.jpg\n",
            "./n0769753700000424.jpg\n",
            "./n0769753700000430.jpg\n",
            "./n0769753700000432.jpg\n",
            "./n0769753700000436.jpg\n",
            "./n0769753700000437.jpg\n",
            "./n0769753700000441.jpg\n",
            "./n0769753700000442.jpg\n",
            "./n0769753700000444.jpg\n",
            "./n0769753700000446.jpg\n",
            "./n0769753700000447.jpg\n",
            "./n0769753700000448.jpg\n",
            "./n0769753700000450.jpg\n",
            "./n0769753700000451.jpg\n",
            "./n0769753700000452.jpg\n",
            "./n0769753700000453.jpg\n",
            "./n0769753700000455.jpg\n",
            "./n0769753700000459.jpg\n",
            "./n0769753700000460.jpg\n",
            "./n0769753700000461.jpg\n",
            "./n0769753700000462.jpg\n",
            "./n0769753700000464.jpg\n",
            "./n0769753700000466.jpg\n",
            "./n0769753700000469.jpg\n",
            "./n0769753700000470.jpg\n",
            "./n0769753700000472.jpg\n",
            "./n0769753700000473.jpg\n",
            "./n0769753700000475.jpg\n",
            "./n0769753700000476.jpg\n",
            "./n0769753700000478.jpg\n",
            "./n0769753700000480.jpg\n",
            "./n0769753700000481.jpg\n",
            "./n0769753700000482.jpg\n",
            "./n0769753700000485.jpg\n",
            "./n0769753700000489.jpg\n",
            "./n0769753700000490.jpg\n",
            "./n0769753700000493.jpg\n",
            "./n0769753700000494.jpg\n",
            "./n0769753700000495.jpg\n",
            "./n0769753700000498.jpg\n",
            "./n0769753700000499.jpg\n",
            "./n0769753700000501.jpg\n",
            "./n0769753700000507.jpg\n",
            "./n0769753700000508.jpg\n",
            "./n0769753700000509.jpg\n",
            "./n0769753700000510.jpg\n",
            "./n0769753700000511.jpg\n",
            "./n0769753700000512.jpg\n",
            "./n0769753700000516.jpg\n",
            "./n0769753700000520.jpg\n",
            "./n0769753700000526.jpg\n",
            "./n0769753700000527.jpg\n",
            "./n0769753700000530.jpg\n",
            "./n0769753700000535.jpg\n",
            "./n0769753700000537.jpg\n",
            "./n0769753700000538.jpg\n",
            "./n0769753700000543.jpg\n",
            "./n0769753700000545.jpg\n",
            "./n0769753700000546.jpg\n",
            "./n0769753700000547.jpg\n",
            "./n0769753700000548.jpg\n",
            "./n0769753700000554.jpg\n",
            "./n0769753700000558.jpg\n",
            "./n0769753700000559.jpg\n",
            "./n0769753700000560.jpg\n",
            "./n0769753700000562.jpg\n",
            "./n0769753700000566.jpg\n",
            "./n0769753700000567.jpg\n",
            "./n0769753700000569.jpg\n",
            "./n0769753700000572.jpg\n",
            "./n0769753700000573.jpg\n",
            "./n0769753700000574.jpg\n",
            "./n0769753700000575.jpg\n",
            "./n0769753700000576.jpg\n",
            "./n0769753700000577.jpg\n",
            "./n0769753700000578.jpg\n",
            "./n0769753700000585.jpg\n",
            "./n0769753700000586.jpg\n",
            "./n0769753700000588.jpg\n",
            "./n0769753700000589.jpg\n",
            "./n0769753700000590.jpg\n",
            "./n0769753700000591.jpg\n",
            "./n0769753700000593.jpg\n",
            "./n0769753700000597.jpg\n",
            "./n0769753700000598.jpg\n",
            "./n0769753700000599.jpg\n",
            "./n0769753700000601.jpg\n",
            "./n0769753700000603.jpg\n",
            "./n0769753700000604.jpg\n",
            "./n0769753700000606.jpg\n",
            "./n0769753700000609.jpg\n",
            "./n0769753700000610.jpg\n",
            "./n0769753700000614.jpg\n",
            "./n0769753700000617.jpg\n",
            "./n0769753700000619.jpg\n",
            "./n0769753700000621.jpg\n",
            "./n0769753700000622.jpg\n",
            "./n0769753700000624.jpg\n",
            "./n0769753700000625.jpg\n",
            "./n0769753700000626.jpg\n",
            "./n0769753700000629.jpg\n",
            "./n0769753700000633.jpg\n",
            "./n0769753700000634.jpg\n",
            "./n0769753700000637.jpg\n",
            "./n0769753700000638.jpg\n",
            "./n0769753700000639.jpg\n",
            "./n0769753700000640.jpg\n",
            "./n0769753700000642.jpg\n",
            "./n0769753700000646.jpg\n",
            "./n0769753700000648.jpg\n",
            "./n0769753700000650.jpg\n",
            "./n0769753700000651.jpg\n",
            "./n0769753700000652.jpg\n",
            "./n0769753700000658.jpg\n",
            "./n0769753700000659.jpg\n",
            "./n0769753700000662.jpg\n",
            "./n0769753700000666.jpg\n",
            "./n0769753700000668.jpg\n",
            "./n0769753700000673.jpg\n",
            "./n0769753700000674.jpg\n",
            "./n0769753700000675.jpg\n",
            "./n0769753700000677.jpg\n",
            "./n0769753700000678.jpg\n",
            "./n0769753700000679.jpg\n",
            "./n0769753700000680.jpg\n",
            "./n0769753700000681.jpg\n",
            "./n0769753700000683.jpg\n",
            "./n0769753700000685.jpg\n",
            "./n0769753700000686.jpg\n",
            "./n0769753700000687.jpg\n",
            "./n0769753700000688.jpg\n",
            "./n0769753700000690.jpg\n",
            "./n0769753700000691.jpg\n",
            "./n0769753700000692.jpg\n",
            "./n0769753700000695.jpg\n",
            "./n0769753700000701.jpg\n",
            "./n0769753700000702.jpg\n",
            "./n0769753700000703.jpg\n",
            "./n0769753700000704.jpg\n",
            "./n0769753700000706.jpg\n",
            "./n0769753700000707.jpg\n",
            "./n0769753700000708.jpg\n",
            "./n0769753700000710.jpg\n",
            "./n0769753700000712.jpg\n",
            "./n0769753700000713.jpg\n",
            "./n0769753700000714.jpg\n",
            "./n0769753700000715.jpg\n",
            "./n0769753700000717.jpg\n",
            "./n0769753700000718.jpg\n",
            "./n0769753700000719.jpg\n",
            "./n0769753700000720.jpg\n",
            "./n0769753700000721.jpg\n",
            "./n0769753700000722.jpg\n",
            "./n0769753700000724.jpg\n",
            "./n0769753700000725.jpg\n",
            "./n0769753700000729.jpg\n",
            "./n0769753700000731.jpg\n",
            "./n0769753700000732.jpg\n",
            "./n0769753700000736.jpg\n",
            "./n0769753700000738.jpg\n",
            "./n0769753700000743.jpg\n",
            "./n0769753700000744.jpg\n",
            "./n0769753700000746.jpg\n",
            "./n0769753700000748.jpg\n",
            "./n0769753700000749.jpg\n",
            "./n0769753700000756.jpg\n",
            "./n0769753700000759.jpg\n",
            "./n0769753700000760.jpg\n",
            "./n0769753700000765.jpg\n",
            "./n0769753700000766.jpg\n",
            "./n0769753700000769.jpg\n",
            "./n0769753700000770.jpg\n",
            "./n0769753700000772.jpg\n",
            "./n0769753700000774.jpg\n",
            "./n0769753700000776.jpg\n",
            "./n0769753700000778.jpg\n",
            "./n0769753700000779.jpg\n",
            "./n0769753700000781.jpg\n",
            "./n0769753700000783.jpg\n",
            "./n0769753700000785.jpg\n",
            "./n0769753700000787.jpg\n",
            "./n0769753700000790.jpg\n",
            "./n0769753700000793.jpg\n",
            "./n0769753700000794.jpg\n",
            "./n0769753700000795.jpg\n",
            "./n0769753700000796.jpg\n",
            "./n0769753700000799.jpg\n",
            "./n0769753700000801.jpg\n",
            "./n0769753700000802.jpg\n",
            "./n0769753700000805.jpg\n",
            "./n0769753700000810.jpg\n",
            "./n0769753700000813.jpg\n",
            "./n0769753700000815.jpg\n",
            "./n0769753700000820.jpg\n",
            "./n0769753700000821.jpg\n",
            "./n0769753700000823.jpg\n",
            "./n0769753700000826.jpg\n",
            "./n0769753700000829.jpg\n",
            "./n0769753700000831.jpg\n",
            "./n0769753700000833.jpg\n",
            "./n0769753700000834.jpg\n",
            "./n0769753700000838.jpg\n",
            "./n0769753700000839.jpg\n",
            "./n0769753700000840.jpg\n",
            "./n0769753700000843.jpg\n",
            "./n0769753700000844.jpg\n",
            "./n0769753700000849.jpg\n",
            "./n0769753700000850.jpg\n",
            "./n0769753700000853.jpg\n",
            "./n0769753700000855.jpg\n",
            "./n0769753700000857.jpg\n",
            "./n0769753700000860.jpg\n",
            "./n0769753700000861.jpg\n",
            "./n0769753700000862.jpg\n",
            "./n0769753700000863.jpg\n",
            "./n0769753700000864.jpg\n",
            "./n0769753700000867.jpg\n",
            "./n0769753700000870.jpg\n",
            "./n0769753700000871.jpg\n",
            "./n0769753700000872.jpg\n",
            "./n0769753700000873.jpg\n",
            "./n0769753700000883.jpg\n",
            "./n0769753700000886.jpg\n",
            "./n0769753700000887.jpg\n",
            "./n0769753700000888.jpg\n",
            "./n0769753700000890.jpg\n",
            "./n0769753700000891.jpg\n",
            "./n0769753700000892.jpg\n",
            "./n0769753700000894.jpg\n",
            "./n0769753700000895.jpg\n",
            "./n0769753700000896.jpg\n",
            "./n0769753700000901.jpg\n",
            "./n0769753700000902.jpg\n",
            "./n0769753700000903.jpg\n",
            "./n0769753700000906.jpg\n",
            "./n0769753700000908.jpg\n",
            "./n0769753700000909.jpg\n",
            "./n0769753700000910.jpg\n",
            "./n0769753700000911.jpg\n",
            "./n0769753700000915.jpg\n",
            "./n0769753700000918.jpg\n",
            "./n0769753700000921.jpg\n",
            "./n0769753700000923.jpg\n",
            "./n0769753700000924.jpg\n",
            "./n0769753700000925.jpg\n",
            "./n0769753700000927.jpg\n",
            "./n0769753700000928.jpg\n",
            "./n0769753700000929.jpg\n",
            "./n0769753700000930.jpg\n",
            "./n0769753700000932.jpg\n",
            "./n0769753700000934.jpg\n",
            "./n0769753700000938.jpg\n",
            "./n0769753700000940.jpg\n",
            "./n0769753700000945.jpg\n",
            "./n0769753700000947.jpg\n",
            "./n0769753700000951.jpg\n",
            "./n0769753700000952.jpg\n",
            "./n0769753700000956.jpg\n",
            "./n0769753700000963.jpg\n",
            "./n0769753700000966.jpg\n",
            "./n0769753700000968.jpg\n",
            "./n0769753700000969.jpg\n",
            "./n0769753700000971.jpg\n",
            "./n0769753700000972.jpg\n",
            "./n0769753700000973.jpg\n",
            "./n0769753700000978.jpg\n",
            "./n0769753700000981.jpg\n",
            "./n0769753700000982.jpg\n",
            "./n0769753700000983.jpg\n",
            "./n0769753700000990.jpg\n",
            "./n0769753700000991.jpg\n",
            "./n0769753700000992.jpg\n",
            "./n0769753700000994.jpg\n",
            "./n0769753700000999.jpg\n",
            "./n0769753700001000.jpg\n",
            "./n0769753700001001.jpg\n",
            "./n0769753700001002.jpg\n",
            "./n0769753700001005.jpg\n",
            "./n0769753700001008.jpg\n",
            "./n0769753700001011.jpg\n",
            "./n0769753700001012.jpg\n",
            "./n0769753700001021.jpg\n",
            "./n0769753700001031.jpg\n",
            "./n0769753700001033.jpg\n",
            "./n0769753700001035.jpg\n",
            "./n0769753700001036.jpg\n",
            "./n0769753700001037.jpg\n",
            "./n0769753700001038.jpg\n",
            "./n0769753700001044.jpg\n",
            "./n0769753700001045.jpg\n",
            "./n0769753700001048.jpg\n",
            "./n0769753700001049.jpg\n",
            "./n0769753700001051.jpg\n",
            "./n0769753700001052.jpg\n",
            "./n0769753700001053.jpg\n",
            "./n0769753700001054.jpg\n",
            "./n0769753700001057.jpg\n",
            "./n0769753700001062.jpg\n",
            "./n0769753700001065.jpg\n",
            "./n0769753700001068.jpg\n",
            "./n0769753700001070.jpg\n",
            "./n0769753700001071.jpg\n",
            "./n0769753700001074.jpg\n",
            "./n0769753700001076.jpg\n",
            "./n0769753700001077.jpg\n",
            "./n0769753700001081.jpg\n",
            "./n0769753700001082.jpg\n",
            "./n0769753700001083.jpg\n",
            "./n0769753700001087.jpg\n",
            "./n0769753700001088.jpg\n",
            "./n0769753700001090.jpg\n",
            "./n0769753700001092.jpg\n",
            "./n0769753700001093.jpg\n",
            "./n0769753700001094.jpg\n",
            "./n0769753700001095.jpg\n",
            "./n0769753700001096.jpg\n",
            "./n0769753700001099.jpg\n",
            "./n0769753700001100.jpg\n",
            "./n0769753700001103.jpg\n",
            "./n0769753700001104.jpg\n",
            "./n0769753700001106.jpg\n",
            "./n0769753700001107.jpg\n",
            "./n0769753700001108.jpg\n",
            "./n0769753700001109.jpg\n",
            "./n0769753700001111.jpg\n",
            "./n0769753700001113.jpg\n",
            "./n0769753700001116.jpg\n",
            "./n0769753700001117.jpg\n",
            "./n0769753700001119.jpg\n",
            "./n0769753700001121.jpg\n",
            "./n0769753700001125.jpg\n",
            "./n0769753700001127.jpg\n",
            "./n0769753700001130.jpg\n",
            "./n0769753700001133.jpg\n",
            "./n0769753700001135.jpg\n",
            "./n0769753700001136.jpg\n",
            "./n0769753700001137.jpg\n",
            "./n0769753700001138.jpg\n",
            "./n0769753700001141.jpg\n",
            "./n0769753700001143.jpg\n",
            "./n0769753700001144.jpg\n",
            "./n0769753700001146.jpg\n",
            "./n0769753700001149.jpg\n",
            "./n0769753700001151.jpg\n",
            "./n0769753700001153.jpg\n",
            "./n0769753700001154.jpg\n",
            "./n0769753700001155.jpg\n",
            "./n0769753700001159.jpg\n",
            "./n0769753700001160.jpg\n",
            "./n0769753700001161.jpg\n",
            "./n0769753700001163.jpg\n",
            "./n0769753700001164.jpg\n",
            "./n0769753700001165.jpg\n",
            "./n0769753700001167.jpg\n",
            "./n0769753700001170.jpg\n",
            "./n0769753700001171.jpg\n",
            "./n0769753700001172.jpg\n",
            "./n0769753700001173.jpg\n",
            "./n0769753700001174.jpg\n",
            "./n0769753700001175.jpg\n",
            "./n0769753700001179.jpg\n",
            "./n0769753700001180.jpg\n",
            "./n0769753700001182.jpg\n",
            "./n0769753700001184.jpg\n",
            "./n0769753700001186.jpg\n",
            "./n0769753700001189.jpg\n",
            "./n0769753700001190.jpg\n",
            "./n0769753700001193.jpg\n",
            "./n0769753700001194.jpg\n",
            "./n0769753700001199.jpg\n",
            "./n0769753700001200.jpg\n",
            "./n0769753700001202.jpg\n",
            "./n0769753700001203.jpg\n",
            "./n0769753700001208.jpg\n",
            "./n0769753700001209.jpg\n",
            "./n0769753700001210.jpg\n",
            "./n0769753700001214.jpg\n",
            "./n0769753700001219.jpg\n",
            "./n0769753700001221.jpg\n",
            "./n0769753700001222.jpg\n",
            "./n0769753700001224.jpg\n",
            "./n0769753700001225.jpg\n",
            "./n0769753700001227.jpg\n",
            "./n0769753700001228.jpg\n",
            "./n0769753700001231.jpg\n",
            "./n0769753700001234.jpg\n",
            "./n0769753700001239.jpg\n",
            "./n0769753700001240.jpg\n",
            "./n0769753700001242.jpg\n",
            "./n0769753700001248.jpg\n",
            "./n0769753700001249.jpg\n",
            "./n0769753700001254.jpg\n",
            "./n0769753700001255.jpg\n",
            "./n0769753700001256.jpg\n",
            "./n0769753700001260.jpg\n",
            "./n0769753700001263.jpg\n",
            "./n0769753700001265.jpg\n",
            "./n0769753700001268.jpg\n",
            "./n0769753700001283.jpg\n",
            "./n0769753700001284.jpg\n",
            "./n0769753700001285.jpg\n",
            "./n0769753700001286.jpg\n",
            "./n0769753700001287.jpg\n",
            "./n0769753700001288.jpg\n",
            "./n0769753700001290.jpg\n",
            "./n0769753700001291.jpg\n",
            "./n0769753700001294.jpg\n",
            "./n0769753700001295.jpg\n",
            "./n0769753700001296.jpg\n",
            "./n0769753700001298.jpg\n",
            "./n0769753700001299.jpg\n",
            "./n0769753700001300.jpg\n",
            "./n0774760700000001.jpg\n",
            "./n0774760700000006.jpg\n",
            "./n0774760700000007.jpg\n",
            "./n0774760700000008.jpg\n",
            "./n0774760700000012.jpg\n",
            "./n0774760700000014.jpg\n",
            "./n0774760700000015.jpg\n",
            "./n0774760700000017.jpg\n",
            "./n0774760700000019.jpg\n",
            "./n0774760700000023.jpg\n",
            "./n0774760700000026.jpg\n",
            "./n0774760700000030.jpg\n",
            "./n0774760700000031.jpg\n",
            "./n0774760700000032.jpg\n",
            "./n0774760700000033.jpg\n",
            "./n0774760700000034.jpg\n",
            "./n0774760700000037.jpg\n",
            "./n0774760700000039.jpg\n",
            "./n0774760700000042.jpg\n",
            "./n0774760700000047.jpg\n",
            "./n0774760700000049.jpg\n",
            "./n0774760700000053.jpg\n",
            "./n0774760700000057.jpg\n",
            "./n0774760700000059.jpg\n",
            "./n0774760700000060.jpg\n",
            "./n0774760700000062.jpg\n",
            "./n0774760700000064.jpg\n",
            "./n0774760700000065.jpg\n",
            "./n0774760700000066.jpg\n",
            "./n0774760700000067.jpg\n",
            "./n0774760700000071.jpg\n",
            "./n0774760700000073.jpg\n",
            "./n0774760700000076.jpg\n",
            "./n0774760700000082.jpg\n",
            "./n0774760700000083.jpg\n",
            "./n0774760700000087.jpg\n",
            "./n0774760700000088.jpg\n",
            "./n0774760700000090.jpg\n",
            "./n0774760700000091.jpg\n",
            "./n0774760700000094.jpg\n",
            "./n0774760700000098.jpg\n",
            "./n0774760700000102.jpg\n",
            "./n0774760700000103.jpg\n",
            "./n0774760700000104.jpg\n",
            "./n0774760700000105.jpg\n",
            "./n0774760700000107.jpg\n",
            "./n0774760700000111.jpg\n",
            "./n0774760700000113.jpg\n",
            "./n0774760700000117.jpg\n",
            "./n0774760700000119.jpg\n",
            "./n0774760700000120.jpg\n",
            "./n0774760700000122.jpg\n",
            "./n0774760700000125.jpg\n",
            "./n0774760700000128.jpg\n",
            "./n0774760700000129.jpg\n",
            "./n0774760700000131.jpg\n",
            "./n0774760700000132.jpg\n",
            "./n0774760700000133.jpg\n",
            "./n0774760700000136.jpg\n",
            "./n0774760700000140.jpg\n",
            "./n0774760700000143.jpg\n",
            "./n0774760700000145.jpg\n",
            "./n0774760700000150.jpg\n",
            "./n0774760700000153.jpg\n",
            "./n0774760700000157.jpg\n",
            "./n0774760700000158.jpg\n",
            "./n0774760700000159.jpg\n",
            "./n0774760700000160.jpg\n",
            "./n0774760700000163.jpg\n",
            "./n0774760700000164.jpg\n",
            "./n0774760700000165.jpg\n",
            "./n0774760700000166.jpg\n",
            "./n0774760700000167.jpg\n",
            "./n0774760700000171.jpg\n",
            "./n0774760700000172.jpg\n",
            "./n0774760700000173.jpg\n",
            "./n0774760700000178.jpg\n",
            "./n0774760700000179.jpg\n",
            "./n0774760700000182.jpg\n",
            "./n0774760700000184.jpg\n",
            "./n0774760700000187.jpg\n",
            "./n0774760700000188.jpg\n",
            "./n0774760700000190.jpg\n",
            "./n0774760700000193.jpg\n",
            "./n0774760700000196.jpg\n",
            "./n0774760700000198.jpg\n",
            "./n0774760700000199.jpg\n",
            "./n0774760700000200.jpg\n",
            "./n0774760700000201.jpg\n",
            "./n0774760700000207.jpg\n",
            "./n0774760700000209.jpg\n",
            "./n0774760700000210.jpg\n",
            "./n0774760700000215.jpg\n",
            "./n0774760700000216.jpg\n",
            "./n0774760700000217.jpg\n",
            "./n0774760700000219.jpg\n",
            "./n0774760700000226.jpg\n",
            "./n0774760700000230.jpg\n",
            "./n0774760700000234.jpg\n",
            "./n0774760700000236.jpg\n",
            "./n0774760700000237.jpg\n",
            "./n0774760700000238.jpg\n",
            "./n0774760700000244.jpg\n",
            "./n0774760700000246.jpg\n",
            "./n0774760700000249.jpg\n",
            "./n0774760700000252.jpg\n",
            "./n0774760700000253.jpg\n",
            "./n0774760700000254.jpg\n",
            "./n0774760700000256.jpg\n",
            "./n0774760700000258.jpg\n",
            "./n0774760700000260.jpg\n",
            "./n0774760700000262.jpg\n",
            "./n0774760700000264.jpg\n",
            "./n0774760700000266.jpg\n",
            "./n0774760700000268.jpg\n",
            "./n0774760700000269.jpg\n",
            "./n0774760700000273.jpg\n",
            "./n0774760700000275.jpg\n",
            "./n0774760700000276.jpg\n",
            "./n0774760700000277.jpg\n",
            "./n0774760700000278.jpg\n",
            "./n0774760700000280.jpg\n",
            "./n0774760700000281.jpg\n",
            "./n0774760700000283.jpg\n",
            "./n0774760700000285.jpg\n",
            "./n0774760700000287.jpg\n",
            "./n0774760700000289.jpg\n",
            "./n0774760700000291.jpg\n",
            "./n0774760700000292.jpg\n",
            "./n0774760700000295.jpg\n",
            "./n0774760700000296.jpg\n",
            "./n0774760700000297.jpg\n",
            "./n0774760700000298.jpg\n",
            "./n0774760700000299.jpg\n",
            "./n0774760700000305.jpg\n",
            "./n0774760700000306.jpg\n",
            "./n0774760700000309.jpg\n",
            "./n0774760700000311.jpg\n",
            "./n0774760700000312.jpg\n",
            "./n0774760700000314.jpg\n",
            "./n0774760700000315.jpg\n",
            "./n0774760700000317.jpg\n",
            "./n0774760700000318.jpg\n",
            "./n0774760700000320.jpg\n",
            "./n0774760700000321.jpg\n",
            "./n0774760700000325.jpg\n",
            "./n0774760700000326.jpg\n",
            "./n0774760700000327.jpg\n",
            "./n0774760700000328.jpg\n",
            "./n0774760700000330.jpg\n",
            "./n0774760700000334.jpg\n",
            "./n0774760700000336.jpg\n",
            "./n0774760700000345.jpg\n",
            "./n0774760700000349.jpg\n",
            "./n0774760700000352.jpg\n",
            "./n0774760700000357.jpg\n",
            "./n0774760700000358.jpg\n",
            "./n0774760700000359.jpg\n",
            "./n0774760700000365.jpg\n",
            "./n0774760700000369.jpg\n",
            "./n0774760700000370.jpg\n",
            "./n0774760700000373.jpg\n",
            "./n0774760700000374.jpg\n",
            "./n0774760700000375.jpg\n",
            "./n0774760700000377.jpg\n",
            "./n0774760700000378.jpg\n",
            "./n0774760700000383.jpg\n",
            "./n0774760700000384.jpg\n",
            "./n0774760700000388.jpg\n",
            "./n0774760700000390.jpg\n",
            "./n0774760700000394.jpg\n",
            "./n0774760700000395.jpg\n",
            "./n0774760700000399.jpg\n",
            "./n0774760700000400.jpg\n",
            "./n0774760700000401.jpg\n",
            "./n0774760700000405.jpg\n",
            "./n0774760700000406.jpg\n",
            "./n0774760700000410.jpg\n",
            "./n0774760700000411.jpg\n",
            "./n0774760700000413.jpg\n",
            "./n0774760700000415.jpg\n",
            "./n0774760700000416.jpg\n",
            "./n0774760700000419.jpg\n",
            "./n0774760700000421.jpg\n",
            "./n0774760700000422.jpg\n",
            "./n0774760700000423.jpg\n",
            "./n0774760700000424.jpg\n",
            "./n0774760700000425.jpg\n",
            "./n0774760700000426.jpg\n",
            "./n0774760700000429.jpg\n",
            "./n0774760700000431.jpg\n",
            "./n0774760700000432.jpg\n",
            "./n0774760700000433.jpg\n",
            "./n0774760700000435.jpg\n",
            "./n0774760700000440.jpg\n",
            "./n0774760700000441.jpg\n",
            "./n0774760700000443.jpg\n",
            "./n0774760700000444.jpg\n",
            "./n0774760700000445.jpg\n",
            "./n0774760700000447.jpg\n",
            "./n0774760700000448.jpg\n",
            "./n0774760700000449.jpg\n",
            "./n0774760700000450.jpg\n",
            "./n0774760700000451.jpg\n",
            "./n0774760700000452.jpg\n",
            "./n0774760700000453.jpg\n",
            "./n0774760700000454.jpg\n",
            "./n0774760700000455.jpg\n",
            "./n0774760700000457.jpg\n",
            "./n0774760700000458.jpg\n",
            "./n0774760700000460.jpg\n",
            "./n0774760700000461.jpg\n",
            "./n0774760700000462.jpg\n",
            "./n0774760700000469.jpg\n",
            "./n0774760700000471.jpg\n",
            "./n0774760700000472.jpg\n",
            "./n0774760700000474.jpg\n",
            "./n0774760700000475.jpg\n",
            "./n0774760700000479.jpg\n",
            "./n0774760700000481.jpg\n",
            "./n0774760700000482.jpg\n",
            "./n0774760700000485.jpg\n",
            "./n0774760700000487.jpg\n",
            "./n0774760700000488.jpg\n",
            "./n0774760700000489.jpg\n",
            "./n0774760700000492.jpg\n",
            "./n0774760700000493.jpg\n",
            "./n0774760700000494.jpg\n",
            "./n0774760700000496.jpg\n",
            "./n0774760700000497.jpg\n",
            "./n0774760700000499.jpg\n",
            "./n0774760700000502.jpg\n",
            "./n0774760700000503.jpg\n",
            "./n0774760700000506.jpg\n",
            "./n0774760700000507.jpg\n",
            "./n0774760700000508.jpg\n",
            "./n0774760700000510.jpg\n",
            "./n0774760700000512.jpg\n",
            "./n0774760700000513.jpg\n",
            "./n0774760700000517.jpg\n",
            "./n0774760700000518.jpg\n",
            "./n0774760700000519.jpg\n",
            "./n0774760700000521.jpg\n",
            "./n0774760700000522.jpg\n",
            "./n0774760700000524.jpg\n",
            "./n0774760700000525.jpg\n",
            "./n0774760700000527.jpg\n",
            "./n0774760700000529.jpg\n",
            "./n0774760700000532.jpg\n",
            "./n0774760700000534.jpg\n",
            "./n0774760700000536.jpg\n",
            "./n0774760700000537.jpg\n",
            "./n0774760700000539.jpg\n",
            "./n0774760700000540.jpg\n",
            "./n0774760700000543.jpg\n",
            "./n0774760700000545.jpg\n",
            "./n0774760700000547.jpg\n",
            "./n0774760700000549.jpg\n",
            "./n0774760700000550.jpg\n",
            "./n0774760700000553.jpg\n",
            "./n0774760700000554.jpg\n",
            "./n0774760700000556.jpg\n",
            "./n0774760700000557.jpg\n",
            "./n0774760700000562.jpg\n",
            "./n0774760700000564.jpg\n",
            "./n0774760700000566.jpg\n",
            "./n0774760700000567.jpg\n",
            "./n0774760700000572.jpg\n",
            "./n0774760700000573.jpg\n",
            "./n0774760700000574.jpg\n",
            "./n0774760700000576.jpg\n",
            "./n0774760700000577.jpg\n",
            "./n0774760700000578.jpg\n",
            "./n0774760700000579.jpg\n",
            "./n0774760700000580.jpg\n",
            "./n0774760700000583.jpg\n",
            "./n0774760700000585.jpg\n",
            "./n0774760700000588.jpg\n",
            "./n0774760700000590.jpg\n",
            "./n0774760700000591.jpg\n",
            "./n0774760700000594.jpg\n",
            "./n0774760700000595.jpg\n",
            "./n0774760700000599.jpg\n",
            "./n0774760700000601.jpg\n",
            "./n0774760700000603.jpg\n",
            "./n0774760700000612.jpg\n",
            "./n0774760700000615.jpg\n",
            "./n0774760700000616.jpg\n",
            "./n0774760700000617.jpg\n",
            "./n0774760700000618.jpg\n",
            "./n0774760700000620.jpg\n",
            "./n0774760700000621.jpg\n",
            "./n0774760700000622.jpg\n",
            "./n0774760700000630.jpg\n",
            "./n0774760700000634.jpg\n",
            "./n0774760700000635.jpg\n",
            "./n0774760700000636.jpg\n",
            "./n0774760700000637.jpg\n",
            "./n0774760700000640.jpg\n",
            "./n0774760700000641.jpg\n",
            "./n0774760700000643.jpg\n",
            "./n0774760700000644.jpg\n",
            "./n0774760700000648.jpg\n",
            "./n0774760700000649.jpg\n",
            "./n0774760700000651.jpg\n",
            "./n0774760700000652.jpg\n",
            "./n0774760700000653.jpg\n",
            "./n0774760700000660.jpg\n",
            "./n0774760700000661.jpg\n",
            "./n0774760700000668.jpg\n",
            "./n0774760700000672.jpg\n",
            "./n0774760700000675.jpg\n",
            "./n0774760700000678.jpg\n",
            "./n0774760700000680.jpg\n",
            "./n0774760700000681.jpg\n",
            "./n0774760700000684.jpg\n",
            "./n0774760700000686.jpg\n",
            "./n0774760700000689.jpg\n",
            "./n0774760700000691.jpg\n",
            "./n0774760700000693.jpg\n",
            "./n0774760700000695.jpg\n",
            "./n0774760700000696.jpg\n",
            "./n0774760700000699.jpg\n",
            "./n0774760700000705.jpg\n",
            "./n0774760700000710.jpg\n",
            "./n0774760700000712.jpg\n",
            "./n0774760700000714.jpg\n",
            "./n0774760700000716.jpg\n",
            "./n0774760700000717.jpg\n",
            "./n0774760700000719.jpg\n",
            "./n0774760700000720.jpg\n",
            "./n0774760700000722.jpg\n",
            "./n0774760700000723.jpg\n",
            "./n0774760700000724.jpg\n",
            "./n0774760700000726.jpg\n",
            "./n0774760700000727.jpg\n",
            "./n0774760700000728.jpg\n",
            "./n0774760700000729.jpg\n",
            "./n0774760700000731.jpg\n",
            "./n0774760700000736.jpg\n",
            "./n0774760700000737.jpg\n",
            "./n0774760700000739.jpg\n",
            "./n0774760700000740.jpg\n",
            "./n0774760700000742.jpg\n",
            "./n0774760700000748.jpg\n",
            "./n0774760700000750.jpg\n",
            "./n0774760700000751.jpg\n",
            "./n0774760700000752.jpg\n",
            "./n0774760700000754.jpg\n",
            "./n0774760700000755.jpg\n",
            "./n0774760700000756.jpg\n",
            "./n0774760700000757.jpg\n",
            "./n0774760700000758.jpg\n",
            "./n0774760700000759.jpg\n",
            "./n0774760700000764.jpg\n",
            "./n0774760700000766.jpg\n",
            "./n0774760700000767.jpg\n",
            "./n0774760700000768.jpg\n",
            "./n0774760700000770.jpg\n",
            "./n0774760700000772.jpg\n",
            "./n0774760700000773.jpg\n",
            "./n0774760700000776.jpg\n",
            "./n0774760700000779.jpg\n",
            "./n0774760700000783.jpg\n",
            "./n0774760700000785.jpg\n",
            "./n0774760700000786.jpg\n",
            "./n0774760700000787.jpg\n",
            "./n0774760700000788.jpg\n",
            "./n0774760700000789.jpg\n",
            "./n0774760700000790.jpg\n",
            "./n0774760700000791.jpg\n",
            "./n0774760700000794.jpg\n",
            "./n0774760700000800.jpg\n",
            "./n0774760700000801.jpg\n",
            "./n0774760700000803.jpg\n",
            "./n0774760700000808.jpg\n",
            "./n0774760700000810.jpg\n",
            "./n0774760700000813.jpg\n",
            "./n0774760700000817.jpg\n",
            "./n0774760700000819.jpg\n",
            "./n0774760700000822.jpg\n",
            "./n0774760700000823.jpg\n",
            "./n0774760700000824.jpg\n",
            "./n0774760700000825.jpg\n",
            "./n0774760700000826.jpg\n",
            "./n0774760700000827.jpg\n",
            "./n0774760700000829.jpg\n",
            "./n0774760700000830.jpg\n",
            "./n0774760700000832.jpg\n",
            "./n0774760700000834.jpg\n",
            "./n0774760700000835.jpg\n",
            "./n0774760700000836.jpg\n",
            "./n0774760700000837.jpg\n",
            "./n0774760700000838.jpg\n",
            "./n0774760700000839.jpg\n",
            "./n0774760700000841.jpg\n",
            "./n0774760700000848.jpg\n",
            "./n0774760700000851.jpg\n",
            "./n0774760700000852.jpg\n",
            "./n0774760700000853.jpg\n",
            "./n0774760700000855.jpg\n",
            "./n0774760700000856.jpg\n",
            "./n0774760700000857.jpg\n",
            "./n0774760700000858.jpg\n",
            "./n0774760700000861.jpg\n",
            "./n0774760700000862.jpg\n",
            "./n0774760700000869.jpg\n",
            "./n0774760700000870.jpg\n",
            "./n0774760700000875.jpg\n",
            "./n0774760700000876.jpg\n",
            "./n0774760700000879.jpg\n",
            "./n0774760700000881.jpg\n",
            "./n0774760700000888.jpg\n",
            "./n0774760700000889.jpg\n",
            "./n0774760700000890.jpg\n",
            "./n0774760700000894.jpg\n",
            "./n0774760700000899.jpg\n",
            "./n0774760700000901.jpg\n",
            "./n0774760700000904.jpg\n",
            "./n0774760700000906.jpg\n",
            "./n0774760700000908.jpg\n",
            "./n0774760700000909.jpg\n",
            "./n0774760700000912.jpg\n",
            "./n0774760700000913.jpg\n",
            "./n0774760700000914.jpg\n",
            "./n0774760700000915.jpg\n",
            "./n0774760700000918.jpg\n",
            "./n0774760700000920.jpg\n",
            "./n0774760700000921.jpg\n",
            "./n0774760700000927.jpg\n",
            "./n0774760700000928.jpg\n",
            "./n0774760700000929.jpg\n",
            "./n0774760700000930.jpg\n",
            "./n0774760700000931.jpg\n",
            "./n0774760700000932.jpg\n",
            "./n0774760700000933.jpg\n",
            "./n0774760700000935.jpg\n",
            "./n0774760700000936.jpg\n",
            "./n0774760700000937.jpg\n",
            "./n0774760700000938.jpg\n",
            "./n0774760700000939.jpg\n",
            "./n0774760700000940.jpg\n",
            "./n0774760700000945.jpg\n",
            "./n0774760700000948.jpg\n",
            "./n0774760700000958.jpg\n",
            "./n0774760700000959.jpg\n",
            "./n0774760700000961.jpg\n",
            "./n0774760700000962.jpg\n",
            "./n0774760700000966.jpg\n",
            "./n0774760700000971.jpg\n",
            "./n0774760700000974.jpg\n",
            "./n0774760700000975.jpg\n",
            "./n0774760700000978.jpg\n",
            "./n0774760700000982.jpg\n",
            "./n0774760700000984.jpg\n",
            "./n0774760700000988.jpg\n",
            "./n0774760700000989.jpg\n",
            "./n0774760700000990.jpg\n",
            "./n0774760700000991.jpg\n",
            "./n0774760700000992.jpg\n",
            "./n0774760700000993.jpg\n",
            "./n0774760700000994.jpg\n",
            "./n0774760700000996.jpg\n",
            "./n0774760700001000.jpg\n",
            "./n0774760700001001.jpg\n",
            "./n0774760700001004.jpg\n",
            "./n0774760700001007.jpg\n",
            "./n0774760700001010.jpg\n",
            "./n0774760700001012.jpg\n",
            "./n0774760700001013.jpg\n",
            "./n0774760700001017.jpg\n",
            "./n0774760700001022.jpg\n",
            "./n0774760700001024.jpg\n",
            "./n0774760700001026.jpg\n",
            "./n0774760700001027.jpg\n",
            "./n0774760700001028.jpg\n",
            "./n0774760700001032.jpg\n",
            "./n0774760700001033.jpg\n",
            "./n0774760700001037.jpg\n",
            "./n0774760700001038.jpg\n",
            "./n0774760700001040.jpg\n",
            "./n0774760700001041.jpg\n",
            "./n0774760700001046.jpg\n",
            "./n0774760700001048.jpg\n",
            "./n0774760700001054.jpg\n",
            "./n0774760700001055.jpg\n",
            "./n0774760700001058.jpg\n",
            "./n0774760700001061.jpg\n",
            "./n0774760700001062.jpg\n",
            "./n0774760700001064.jpg\n",
            "./n0774760700001067.jpg\n",
            "./n0774760700001068.jpg\n",
            "./n0774760700001069.jpg\n",
            "./n0774760700001071.jpg\n",
            "./n0774760700001073.jpg\n",
            "./n0774760700001077.jpg\n",
            "./n0774760700001079.jpg\n",
            "./n0774760700001082.jpg\n",
            "./n0774760700001083.jpg\n",
            "./n0774760700001084.jpg\n",
            "./n0774760700001086.jpg\n",
            "./n0774760700001087.jpg\n",
            "./n0774760700001088.jpg\n",
            "./n0774760700001090.jpg\n",
            "./n0774760700001092.jpg\n",
            "./n0774760700001093.jpg\n",
            "./n0774760700001094.jpg\n",
            "./n0774760700001095.jpg\n",
            "./n0774760700001097.jpg\n",
            "./n0774760700001102.jpg\n",
            "./n0774760700001104.jpg\n",
            "./n0774760700001105.jpg\n",
            "./n0774760700001106.jpg\n",
            "./n0774760700001107.jpg\n",
            "./n0774760700001109.jpg\n",
            "./n0774760700001110.jpg\n",
            "./n0774760700001111.jpg\n",
            "./n0774760700001112.jpg\n",
            "./n0774760700001113.jpg\n",
            "./n0774760700001117.jpg\n",
            "./n0774760700001121.jpg\n",
            "./n0774760700001122.jpg\n",
            "./n0774760700001124.jpg\n",
            "./n0774760700001126.jpg\n",
            "./n0774760700001129.jpg\n",
            "./n0774760700001130.jpg\n",
            "./n0774760700001133.jpg\n",
            "./n0774760700001136.jpg\n",
            "./n0774760700001141.jpg\n",
            "./n0774760700001142.jpg\n",
            "./n0774760700001144.jpg\n",
            "./n0774760700001146.jpg\n",
            "./n0774760700001148.jpg\n",
            "./n0774760700001149.jpg\n",
            "./n0774760700001150.jpg\n",
            "./n0774760700001152.jpg\n",
            "./n0774760700001153.jpg\n",
            "./n0774760700001157.jpg\n",
            "./n0774760700001158.jpg\n",
            "./n0774760700001160.jpg\n",
            "./n0774760700001167.jpg\n",
            "./n0774760700001169.jpg\n",
            "./n0774760700001170.jpg\n",
            "./n0774760700001171.jpg\n",
            "./n0774760700001174.jpg\n",
            "./n0774760700001176.jpg\n",
            "./n0774760700001181.jpg\n",
            "./n0774760700001184.jpg\n",
            "./n0774760700001186.jpg\n",
            "./n0774760700001192.jpg\n",
            "./n0774760700001196.jpg\n",
            "./n0774760700001197.jpg\n",
            "./n0774760700001202.jpg\n",
            "./n0774760700001203.jpg\n",
            "./n0774760700001204.jpg\n",
            "./n0774760700001205.jpg\n",
            "./n0774760700001208.jpg\n",
            "./n0774760700001209.jpg\n",
            "./n0774760700001212.jpg\n",
            "./n0774760700001216.jpg\n",
            "./n0774760700001217.jpg\n",
            "./n0774760700001218.jpg\n",
            "./n0774760700001220.jpg\n",
            "./n0774760700001221.jpg\n",
            "./n0774760700001223.jpg\n",
            "./n0774760700001225.jpg\n",
            "./n0774760700001228.jpg\n",
            "./n0774760700001231.jpg\n",
            "./n0774760700001232.jpg\n",
            "./n0774760700001233.jpg\n",
            "./n0774760700001234.jpg\n",
            "./n0774760700001235.jpg\n",
            "./n0774760700001245.jpg\n",
            "./n0774760700001247.jpg\n",
            "./n0774760700001248.jpg\n",
            "./n0774760700001253.jpg\n",
            "./n0774760700001254.jpg\n",
            "./n0774760700001255.jpg\n",
            "./n0774760700001258.jpg\n",
            "./n0774760700001259.jpg\n",
            "./n0774760700001263.jpg\n",
            "./n0774760700001264.jpg\n",
            "./n0774760700001266.jpg\n",
            "./n0774760700001268.jpg\n",
            "./n0774760700001269.jpg\n",
            "./n0774760700001274.jpg\n",
            "./n0774760700001275.jpg\n",
            "./n0774760700001276.jpg\n",
            "./n0774760700001278.jpg\n",
            "./n0774760700001279.jpg\n",
            "./n0774760700001280.jpg\n",
            "./n0774760700001282.jpg\n",
            "./n0774760700001283.jpg\n",
            "./n0774760700001284.jpg\n",
            "./n0774760700001289.jpg\n",
            "./n0774760700001290.jpg\n",
            "./n0774760700001291.jpg\n",
            "./n0774760700001297.jpg\n",
            "./n0774760700001298.jpg\n",
            "./n0774760700001299.jpg\n",
            "./n0924646400000004.jpg\n",
            "./n0924646400000005.jpg\n",
            "./n0924646400000006.jpg\n",
            "./n0924646400000007.jpg\n",
            "./n0924646400000008.jpg\n",
            "./n0924646400000009.jpg\n",
            "./n0924646400000010.jpg\n",
            "./n0924646400000011.jpg\n",
            "./n0924646400000015.jpg\n",
            "./n0924646400000016.jpg\n",
            "./n0924646400000017.jpg\n",
            "./n0924646400000018.jpg\n",
            "./n0924646400000020.jpg\n",
            "./n0924646400000021.jpg\n",
            "./n0924646400000022.jpg\n",
            "./n0924646400000024.jpg\n",
            "./n0924646400000025.jpg\n",
            "./n0924646400000027.jpg\n",
            "./n0924646400000029.jpg\n",
            "./n0924646400000036.jpg\n",
            "./n0924646400000038.jpg\n",
            "./n0924646400000039.jpg\n",
            "./n0924646400000040.jpg\n",
            "./n0924646400000041.jpg\n",
            "./n0924646400000042.jpg\n",
            "./n0924646400000043.jpg\n",
            "./n0924646400000045.jpg\n",
            "./n0924646400000046.jpg\n",
            "./n0924646400000049.jpg\n",
            "./n0924646400000056.jpg\n",
            "./n0924646400000058.jpg\n",
            "./n0924646400000059.jpg\n",
            "./n0924646400000060.jpg\n",
            "./n0924646400000061.jpg\n",
            "./n0924646400000062.jpg\n",
            "./n0924646400000064.jpg\n",
            "./n0924646400000067.jpg\n",
            "./n0924646400000068.jpg\n",
            "./n0924646400000070.jpg\n",
            "./n0924646400000075.jpg\n",
            "./n0924646400000076.jpg\n",
            "./n0924646400000077.jpg\n",
            "./n0924646400000078.jpg\n",
            "./n0924646400000079.jpg\n",
            "./n0924646400000083.jpg\n",
            "./n0924646400000087.jpg\n",
            "./n0924646400000088.jpg\n",
            "./n0924646400000089.jpg\n",
            "./n0924646400000092.jpg\n",
            "./n0924646400000093.jpg\n",
            "./n0924646400000095.jpg\n",
            "./n0924646400000096.jpg\n",
            "./n0924646400000097.jpg\n",
            "./n0924646400000098.jpg\n",
            "./n0924646400000100.jpg\n",
            "./n0924646400000102.jpg\n",
            "./n0924646400000107.jpg\n",
            "./n0924646400000109.jpg\n",
            "./n0924646400000110.jpg\n",
            "./n0924646400000113.jpg\n",
            "./n0924646400000114.jpg\n",
            "./n0924646400000115.jpg\n",
            "./n0924646400000117.jpg\n",
            "./n0924646400000118.jpg\n",
            "./n0924646400000119.jpg\n",
            "./n0924646400000120.jpg\n",
            "./n0924646400000121.jpg\n",
            "./n0924646400000125.jpg\n",
            "./n0924646400000126.jpg\n",
            "./n0924646400000128.jpg\n",
            "./n0924646400000129.jpg\n",
            "./n0924646400000130.jpg\n",
            "./n0924646400000131.jpg\n",
            "./n0924646400000132.jpg\n",
            "./n0924646400000133.jpg\n",
            "./n0924646400000136.jpg\n",
            "./n0924646400000137.jpg\n",
            "./n0924646400000141.jpg\n",
            "./n0924646400000142.jpg\n",
            "./n0924646400000143.jpg\n",
            "./n0924646400000144.jpg\n",
            "./n0924646400000145.jpg\n",
            "./n0924646400000146.jpg\n",
            "./n0924646400000149.jpg\n",
            "./n0924646400000152.jpg\n",
            "./n0924646400000154.jpg\n",
            "./n0924646400000156.jpg\n",
            "./n0924646400000158.jpg\n",
            "./n0924646400000168.jpg\n",
            "./n0924646400000170.jpg\n",
            "./n0924646400000171.jpg\n",
            "./n0924646400000173.jpg\n",
            "./n0924646400000174.jpg\n",
            "./n0924646400000180.jpg\n",
            "./n0924646400000181.jpg\n",
            "./n0924646400000182.jpg\n",
            "./n0924646400000188.jpg\n",
            "./n0924646400000191.jpg\n",
            "./n0924646400000192.jpg\n",
            "./n0924646400000193.jpg\n",
            "./n0924646400000194.jpg\n",
            "./n0924646400000195.jpg\n",
            "./n0924646400000197.jpg\n",
            "./n0924646400000198.jpg\n",
            "./n0924646400000204.jpg\n",
            "./n0924646400000205.jpg\n",
            "./n0924646400000206.jpg\n",
            "./n0924646400000207.jpg\n",
            "./n0924646400000209.jpg\n",
            "./n0924646400000214.jpg\n",
            "./n0924646400000216.jpg\n",
            "./n0924646400000219.jpg\n",
            "./n0924646400000220.jpg\n",
            "./n0924646400000221.jpg\n",
            "./n0924646400000222.jpg\n",
            "./n0924646400000224.jpg\n",
            "./n0924646400000230.jpg\n",
            "./n0924646400000235.jpg\n",
            "./n0924646400000238.jpg\n",
            "./n0924646400000239.jpg\n",
            "./n0924646400000240.jpg\n",
            "./n0924646400000241.jpg\n",
            "./n0924646400000243.jpg\n",
            "./n0924646400000244.jpg\n",
            "./n0924646400000246.jpg\n",
            "./n0924646400000247.jpg\n",
            "./n0924646400000249.jpg\n",
            "./n0924646400000251.jpg\n",
            "./n0924646400000253.jpg\n",
            "./n0924646400000255.jpg\n",
            "./n0924646400000260.jpg\n",
            "./n0924646400000261.jpg\n",
            "./n0924646400000263.jpg\n",
            "./n0924646400000265.jpg\n",
            "./n0924646400000266.jpg\n",
            "./n0924646400000267.jpg\n",
            "./n0924646400000268.jpg\n",
            "./n0924646400000269.jpg\n",
            "./n0924646400000276.jpg\n",
            "./n0924646400000277.jpg\n",
            "./n0924646400000282.jpg\n",
            "./n0924646400000283.jpg\n",
            "./n0924646400000284.jpg\n",
            "./n0924646400000288.jpg\n",
            "./n0924646400000291.jpg\n",
            "./n0924646400000296.jpg\n",
            "./n0924646400000302.jpg\n",
            "./n0924646400000303.jpg\n",
            "./n0924646400000304.jpg\n",
            "./n0924646400000310.jpg\n",
            "./n0924646400000315.jpg\n",
            "./n0924646400000317.jpg\n",
            "./n0924646400000319.jpg\n",
            "./n0924646400000320.jpg\n",
            "./n0924646400000321.jpg\n",
            "./n0924646400000327.jpg\n",
            "./n0924646400000328.jpg\n",
            "./n0924646400000329.jpg\n",
            "./n0924646400000333.jpg\n",
            "./n0924646400000335.jpg\n",
            "./n0924646400000336.jpg\n",
            "./n0924646400000337.jpg\n",
            "./n0924646400000338.jpg\n",
            "./n0924646400000340.jpg\n",
            "./n0924646400000341.jpg\n",
            "./n0924646400000343.jpg\n",
            "./n0924646400000344.jpg\n",
            "./n0924646400000345.jpg\n",
            "./n0924646400000346.jpg\n",
            "./n0924646400000347.jpg\n",
            "./n0924646400000350.jpg\n",
            "./n0924646400000356.jpg\n",
            "./n0924646400000361.jpg\n",
            "./n0924646400000364.jpg\n",
            "./n0924646400000366.jpg\n",
            "./n0924646400000367.jpg\n",
            "./n0924646400000370.jpg\n",
            "./n0924646400000373.jpg\n",
            "./n0924646400000375.jpg\n",
            "./n0924646400000376.jpg\n",
            "./n0924646400000379.jpg\n",
            "./n0924646400000380.jpg\n",
            "./n0924646400000381.jpg\n",
            "./n0924646400000383.jpg\n",
            "./n0924646400000385.jpg\n",
            "./n0924646400000386.jpg\n",
            "./n0924646400000389.jpg\n",
            "./n0924646400000390.jpg\n",
            "./n0924646400000392.jpg\n",
            "./n0924646400000397.jpg\n",
            "./n0924646400000400.jpg\n",
            "./n0924646400000403.jpg\n",
            "./n0924646400000404.jpg\n",
            "./n0924646400000405.jpg\n",
            "./n0924646400000406.jpg\n",
            "./n0924646400000407.jpg\n",
            "./n0924646400000410.jpg\n",
            "./n0924646400000413.jpg\n",
            "./n0924646400000414.jpg\n",
            "./n0924646400000423.jpg\n",
            "./n0924646400000424.jpg\n",
            "./n0924646400000426.jpg\n",
            "./n0924646400000427.jpg\n",
            "./n0924646400000428.jpg\n",
            "./n0924646400000429.jpg\n",
            "./n0924646400000430.jpg\n",
            "./n0924646400000431.jpg\n",
            "./n0924646400000432.jpg\n",
            "./n0924646400000433.jpg\n",
            "./n0924646400000435.jpg\n",
            "./n0924646400000436.jpg\n",
            "./n0924646400000439.jpg\n",
            "./n0924646400000441.jpg\n",
            "./n0924646400000443.jpg\n",
            "./n0924646400000444.jpg\n",
            "./n0924646400000448.jpg\n",
            "./n0924646400000449.jpg\n",
            "./n0924646400000450.jpg\n",
            "./n0924646400000454.jpg\n",
            "./n0924646400000456.jpg\n",
            "./n0924646400000457.jpg\n",
            "./n0924646400000459.jpg\n",
            "./n0924646400000460.jpg\n",
            "./n0924646400000461.jpg\n",
            "./n0924646400000463.jpg\n",
            "./n0924646400000464.jpg\n",
            "./n0924646400000467.jpg\n",
            "./n0924646400000468.jpg\n",
            "./n0924646400000471.jpg\n",
            "./n0924646400000473.jpg\n",
            "./n0924646400000476.jpg\n",
            "./n0924646400000477.jpg\n",
            "./n0924646400000478.jpg\n",
            "./n0924646400000483.jpg\n",
            "./n0924646400000486.jpg\n",
            "./n0924646400000492.jpg\n",
            "./n0924646400000499.jpg\n",
            "./n0924646400000501.jpg\n",
            "./n0924646400000503.jpg\n",
            "./n0924646400000507.jpg\n",
            "./n0924646400000509.jpg\n",
            "./n0924646400000511.jpg\n",
            "./n0924646400000514.jpg\n",
            "./n0924646400000515.jpg\n",
            "./n0924646400000519.jpg\n",
            "./n0924646400000520.jpg\n",
            "./n0924646400000521.jpg\n",
            "./n0924646400000522.jpg\n",
            "./n0924646400000523.jpg\n",
            "./n0924646400000527.jpg\n",
            "./n0924646400000528.jpg\n",
            "./n0924646400000530.jpg\n",
            "./n0924646400000531.jpg\n",
            "./n0924646400000533.jpg\n",
            "./n0924646400000534.jpg\n",
            "./n0924646400000537.jpg\n",
            "./n0924646400000542.jpg\n",
            "./n0924646400000543.jpg\n",
            "./n0924646400000546.jpg\n",
            "./n0924646400000548.jpg\n",
            "./n0924646400000552.jpg\n",
            "./n0924646400000553.jpg\n",
            "./n0924646400000555.jpg\n",
            "./n0924646400000559.jpg\n",
            "./n0924646400000560.jpg\n",
            "./n0924646400000573.jpg\n",
            "./n0924646400000574.jpg\n",
            "./n0924646400000577.jpg\n",
            "./n0924646400000579.jpg\n",
            "./n0924646400000581.jpg\n",
            "./n0924646400000582.jpg\n",
            "./n0924646400000587.jpg\n",
            "./n0924646400000589.jpg\n",
            "./n0924646400000590.jpg\n",
            "./n0924646400000591.jpg\n",
            "./n0924646400000592.jpg\n",
            "./n0924646400000593.jpg\n",
            "./n0924646400000596.jpg\n",
            "./n0924646400000599.jpg\n",
            "./n0924646400000600.jpg\n",
            "./n0924646400000603.jpg\n",
            "./n0924646400000606.jpg\n",
            "./n0924646400000608.jpg\n",
            "./n0924646400000611.jpg\n",
            "./n0924646400000614.jpg\n",
            "./n0924646400000616.jpg\n",
            "./n0924646400000624.jpg\n",
            "./n0924646400000625.jpg\n",
            "./n0924646400000626.jpg\n",
            "./n0924646400000631.jpg\n",
            "./n0924646400000632.jpg\n",
            "./n0924646400000633.jpg\n",
            "./n0924646400000634.jpg\n",
            "./n0924646400000636.jpg\n",
            "./n0924646400000638.jpg\n",
            "./n0924646400000641.jpg\n",
            "./n0924646400000643.jpg\n",
            "./n0924646400000644.jpg\n",
            "./n0924646400000649.jpg\n",
            "./n0924646400000651.jpg\n",
            "./n0924646400000652.jpg\n",
            "./n0924646400000656.jpg\n",
            "./n0924646400000657.jpg\n",
            "./n0924646400000660.jpg\n",
            "./n0924646400000661.jpg\n",
            "./n0924646400000662.jpg\n",
            "./n0924646400000663.jpg\n",
            "./n0924646400000666.jpg\n",
            "./n0924646400000669.jpg\n",
            "./n0924646400000670.jpg\n",
            "./n0924646400000671.jpg\n",
            "./n0924646400000675.jpg\n",
            "./n0924646400000676.jpg\n",
            "./n0924646400000677.jpg\n",
            "./n0924646400000681.jpg\n",
            "./n0924646400000682.jpg\n",
            "./n0924646400000683.jpg\n",
            "./n0924646400000685.jpg\n",
            "./n0924646400000686.jpg\n",
            "./n0924646400000687.jpg\n",
            "./n0924646400000692.jpg\n",
            "./n0924646400000693.jpg\n",
            "./n0924646400000696.jpg\n",
            "./n0924646400000698.jpg\n",
            "./n0924646400000699.jpg\n",
            "./n0924646400000700.jpg\n",
            "./n0924646400000702.jpg\n",
            "./n0924646400000704.jpg\n",
            "./n0924646400000708.jpg\n",
            "./n0924646400000709.jpg\n",
            "./n0924646400000710.jpg\n",
            "./n0924646400000713.jpg\n",
            "./n0924646400000716.jpg\n",
            "./n0924646400000717.jpg\n",
            "./n0924646400000720.jpg\n",
            "./n0924646400000721.jpg\n",
            "./n0924646400000725.jpg\n",
            "./n0924646400000729.jpg\n",
            "./n0924646400000730.jpg\n",
            "./n0924646400000732.jpg\n",
            "./n0924646400000733.jpg\n",
            "./n0924646400000734.jpg\n",
            "./n0924646400000738.jpg\n",
            "./n0924646400000743.jpg\n",
            "./n0924646400000744.jpg\n",
            "./n0924646400000745.jpg\n",
            "./n0924646400000747.jpg\n",
            "./n0924646400000748.jpg\n",
            "./n0924646400000752.jpg\n",
            "./n0924646400000754.jpg\n",
            "./n0924646400000756.jpg\n",
            "./n0924646400000758.jpg\n",
            "./n0924646400000759.jpg\n",
            "./n0924646400000762.jpg\n",
            "./n0924646400000765.jpg\n",
            "./n0924646400000766.jpg\n",
            "./n0924646400000768.jpg\n",
            "./n0924646400000769.jpg\n",
            "./n0924646400000770.jpg\n",
            "./n0924646400000771.jpg\n",
            "./n0924646400000772.jpg\n",
            "./n0924646400000778.jpg\n",
            "./n0924646400000782.jpg\n",
            "./n0924646400000784.jpg\n",
            "./n0924646400000785.jpg\n",
            "./n0924646400000788.jpg\n",
            "./n0924646400000794.jpg\n",
            "./n0924646400000796.jpg\n",
            "./n0924646400000797.jpg\n",
            "./n0924646400000798.jpg\n",
            "./n0924646400000800.jpg\n",
            "./n0924646400000801.jpg\n",
            "./n0924646400000804.jpg\n",
            "./n0924646400000805.jpg\n",
            "./n0924646400000808.jpg\n",
            "./n0924646400000809.jpg\n",
            "./n0924646400000811.jpg\n",
            "./n0924646400000813.jpg\n",
            "./n0924646400000817.jpg\n",
            "./n0924646400000819.jpg\n",
            "./n0924646400000821.jpg\n",
            "./n0924646400000825.jpg\n",
            "./n0924646400000827.jpg\n",
            "./n0924646400000833.jpg\n",
            "./n0924646400000834.jpg\n",
            "./n0924646400000835.jpg\n",
            "./n0924646400000837.jpg\n",
            "./n0924646400000838.jpg\n",
            "./n0924646400000841.jpg\n",
            "./n0924646400000842.jpg\n",
            "./n0924646400000846.jpg\n",
            "./n0924646400000847.jpg\n",
            "./n0924646400000850.jpg\n",
            "./n0924646400000854.jpg\n",
            "./n0924646400000855.jpg\n",
            "./n0924646400000857.jpg\n",
            "./n0924646400000858.jpg\n",
            "./n0924646400000859.jpg\n",
            "./n0924646400000866.jpg\n",
            "./n0924646400000867.jpg\n",
            "./n0924646400000870.jpg\n",
            "./n0924646400000871.jpg\n",
            "./n0924646400000873.jpg\n",
            "./n0924646400000874.jpg\n",
            "./n0924646400000876.jpg\n",
            "./n0924646400000877.jpg\n",
            "./n0924646400000881.jpg\n",
            "./n0924646400000882.jpg\n",
            "./n0924646400000885.jpg\n",
            "./n0924646400000887.jpg\n",
            "./n0924646400000889.jpg\n",
            "./n0924646400000892.jpg\n",
            "./n0924646400000893.jpg\n",
            "./n0924646400000894.jpg\n",
            "./n0924646400000895.jpg\n",
            "./n0924646400000896.jpg\n",
            "./n0924646400000898.jpg\n",
            "./n0924646400000899.jpg\n",
            "./n0924646400000901.jpg\n",
            "./n0924646400000903.jpg\n",
            "./n0924646400000904.jpg\n",
            "./n0924646400000909.jpg\n",
            "./n0924646400000913.jpg\n",
            "./n0924646400000914.jpg\n",
            "./n0924646400000915.jpg\n",
            "./n0924646400000919.jpg\n",
            "./n0924646400000920.jpg\n",
            "./n0924646400000922.jpg\n",
            "./n0924646400000923.jpg\n",
            "./n0924646400000927.jpg\n",
            "./n0924646400000929.jpg\n",
            "./n0924646400000930.jpg\n",
            "./n0924646400000931.jpg\n",
            "./n0924646400000937.jpg\n",
            "./n0924646400000939.jpg\n",
            "./n0924646400000940.jpg\n",
            "./n0924646400000941.jpg\n",
            "./n0924646400000943.jpg\n",
            "./n0924646400000945.jpg\n",
            "./n0924646400000947.jpg\n",
            "./n0924646400000949.jpg\n",
            "./n0924646400000951.jpg\n",
            "./n0924646400000952.jpg\n",
            "./n0924646400000953.jpg\n",
            "./n0924646400000961.jpg\n",
            "./n0924646400000967.jpg\n",
            "./n0924646400000968.jpg\n",
            "./n0924646400000969.jpg\n",
            "./n0924646400000975.jpg\n",
            "./n0924646400000976.jpg\n",
            "./n0924646400000979.jpg\n",
            "./n0924646400000980.jpg\n",
            "./n0924646400000981.jpg\n",
            "./n0924646400000982.jpg\n",
            "./n0924646400000984.jpg\n",
            "./n0924646400000986.jpg\n",
            "./n0924646400000989.jpg\n",
            "./n0924646400000992.jpg\n",
            "./n0924646400000993.jpg\n",
            "./n0924646400000999.jpg\n",
            "./n0924646400001000.jpg\n",
            "./n0924646400001004.jpg\n",
            "./n0924646400001009.jpg\n",
            "./n0924646400001010.jpg\n",
            "./n0924646400001011.jpg\n",
            "./n0924646400001013.jpg\n",
            "./n0924646400001014.jpg\n",
            "./n0924646400001015.jpg\n",
            "./n0924646400001019.jpg\n",
            "./n0924646400001020.jpg\n",
            "./n0924646400001024.jpg\n",
            "./n0924646400001025.jpg\n",
            "./n0924646400001026.jpg\n",
            "./n0924646400001027.jpg\n",
            "./n0924646400001028.jpg\n",
            "./n0924646400001031.jpg\n",
            "./n0924646400001033.jpg\n",
            "./n0924646400001036.jpg\n",
            "./n0924646400001037.jpg\n",
            "./n0924646400001039.jpg\n",
            "./n0924646400001042.jpg\n",
            "./n0924646400001045.jpg\n",
            "./n0924646400001049.jpg\n",
            "./n0924646400001055.jpg\n",
            "./n0924646400001058.jpg\n",
            "./n0924646400001059.jpg\n",
            "./n0924646400001063.jpg\n",
            "./n0924646400001064.jpg\n",
            "./n0924646400001066.jpg\n",
            "./n0924646400001069.jpg\n",
            "./n0924646400001070.jpg\n",
            "./n0924646400001071.jpg\n",
            "./n0924646400001072.jpg\n",
            "./n0924646400001074.jpg\n",
            "./n0924646400001075.jpg\n",
            "./n0924646400001076.jpg\n",
            "./n0924646400001078.jpg\n",
            "./n0924646400001083.jpg\n",
            "./n0924646400001084.jpg\n",
            "./n0924646400001086.jpg\n",
            "./n0924646400001087.jpg\n",
            "./n0924646400001091.jpg\n",
            "./n0924646400001092.jpg\n",
            "./n0924646400001094.jpg\n",
            "./n0924646400001095.jpg\n",
            "./n0924646400001096.jpg\n",
            "./n0924646400001097.jpg\n",
            "./n0924646400001099.jpg\n",
            "./n0924646400001100.jpg\n",
            "./n0924646400001102.jpg\n",
            "./n0924646400001105.jpg\n",
            "./n0924646400001106.jpg\n",
            "./n0924646400001109.jpg\n",
            "./n0924646400001113.jpg\n",
            "./n0924646400001114.jpg\n",
            "./n0924646400001115.jpg\n",
            "./n0924646400001117.jpg\n",
            "./n0924646400001118.jpg\n",
            "./n0924646400001122.jpg\n",
            "./n0924646400001123.jpg\n",
            "./n0924646400001127.jpg\n",
            "./n0924646400001132.jpg\n",
            "./n0924646400001135.jpg\n",
            "./n0924646400001137.jpg\n",
            "./n0924646400001138.jpg\n",
            "./n0924646400001139.jpg\n",
            "./n0924646400001142.jpg\n",
            "./n0924646400001143.jpg\n",
            "./n0924646400001145.jpg\n",
            "./n0924646400001146.jpg\n",
            "./n0924646400001147.jpg\n",
            "./n0924646400001149.jpg\n",
            "./n0924646400001151.jpg\n",
            "./n0924646400001152.jpg\n",
            "./n0924646400001155.jpg\n",
            "./n0924646400001156.jpg\n",
            "./n0924646400001157.jpg\n",
            "./n0924646400001164.jpg\n",
            "./n0924646400001165.jpg\n",
            "./n0924646400001166.jpg\n",
            "./n0924646400001167.jpg\n",
            "./n0924646400001175.jpg\n",
            "./n0924646400001180.jpg\n",
            "./n0924646400001181.jpg\n",
            "./n0924646400001189.jpg\n",
            "./n0924646400001191.jpg\n",
            "./n0924646400001194.jpg\n",
            "./n0924646400001196.jpg\n",
            "./n0924646400001198.jpg\n",
            "./n0924646400001199.jpg\n",
            "./n0924646400001203.jpg\n",
            "./n0924646400001204.jpg\n",
            "./n0924646400001205.jpg\n",
            "./n0924646400001206.jpg\n",
            "./n0924646400001207.jpg\n",
            "./n0924646400001208.jpg\n",
            "./n0924646400001213.jpg\n",
            "./n0924646400001215.jpg\n",
            "./n0924646400001218.jpg\n",
            "./n0924646400001219.jpg\n",
            "./n0924646400001220.jpg\n",
            "./n0924646400001222.jpg\n",
            "./n0924646400001223.jpg\n",
            "./n0924646400001228.jpg\n",
            "./n0924646400001229.jpg\n",
            "./n0924646400001230.jpg\n",
            "./n0924646400001231.jpg\n",
            "./n0924646400001232.jpg\n",
            "./n0924646400001233.jpg\n",
            "./n0924646400001234.jpg\n",
            "./n0924646400001237.jpg\n",
            "./n0924646400001239.jpg\n",
            "./n0924646400001240.jpg\n",
            "./n0924646400001247.jpg\n",
            "./n0924646400001248.jpg\n",
            "./n0924646400001250.jpg\n",
            "./n0924646400001251.jpg\n",
            "./n0924646400001252.jpg\n",
            "./n0924646400001253.jpg\n",
            "./n0924646400001254.jpg\n",
            "./n0924646400001255.jpg\n",
            "./n0924646400001263.jpg\n",
            "./n0924646400001264.jpg\n",
            "./n0924646400001266.jpg\n",
            "./n0924646400001268.jpg\n",
            "./n0924646400001269.jpg\n",
            "./n0924646400001272.jpg\n",
            "./n0924646400001274.jpg\n",
            "./n0924646400001277.jpg\n",
            "./n0924646400001279.jpg\n",
            "./n0924646400001280.jpg\n",
            "./n0924646400001282.jpg\n",
            "./n0924646400001283.jpg\n",
            "./n0924646400001287.jpg\n",
            "./n0924646400001291.jpg\n",
            "./n0924646400001292.jpg\n",
            "./n0924646400001293.jpg\n",
            "./n0924646400001295.jpg\n",
            "./n0924646400001297.jpg\n",
            "./n0924646400001298.jpg\n",
            "./n0925647900000002.jpg\n",
            "./n0925647900000003.jpg\n",
            "./n0925647900000006.jpg\n",
            "./n0925647900000007.jpg\n",
            "./n0925647900000012.jpg\n",
            "./n0925647900000013.jpg\n",
            "./n0925647900000014.jpg\n",
            "./n0925647900000015.jpg\n",
            "./n0925647900000016.jpg\n",
            "./n0925647900000020.jpg\n",
            "./n0925647900000021.jpg\n",
            "./n0925647900000027.jpg\n",
            "./n0925647900000029.jpg\n",
            "./n0925647900000033.jpg\n",
            "./n0925647900000034.jpg\n",
            "./n0925647900000036.jpg\n",
            "./n0925647900000040.jpg\n",
            "./n0925647900000041.jpg\n",
            "./n0925647900000042.jpg\n",
            "./n0925647900000049.jpg\n",
            "./n0925647900000052.jpg\n",
            "./n0925647900000055.jpg\n",
            "./n0925647900000056.jpg\n",
            "./n0925647900000059.jpg\n",
            "./n0925647900000060.jpg\n",
            "./n0925647900000062.jpg\n",
            "./n0925647900000065.jpg\n",
            "./n0925647900000066.jpg\n",
            "./n0925647900000068.jpg\n",
            "./n0925647900000069.jpg\n",
            "./n0925647900000070.jpg\n",
            "./n0925647900000072.jpg\n",
            "./n0925647900000073.jpg\n",
            "./n0925647900000075.jpg\n",
            "./n0925647900000080.jpg\n",
            "./n0925647900000081.jpg\n",
            "./n0925647900000082.jpg\n",
            "./n0925647900000083.jpg\n",
            "./n0925647900000089.jpg\n",
            "./n0925647900000090.jpg\n",
            "./n0925647900000091.jpg\n",
            "./n0925647900000094.jpg\n",
            "./n0925647900000096.jpg\n",
            "./n0925647900000097.jpg\n",
            "./n0925647900000104.jpg\n",
            "./n0925647900000105.jpg\n",
            "./n0925647900000106.jpg\n",
            "./n0925647900000108.jpg\n",
            "./n0925647900000111.jpg\n",
            "./n0925647900000114.jpg\n",
            "./n0925647900000115.jpg\n",
            "./n0925647900000119.jpg\n",
            "./n0925647900000123.jpg\n",
            "./n0925647900000126.jpg\n",
            "./n0925647900000127.jpg\n",
            "./n0925647900000129.jpg\n",
            "./n0925647900000131.jpg\n",
            "./n0925647900000132.jpg\n",
            "./n0925647900000134.jpg\n",
            "./n0925647900000140.jpg\n",
            "./n0925647900000142.jpg\n",
            "./n0925647900000143.jpg\n",
            "./n0925647900000144.jpg\n",
            "./n0925647900000145.jpg\n",
            "./n0925647900000149.jpg\n",
            "./n0925647900000150.jpg\n",
            "./n0925647900000151.jpg\n",
            "./n0925647900000154.jpg\n",
            "./n0925647900000155.jpg\n",
            "./n0925647900000159.jpg\n",
            "./n0925647900000161.jpg\n",
            "./n0925647900000162.jpg\n",
            "./n0925647900000163.jpg\n",
            "./n0925647900000165.jpg\n",
            "./n0925647900000166.jpg\n",
            "./n0925647900000171.jpg\n",
            "./n0925647900000175.jpg\n",
            "./n0925647900000176.jpg\n",
            "./n0925647900000177.jpg\n",
            "./n0925647900000179.jpg\n",
            "./n0925647900000180.jpg\n",
            "./n0925647900000181.jpg\n",
            "./n0925647900000183.jpg\n",
            "./n0925647900000189.jpg\n",
            "./n0925647900000190.jpg\n",
            "./n0925647900000193.jpg\n",
            "./n0925647900000194.jpg\n",
            "./n0925647900000195.jpg\n",
            "./n0925647900000205.jpg\n",
            "./n0925647900000206.jpg\n",
            "./n0925647900000208.jpg\n",
            "./n0925647900000210.jpg\n",
            "./n0925647900000213.jpg\n",
            "./n0925647900000215.jpg\n",
            "./n0925647900000216.jpg\n",
            "./n0925647900000217.jpg\n",
            "./n0925647900000224.jpg\n",
            "./n0925647900000227.jpg\n",
            "./n0925647900000229.jpg\n",
            "./n0925647900000232.jpg\n",
            "./n0925647900000233.jpg\n",
            "./n0925647900000237.jpg\n",
            "./n0925647900000239.jpg\n",
            "./n0925647900000240.jpg\n",
            "./n0925647900000249.jpg\n",
            "./n0925647900000255.jpg\n",
            "./n0925647900000257.jpg\n",
            "./n0925647900000259.jpg\n",
            "./n0925647900000260.jpg\n",
            "./n0925647900000261.jpg\n",
            "./n0925647900000262.jpg\n",
            "./n0925647900000267.jpg\n",
            "./n0925647900000271.jpg\n",
            "./n0925647900000273.jpg\n",
            "./n0925647900000275.jpg\n",
            "./n0925647900000276.jpg\n",
            "./n0925647900000277.jpg\n",
            "./n0925647900000279.jpg\n",
            "./n0925647900000281.jpg\n",
            "./n0925647900000282.jpg\n",
            "./n0925647900000284.jpg\n",
            "./n0925647900000287.jpg\n",
            "./n0925647900000289.jpg\n",
            "./n0925647900000293.jpg\n",
            "./n0925647900000294.jpg\n",
            "./n0925647900000296.jpg\n",
            "./n0925647900000300.jpg\n",
            "./n0925647900000303.jpg\n",
            "./n0925647900000305.jpg\n",
            "./n0925647900000307.jpg\n",
            "./n0925647900000313.jpg\n",
            "./n0925647900000317.jpg\n",
            "./n0925647900000323.jpg\n",
            "./n0925647900000325.jpg\n",
            "./n0925647900000327.jpg\n",
            "./n0925647900000329.jpg\n",
            "./n0925647900000331.jpg\n",
            "./n0925647900000332.jpg\n",
            "./n0925647900000335.jpg\n",
            "./n0925647900000336.jpg\n",
            "./n0925647900000337.jpg\n",
            "./n0925647900000339.jpg\n",
            "./n0925647900000340.jpg\n",
            "./n0925647900000341.jpg\n",
            "./n0925647900000342.jpg\n",
            "./n0925647900000343.jpg\n",
            "./n0925647900000345.jpg\n",
            "./n0925647900000346.jpg\n",
            "./n0925647900000348.jpg\n",
            "./n0925647900000350.jpg\n",
            "./n0925647900000351.jpg\n",
            "./n0925647900000352.jpg\n",
            "./n0925647900000353.jpg\n",
            "./n0925647900000354.jpg\n",
            "./n0925647900000355.jpg\n",
            "./n0925647900000356.jpg\n",
            "./n0925647900000358.jpg\n",
            "./n0925647900000359.jpg\n",
            "./n0925647900000360.jpg\n",
            "./n0925647900000366.jpg\n",
            "./n0925647900000367.jpg\n",
            "./n0925647900000369.jpg\n",
            "./n0925647900000371.jpg\n",
            "./n0925647900000372.jpg\n",
            "./n0925647900000374.jpg\n",
            "./n0925647900000375.jpg\n",
            "./n0925647900000376.jpg\n",
            "./n0925647900000377.jpg\n",
            "./n0925647900000378.jpg\n",
            "./n0925647900000379.jpg\n",
            "./n0925647900000384.jpg\n",
            "./n0925647900000389.jpg\n",
            "./n0925647900000390.jpg\n",
            "./n0925647900000391.jpg\n",
            "./n0925647900000393.jpg\n",
            "./n0925647900000396.jpg\n",
            "./n0925647900000398.jpg\n",
            "./n0925647900000400.jpg\n",
            "./n0925647900000401.jpg\n",
            "./n0925647900000403.jpg\n",
            "./n0925647900000405.jpg\n",
            "./n0925647900000406.jpg\n",
            "./n0925647900000410.jpg\n",
            "./n0925647900000411.jpg\n",
            "./n0925647900000413.jpg\n",
            "./n0925647900000417.jpg\n",
            "./n0925647900000418.jpg\n",
            "./n0925647900000420.jpg\n",
            "./n0925647900000424.jpg\n",
            "./n0925647900000426.jpg\n",
            "./n0925647900000427.jpg\n",
            "./n0925647900000429.jpg\n",
            "./n0925647900000430.jpg\n",
            "./n0925647900000433.jpg\n",
            "./n0925647900000434.jpg\n",
            "./n0925647900000437.jpg\n",
            "./n0925647900000438.jpg\n",
            "./n0925647900000440.jpg\n",
            "./n0925647900000441.jpg\n",
            "./n0925647900000442.jpg\n",
            "./n0925647900000443.jpg\n",
            "./n0925647900000444.jpg\n",
            "./n0925647900000446.jpg\n",
            "./n0925647900000447.jpg\n",
            "./n0925647900000449.jpg\n",
            "./n0925647900000451.jpg\n",
            "./n0925647900000453.jpg\n",
            "./n0925647900000454.jpg\n",
            "./n0925647900000461.jpg\n",
            "./n0925647900000462.jpg\n",
            "./n0925647900000465.jpg\n",
            "./n0925647900000467.jpg\n",
            "./n0925647900000468.jpg\n",
            "./n0925647900000470.jpg\n",
            "./n0925647900000472.jpg\n",
            "./n0925647900000475.jpg\n",
            "./n0925647900000477.jpg\n",
            "./n0925647900000480.jpg\n",
            "./n0925647900000484.jpg\n",
            "./n0925647900000488.jpg\n",
            "./n0925647900000489.jpg\n",
            "./n0925647900000490.jpg\n",
            "./n0925647900000491.jpg\n",
            "./n0925647900000497.jpg\n",
            "./n0925647900000501.jpg\n",
            "./n0925647900000502.jpg\n",
            "./n0925647900000503.jpg\n",
            "./n0925647900000504.jpg\n",
            "./n0925647900000505.jpg\n",
            "./n0925647900000507.jpg\n",
            "./n0925647900000508.jpg\n",
            "./n0925647900000509.jpg\n",
            "./n0925647900000514.jpg\n",
            "./n0925647900000523.jpg\n",
            "./n0925647900000524.jpg\n",
            "./n0925647900000525.jpg\n",
            "./n0925647900000526.jpg\n",
            "./n0925647900000531.jpg\n",
            "./n0925647900000535.jpg\n",
            "./n0925647900000536.jpg\n",
            "./n0925647900000537.jpg\n",
            "./n0925647900000538.jpg\n",
            "./n0925647900000540.jpg\n",
            "./n0925647900000543.jpg\n",
            "./n0925647900000544.jpg\n",
            "./n0925647900000545.jpg\n",
            "./n0925647900000546.jpg\n",
            "./n0925647900000558.jpg\n",
            "./n0925647900000564.jpg\n",
            "./n0925647900000565.jpg\n",
            "./n0925647900000566.jpg\n",
            "./n0925647900000568.jpg\n",
            "./n0925647900000569.jpg\n",
            "./n0925647900000571.jpg\n",
            "./n0925647900000579.jpg\n",
            "./n0925647900000580.jpg\n",
            "./n0925647900000581.jpg\n",
            "./n0925647900000582.jpg\n",
            "./n0925647900000586.jpg\n",
            "./n0925647900000587.jpg\n",
            "./n0925647900000588.jpg\n",
            "./n0925647900000590.jpg\n",
            "./n0925647900000592.jpg\n",
            "./n0925647900000593.jpg\n",
            "./n0925647900000594.jpg\n",
            "./n0925647900000598.jpg\n",
            "./n0925647900000599.jpg\n",
            "./n0925647900000600.jpg\n",
            "./n0925647900000603.jpg\n",
            "./n0925647900000605.jpg\n",
            "./n0925647900000609.jpg\n",
            "./n0925647900000610.jpg\n",
            "./n0925647900000611.jpg\n",
            "./n0925647900000616.jpg\n",
            "./n0925647900000617.jpg\n",
            "./n0925647900000618.jpg\n",
            "./n0925647900000619.jpg\n",
            "./n0925647900000620.jpg\n",
            "./n0925647900000621.jpg\n",
            "./n0925647900000622.jpg\n",
            "./n0925647900000624.jpg\n",
            "./n0925647900000628.jpg\n",
            "./n0925647900000629.jpg\n",
            "./n0925647900000630.jpg\n",
            "./n0925647900000631.jpg\n",
            "./n0925647900000639.jpg\n",
            "./n0925647900000641.jpg\n",
            "./n0925647900000642.jpg\n",
            "./n0925647900000643.jpg\n",
            "./n0925647900000644.jpg\n",
            "./n0925647900000646.jpg\n",
            "./n0925647900000647.jpg\n",
            "./n0925647900000652.jpg\n",
            "./n0925647900000653.jpg\n",
            "./n0925647900000654.jpg\n",
            "./n0925647900000655.jpg\n",
            "./n0925647900000657.jpg\n",
            "./n0925647900000659.jpg\n",
            "./n0925647900000660.jpg\n",
            "./n0925647900000661.jpg\n",
            "./n0925647900000662.jpg\n",
            "./n0925647900000666.jpg\n",
            "./n0925647900000672.jpg\n",
            "./n0925647900000677.jpg\n",
            "./n0925647900000678.jpg\n",
            "./n0925647900000679.jpg\n",
            "./n0925647900000681.jpg\n",
            "./n0925647900000684.jpg\n",
            "./n0925647900000691.jpg\n",
            "./n0925647900000693.jpg\n",
            "./n0925647900000694.jpg\n",
            "./n0925647900000698.jpg\n",
            "./n0925647900000699.jpg\n",
            "./n0925647900000700.jpg\n",
            "./n0925647900000706.jpg\n",
            "./n0925647900000707.jpg\n",
            "./n0925647900000708.jpg\n",
            "./n0925647900000709.jpg\n",
            "./n0925647900000714.jpg\n",
            "./n0925647900000718.jpg\n",
            "./n0925647900000719.jpg\n",
            "./n0925647900000720.jpg\n",
            "./n0925647900000722.jpg\n",
            "./n0925647900000723.jpg\n",
            "./n0925647900000724.jpg\n",
            "./n0925647900000726.jpg\n",
            "./n0925647900000728.jpg\n",
            "./n0925647900000729.jpg\n",
            "./n0925647900000731.jpg\n",
            "./n0925647900000732.jpg\n",
            "./n0925647900000736.jpg\n",
            "./n0925647900000737.jpg\n",
            "./n0925647900000739.jpg\n",
            "./n0925647900000743.jpg\n",
            "./n0925647900000744.jpg\n",
            "./n0925647900000746.jpg\n",
            "./n0925647900000747.jpg\n",
            "./n0925647900000749.jpg\n",
            "./n0925647900000750.jpg\n",
            "./n0925647900000753.jpg\n",
            "./n0925647900000755.jpg\n",
            "./n0925647900000757.jpg\n",
            "./n0925647900000764.jpg\n",
            "./n0925647900000767.jpg\n",
            "./n0925647900000768.jpg\n",
            "./n0925647900000769.jpg\n",
            "./n0925647900000773.jpg\n",
            "./n0925647900000774.jpg\n",
            "./n0925647900000778.jpg\n",
            "./n0925647900000779.jpg\n",
            "./n0925647900000782.jpg\n",
            "./n0925647900000783.jpg\n",
            "./n0925647900000784.jpg\n",
            "./n0925647900000787.jpg\n",
            "./n0925647900000788.jpg\n",
            "./n0925647900000792.jpg\n",
            "./n0925647900000794.jpg\n",
            "./n0925647900000795.jpg\n",
            "./n0925647900000797.jpg\n",
            "./n0925647900000799.jpg\n",
            "./n0925647900000801.jpg\n",
            "./n0925647900000803.jpg\n",
            "./n0925647900000804.jpg\n",
            "./n0925647900000806.jpg\n",
            "./n0925647900000809.jpg\n",
            "./n0925647900000810.jpg\n",
            "./n0925647900000811.jpg\n",
            "./n0925647900000814.jpg\n",
            "./n0925647900000818.jpg\n",
            "./n0925647900000819.jpg\n",
            "./n0925647900000822.jpg\n",
            "./n0925647900000823.jpg\n",
            "./n0925647900000824.jpg\n",
            "./n0925647900000825.jpg\n",
            "./n0925647900000827.jpg\n",
            "./n0925647900000828.jpg\n",
            "./n0925647900000829.jpg\n",
            "./n0925647900000830.jpg\n",
            "./n0925647900000831.jpg\n",
            "./n0925647900000832.jpg\n",
            "./n0925647900000839.jpg\n",
            "./n0925647900000840.jpg\n",
            "./n0925647900000844.jpg\n",
            "./n0925647900000845.jpg\n",
            "./n0925647900000846.jpg\n",
            "./n0925647900000847.jpg\n",
            "./n0925647900000850.jpg\n",
            "./n0925647900000852.jpg\n",
            "./n0925647900000855.jpg\n",
            "./n0925647900000860.jpg\n",
            "./n0925647900000862.jpg\n",
            "./n0925647900000868.jpg\n",
            "./n0925647900000870.jpg\n",
            "./n0925647900000872.jpg\n",
            "./n0925647900000874.jpg\n",
            "./n0925647900000877.jpg\n",
            "./n0925647900000878.jpg\n",
            "./n0925647900000881.jpg\n",
            "./n0925647900000882.jpg\n",
            "./n0925647900000883.jpg\n",
            "./n0925647900000884.jpg\n",
            "./n0925647900000886.jpg\n",
            "./n0925647900000887.jpg\n",
            "./n0925647900000888.jpg\n",
            "./n0925647900000889.jpg\n",
            "./n0925647900000890.jpg\n",
            "./n0925647900000894.jpg\n",
            "./n0925647900000896.jpg\n",
            "./n0925647900000898.jpg\n",
            "./n0925647900000899.jpg\n",
            "./n0925647900000902.jpg\n",
            "./n0925647900000908.jpg\n",
            "./n0925647900000909.jpg\n",
            "./n0925647900000912.jpg\n",
            "./n0925647900000913.jpg\n",
            "./n0925647900000915.jpg\n",
            "./n0925647900000916.jpg\n",
            "./n0925647900000917.jpg\n",
            "./n0925647900000918.jpg\n",
            "./n0925647900000919.jpg\n",
            "./n0925647900000920.jpg\n",
            "./n0925647900000922.jpg\n",
            "./n0925647900000923.jpg\n",
            "./n0925647900000927.jpg\n",
            "./n0925647900000928.jpg\n",
            "./n0925647900000929.jpg\n",
            "./n0925647900000931.jpg\n",
            "./n0925647900000933.jpg\n",
            "./n0925647900000934.jpg\n",
            "./n0925647900000938.jpg\n",
            "./n0925647900000943.jpg\n",
            "./n0925647900000945.jpg\n",
            "./n0925647900000947.jpg\n",
            "./n0925647900000955.jpg\n",
            "./n0925647900000956.jpg\n",
            "./n0925647900000961.jpg\n",
            "./n0925647900000962.jpg\n",
            "./n0925647900000965.jpg\n",
            "./n0925647900000966.jpg\n",
            "./n0925647900000969.jpg\n",
            "./n0925647900000971.jpg\n",
            "./n0925647900000972.jpg\n",
            "./n0925647900000973.jpg\n",
            "./n0925647900000975.jpg\n",
            "./n0925647900000977.jpg\n",
            "./n0925647900000978.jpg\n",
            "./n0925647900000979.jpg\n",
            "./n0925647900000981.jpg\n",
            "./n0925647900000982.jpg\n",
            "./n0925647900000983.jpg\n",
            "./n0925647900000986.jpg\n",
            "./n0925647900000988.jpg\n",
            "./n0925647900000992.jpg\n",
            "./n0925647900000993.jpg\n",
            "./n0925647900000999.jpg\n",
            "./n0925647900001000.jpg\n",
            "./n0925647900001006.jpg\n",
            "./n0925647900001008.jpg\n",
            "./n0925647900001009.jpg\n",
            "./n0925647900001010.jpg\n",
            "./n0925647900001011.jpg\n",
            "./n0925647900001013.jpg\n",
            "./n0925647900001017.jpg\n",
            "./n0925647900001019.jpg\n",
            "./n0925647900001022.jpg\n",
            "./n0925647900001023.jpg\n",
            "./n0925647900001024.jpg\n",
            "./n0925647900001027.jpg\n",
            "./n0925647900001031.jpg\n",
            "./n0925647900001035.jpg\n",
            "./n0925647900001038.jpg\n",
            "./n0925647900001041.jpg\n",
            "./n0925647900001042.jpg\n",
            "./n0925647900001047.jpg\n",
            "./n0925647900001050.jpg\n",
            "./n0925647900001051.jpg\n",
            "./n0925647900001052.jpg\n",
            "./n0925647900001054.jpg\n",
            "./n0925647900001056.jpg\n",
            "./n0925647900001059.jpg\n",
            "./n0925647900001060.jpg\n",
            "./n0925647900001061.jpg\n",
            "./n0925647900001062.jpg\n",
            "./n0925647900001064.jpg\n",
            "./n0925647900001066.jpg\n",
            "./n0925647900001068.jpg\n",
            "./n0925647900001069.jpg\n",
            "./n0925647900001070.jpg\n",
            "./n0925647900001071.jpg\n",
            "./n0925647900001072.jpg\n",
            "./n0925647900001073.jpg\n",
            "./n0925647900001075.jpg\n",
            "./n0925647900001076.jpg\n",
            "./n0925647900001077.jpg\n",
            "./n0925647900001080.jpg\n",
            "./n0925647900001082.jpg\n",
            "./n0925647900001084.jpg\n",
            "./n0925647900001085.jpg\n",
            "./n0925647900001088.jpg\n",
            "./n0925647900001092.jpg\n",
            "./n0925647900001093.jpg\n",
            "./n0925647900001095.jpg\n",
            "./n0925647900001096.jpg\n",
            "./n0925647900001097.jpg\n",
            "./n0925647900001098.jpg\n",
            "./n0925647900001099.jpg\n",
            "./n0925647900001100.jpg\n",
            "./n0925647900001102.jpg\n",
            "./n0925647900001105.jpg\n",
            "./n0925647900001106.jpg\n",
            "./n0925647900001107.jpg\n",
            "./n0925647900001109.jpg\n",
            "./n0925647900001115.jpg\n",
            "./n0925647900001116.jpg\n",
            "./n0925647900001119.jpg\n",
            "./n0925647900001123.jpg\n",
            "./n0925647900001124.jpg\n",
            "./n0925647900001129.jpg\n",
            "./n0925647900001130.jpg\n",
            "./n0925647900001131.jpg\n",
            "./n0925647900001135.jpg\n",
            "./n0925647900001137.jpg\n",
            "./n0925647900001138.jpg\n",
            "./n0925647900001140.jpg\n",
            "./n0925647900001141.jpg\n",
            "./n0925647900001142.jpg\n",
            "./n0925647900001143.jpg\n",
            "./n0925647900001148.jpg\n",
            "./n0925647900001150.jpg\n",
            "./n0925647900001151.jpg\n",
            "./n0925647900001154.jpg\n",
            "./n0925647900001162.jpg\n",
            "./n0925647900001163.jpg\n",
            "./n0925647900001164.jpg\n",
            "./n0925647900001165.jpg\n",
            "./n0925647900001166.jpg\n",
            "./n0925647900001168.jpg\n",
            "./n0925647900001169.jpg\n",
            "./n0925647900001172.jpg\n",
            "./n0925647900001175.jpg\n",
            "./n0925647900001178.jpg\n",
            "./n0925647900001179.jpg\n",
            "./n0925647900001180.jpg\n",
            "./n0925647900001181.jpg\n",
            "./n0925647900001182.jpg\n",
            "./n0925647900001183.jpg\n",
            "./n0925647900001188.jpg\n",
            "./n0925647900001190.jpg\n",
            "./n0925647900001191.jpg\n",
            "./n0925647900001192.jpg\n",
            "./n0925647900001195.jpg\n",
            "./n0925647900001198.jpg\n",
            "./n0925647900001201.jpg\n",
            "./n0925647900001202.jpg\n",
            "./n0925647900001203.jpg\n",
            "./n0925647900001205.jpg\n",
            "./n0925647900001212.jpg\n",
            "./n0925647900001213.jpg\n",
            "./n0925647900001215.jpg\n",
            "./n0925647900001218.jpg\n",
            "./n0925647900001219.jpg\n",
            "./n0925647900001220.jpg\n",
            "./n0925647900001224.jpg\n",
            "./n0925647900001225.jpg\n",
            "./n0925647900001226.jpg\n",
            "./n0925647900001228.jpg\n",
            "./n0925647900001229.jpg\n",
            "./n0925647900001231.jpg\n",
            "./n0925647900001232.jpg\n",
            "./n0925647900001234.jpg\n",
            "./n0925647900001236.jpg\n",
            "./n0925647900001237.jpg\n",
            "./n0925647900001239.jpg\n",
            "./n0925647900001243.jpg\n",
            "./n0925647900001244.jpg\n",
            "./n0925647900001245.jpg\n",
            "./n0925647900001246.jpg\n",
            "./n0925647900001250.jpg\n",
            "./n0925647900001254.jpg\n",
            "./n0925647900001256.jpg\n",
            "./n0925647900001258.jpg\n",
            "./n0925647900001259.jpg\n",
            "./n0925647900001261.jpg\n",
            "./n0925647900001262.jpg\n",
            "./n0925647900001263.jpg\n",
            "./n0925647900001265.jpg\n",
            "./n0925647900001267.jpg\n",
            "./n0925647900001269.jpg\n",
            "./n0925647900001270.jpg\n",
            "./n0925647900001271.jpg\n",
            "./n0925647900001272.jpg\n",
            "./n0925647900001273.jpg\n",
            "./n0925647900001274.jpg\n",
            "./n0925647900001275.jpg\n",
            "./n0925647900001279.jpg\n",
            "./n0925647900001283.jpg\n",
            "./n0925647900001284.jpg\n",
            "./n0925647900001285.jpg\n",
            "./n0925647900001286.jpg\n",
            "./n0925647900001291.jpg\n",
            "./n1305456000000001.jpg\n",
            "./n1305456000000004.jpg\n",
            "./n1305456000000005.jpg\n",
            "./n1305456000000008.jpg\n",
            "./n1305456000000009.jpg\n",
            "./n1305456000000014.jpg\n",
            "./n1305456000000016.jpg\n",
            "./n1305456000000017.jpg\n",
            "./n1305456000000018.jpg\n",
            "./n1305456000000021.jpg\n",
            "./n1305456000000025.jpg\n",
            "./n1305456000000026.jpg\n",
            "./n1305456000000027.jpg\n",
            "./n1305456000000030.jpg\n",
            "./n1305456000000032.jpg\n",
            "./n1305456000000033.jpg\n",
            "./n1305456000000034.jpg\n",
            "./n1305456000000035.jpg\n",
            "./n1305456000000037.jpg\n",
            "./n1305456000000038.jpg\n",
            "./n1305456000000039.jpg\n",
            "./n1305456000000040.jpg\n",
            "./n1305456000000041.jpg\n",
            "./n1305456000000042.jpg\n",
            "./n1305456000000043.jpg\n",
            "./n1305456000000045.jpg\n",
            "./n1305456000000046.jpg\n",
            "./n1305456000000047.jpg\n",
            "./n1305456000000048.jpg\n",
            "./n1305456000000052.jpg\n",
            "./n1305456000000057.jpg\n",
            "./n1305456000000058.jpg\n",
            "./n1305456000000060.jpg\n",
            "./n1305456000000061.jpg\n",
            "./n1305456000000062.jpg\n",
            "./n1305456000000063.jpg\n",
            "./n1305456000000066.jpg\n",
            "./n1305456000000067.jpg\n",
            "./n1305456000000068.jpg\n",
            "./n1305456000000073.jpg\n",
            "./n1305456000000074.jpg\n",
            "./n1305456000000076.jpg\n",
            "./n1305456000000077.jpg\n",
            "./n1305456000000078.jpg\n",
            "./n1305456000000081.jpg\n",
            "./n1305456000000083.jpg\n",
            "./n1305456000000085.jpg\n",
            "./n1305456000000086.jpg\n",
            "./n1305456000000087.jpg\n",
            "./n1305456000000088.jpg\n",
            "./n1305456000000093.jpg\n",
            "./n1305456000000094.jpg\n",
            "./n1305456000000097.jpg\n",
            "./n1305456000000098.jpg\n",
            "./n1305456000000099.jpg\n",
            "./n1305456000000100.jpg\n",
            "./n1305456000000101.jpg\n",
            "./n1305456000000102.jpg\n",
            "./n1305456000000103.jpg\n",
            "./n1305456000000104.jpg\n",
            "./n1305456000000108.jpg\n",
            "./n1305456000000112.jpg\n",
            "./n1305456000000114.jpg\n",
            "./n1305456000000116.jpg\n",
            "./n1305456000000119.jpg\n",
            "./n1305456000000122.jpg\n",
            "./n1305456000000126.jpg\n",
            "./n1305456000000130.jpg\n",
            "./n1305456000000132.jpg\n",
            "./n1305456000000138.jpg\n",
            "./n1305456000000142.jpg\n",
            "./n1305456000000145.jpg\n",
            "./n1305456000000148.jpg\n",
            "./n1305456000000149.jpg\n",
            "./n1305456000000150.jpg\n",
            "./n1305456000000153.jpg\n",
            "./n1305456000000155.jpg\n",
            "./n1305456000000156.jpg\n",
            "./n1305456000000159.jpg\n",
            "./n1305456000000160.jpg\n",
            "./n1305456000000163.jpg\n",
            "./n1305456000000164.jpg\n",
            "./n1305456000000165.jpg\n",
            "./n1305456000000166.jpg\n",
            "./n1305456000000167.jpg\n",
            "./n1305456000000169.jpg\n",
            "./n1305456000000170.jpg\n",
            "./n1305456000000171.jpg\n",
            "./n1305456000000172.jpg\n",
            "./n1305456000000176.jpg\n",
            "./n1305456000000183.jpg\n",
            "./n1305456000000186.jpg\n",
            "./n1305456000000187.jpg\n",
            "./n1305456000000188.jpg\n",
            "./n1305456000000190.jpg\n",
            "./n1305456000000197.jpg\n",
            "./n1305456000000198.jpg\n",
            "./n1305456000000199.jpg\n",
            "./n1305456000000200.jpg\n",
            "./n1305456000000202.jpg\n",
            "./n1305456000000203.jpg\n",
            "./n1305456000000206.jpg\n",
            "./n1305456000000207.jpg\n",
            "./n1305456000000209.jpg\n",
            "./n1305456000000211.jpg\n",
            "./n1305456000000213.jpg\n",
            "./n1305456000000214.jpg\n",
            "./n1305456000000216.jpg\n",
            "./n1305456000000218.jpg\n",
            "./n1305456000000219.jpg\n",
            "./n1305456000000222.jpg\n",
            "./n1305456000000226.jpg\n",
            "./n1305456000000227.jpg\n",
            "./n1305456000000230.jpg\n",
            "./n1305456000000231.jpg\n",
            "./n1305456000000237.jpg\n",
            "./n1305456000000244.jpg\n",
            "./n1305456000000252.jpg\n",
            "./n1305456000000254.jpg\n",
            "./n1305456000000255.jpg\n",
            "./n1305456000000258.jpg\n",
            "./n1305456000000263.jpg\n",
            "./n1305456000000268.jpg\n",
            "./n1305456000000269.jpg\n",
            "./n1305456000000274.jpg\n",
            "./n1305456000000276.jpg\n",
            "./n1305456000000277.jpg\n",
            "./n1305456000000279.jpg\n",
            "./n1305456000000280.jpg\n",
            "./n1305456000000282.jpg\n",
            "./n1305456000000286.jpg\n",
            "./n1305456000000287.jpg\n",
            "./n1305456000000289.jpg\n",
            "./n1305456000000290.jpg\n",
            "./n1305456000000291.jpg\n",
            "./n1305456000000294.jpg\n",
            "./n1305456000000295.jpg\n",
            "./n1305456000000298.jpg\n",
            "./n1305456000000301.jpg\n",
            "./n1305456000000303.jpg\n",
            "./n1305456000000305.jpg\n",
            "./n1305456000000306.jpg\n",
            "./n1305456000000307.jpg\n",
            "./n1305456000000309.jpg\n",
            "./n1305456000000311.jpg\n",
            "./n1305456000000312.jpg\n",
            "./n1305456000000313.jpg\n",
            "./n1305456000000315.jpg\n",
            "./n1305456000000316.jpg\n",
            "./n1305456000000318.jpg\n",
            "./n1305456000000319.jpg\n",
            "./n1305456000000320.jpg\n",
            "./n1305456000000321.jpg\n",
            "./n1305456000000322.jpg\n",
            "./n1305456000000324.jpg\n",
            "./n1305456000000332.jpg\n",
            "./n1305456000000338.jpg\n",
            "./n1305456000000341.jpg\n",
            "./n1305456000000342.jpg\n",
            "./n1305456000000346.jpg\n",
            "./n1305456000000347.jpg\n",
            "./n1305456000000349.jpg\n",
            "./n1305456000000350.jpg\n",
            "./n1305456000000351.jpg\n",
            "./n1305456000000352.jpg\n",
            "./n1305456000000355.jpg\n",
            "./n1305456000000357.jpg\n",
            "./n1305456000000360.jpg\n",
            "./n1305456000000362.jpg\n",
            "./n1305456000000367.jpg\n",
            "./n1305456000000370.jpg\n",
            "./n1305456000000372.jpg\n",
            "./n1305456000000374.jpg\n",
            "./n1305456000000379.jpg\n",
            "./n1305456000000380.jpg\n",
            "./n1305456000000383.jpg\n",
            "./n1305456000000385.jpg\n",
            "./n1305456000000386.jpg\n",
            "./n1305456000000389.jpg\n",
            "./n1305456000000390.jpg\n",
            "./n1305456000000391.jpg\n",
            "./n1305456000000394.jpg\n",
            "./n1305456000000395.jpg\n",
            "./n1305456000000397.jpg\n",
            "./n1305456000000404.jpg\n",
            "./n1305456000000406.jpg\n",
            "./n1305456000000408.jpg\n",
            "./n1305456000000413.jpg\n",
            "./n1305456000000415.jpg\n",
            "./n1305456000000418.jpg\n",
            "./n1305456000000419.jpg\n",
            "./n1305456000000420.jpg\n",
            "./n1305456000000423.jpg\n",
            "./n1305456000000425.jpg\n",
            "./n1305456000000426.jpg\n",
            "./n1305456000000428.jpg\n",
            "./n1305456000000429.jpg\n",
            "./n1305456000000430.jpg\n",
            "./n1305456000000432.jpg\n",
            "./n1305456000000436.jpg\n",
            "./n1305456000000438.jpg\n",
            "./n1305456000000439.jpg\n",
            "./n1305456000000444.jpg\n",
            "./n1305456000000445.jpg\n",
            "./n1305456000000450.jpg\n",
            "./n1305456000000451.jpg\n",
            "./n1305456000000452.jpg\n",
            "./n1305456000000453.jpg\n",
            "./n1305456000000454.jpg\n",
            "./n1305456000000458.jpg\n",
            "./n1305456000000459.jpg\n",
            "./n1305456000000460.jpg\n",
            "./n1305456000000461.jpg\n",
            "./n1305456000000462.jpg\n",
            "./n1305456000000470.jpg\n",
            "./n1305456000000471.jpg\n",
            "./n1305456000000475.jpg\n",
            "./n1305456000000476.jpg\n",
            "./n1305456000000479.jpg\n",
            "./n1305456000000480.jpg\n",
            "./n1305456000000482.jpg\n",
            "./n1305456000000483.jpg\n",
            "./n1305456000000487.jpg\n",
            "./n1305456000000491.jpg\n",
            "./n1305456000000492.jpg\n",
            "./n1305456000000493.jpg\n",
            "./n1305456000000496.jpg\n",
            "./n1305456000000498.jpg\n",
            "./n1305456000000499.jpg\n",
            "./n1305456000000500.jpg\n",
            "./n1305456000000502.jpg\n",
            "./n1305456000000503.jpg\n",
            "./n1305456000000504.jpg\n",
            "./n1305456000000505.jpg\n",
            "./n1305456000000506.jpg\n",
            "./n1305456000000510.jpg\n",
            "./n1305456000000511.jpg\n",
            "./n1305456000000513.jpg\n",
            "./n1305456000000515.jpg\n",
            "./n1305456000000517.jpg\n",
            "./n1305456000000518.jpg\n",
            "./n1305456000000519.jpg\n",
            "./n1305456000000520.jpg\n",
            "./n1305456000000522.jpg\n",
            "./n1305456000000523.jpg\n",
            "./n1305456000000525.jpg\n",
            "./n1305456000000529.jpg\n",
            "./n1305456000000530.jpg\n",
            "./n1305456000000531.jpg\n",
            "./n1305456000000533.jpg\n",
            "./n1305456000000534.jpg\n",
            "./n1305456000000538.jpg\n",
            "./n1305456000000539.jpg\n",
            "./n1305456000000543.jpg\n",
            "./n1305456000000544.jpg\n",
            "./n1305456000000545.jpg\n",
            "./n1305456000000550.jpg\n",
            "./n1305456000000551.jpg\n",
            "./n1305456000000554.jpg\n",
            "./n1305456000000556.jpg\n",
            "./n1305456000000558.jpg\n",
            "./n1305456000000559.jpg\n",
            "./n1305456000000561.jpg\n",
            "./n1305456000000564.jpg\n",
            "./n1305456000000565.jpg\n",
            "./n1305456000000567.jpg\n",
            "./n1305456000000570.jpg\n",
            "./n1305456000000571.jpg\n",
            "./n1305456000000572.jpg\n",
            "./n1305456000000575.jpg\n",
            "./n1305456000000576.jpg\n",
            "./n1305456000000577.jpg\n",
            "./n1305456000000579.jpg\n",
            "./n1305456000000582.jpg\n",
            "./n1305456000000586.jpg\n",
            "./n1305456000000588.jpg\n",
            "./n1305456000000590.jpg\n",
            "./n1305456000000592.jpg\n",
            "./n1305456000000593.jpg\n",
            "./n1305456000000594.jpg\n",
            "./n1305456000000595.jpg\n",
            "./n1305456000000596.jpg\n",
            "./n1305456000000599.jpg\n",
            "./n1305456000000600.jpg\n",
            "./n1305456000000605.jpg\n",
            "./n1305456000000608.jpg\n",
            "./n1305456000000609.jpg\n",
            "./n1305456000000610.jpg\n",
            "./n1305456000000611.jpg\n",
            "./n1305456000000612.jpg\n",
            "./n1305456000000614.jpg\n",
            "./n1305456000000616.jpg\n",
            "./n1305456000000619.jpg\n",
            "./n1305456000000625.jpg\n",
            "./n1305456000000627.jpg\n",
            "./n1305456000000628.jpg\n",
            "./n1305456000000632.jpg\n",
            "./n1305456000000633.jpg\n",
            "./n1305456000000637.jpg\n",
            "./n1305456000000639.jpg\n",
            "./n1305456000000641.jpg\n",
            "./n1305456000000642.jpg\n",
            "./n1305456000000645.jpg\n",
            "./n1305456000000647.jpg\n",
            "./n1305456000000648.jpg\n",
            "./n1305456000000650.jpg\n",
            "./n1305456000000651.jpg\n",
            "./n1305456000000652.jpg\n",
            "./n1305456000000653.jpg\n",
            "./n1305456000000654.jpg\n",
            "./n1305456000000657.jpg\n",
            "./n1305456000000658.jpg\n",
            "./n1305456000000659.jpg\n",
            "./n1305456000000660.jpg\n",
            "./n1305456000000661.jpg\n",
            "./n1305456000000662.jpg\n",
            "./n1305456000000663.jpg\n",
            "./n1305456000000665.jpg\n",
            "./n1305456000000667.jpg\n",
            "./n1305456000000670.jpg\n",
            "./n1305456000000672.jpg\n",
            "./n1305456000000674.jpg\n",
            "./n1305456000000675.jpg\n",
            "./n1305456000000678.jpg\n",
            "./n1305456000000680.jpg\n",
            "./n1305456000000681.jpg\n",
            "./n1305456000000686.jpg\n",
            "./n1305456000000690.jpg\n",
            "./n1305456000000693.jpg\n",
            "./n1305456000000695.jpg\n",
            "./n1305456000000696.jpg\n",
            "./n1305456000000701.jpg\n",
            "./n1305456000000703.jpg\n",
            "./n1305456000000710.jpg\n",
            "./n1305456000000713.jpg\n",
            "./n1305456000000714.jpg\n",
            "./n1305456000000716.jpg\n",
            "./n1305456000000718.jpg\n",
            "./n1305456000000721.jpg\n",
            "./n1305456000000726.jpg\n",
            "./n1305456000000727.jpg\n",
            "./n1305456000000730.jpg\n",
            "./n1305456000000733.jpg\n",
            "./n1305456000000736.jpg\n",
            "./n1305456000000737.jpg\n",
            "./n1305456000000741.jpg\n",
            "./n1305456000000744.jpg\n",
            "./n1305456000000745.jpg\n",
            "./n1305456000000747.jpg\n",
            "./n1305456000000751.jpg\n",
            "./n1305456000000754.jpg\n",
            "./n1305456000000755.jpg\n",
            "./n1305456000000756.jpg\n",
            "./n1305456000000757.jpg\n",
            "./n1305456000000758.jpg\n",
            "./n1305456000000763.jpg\n",
            "./n1305456000000764.jpg\n",
            "./n1305456000000765.jpg\n",
            "./n1305456000000766.jpg\n",
            "./n1305456000000769.jpg\n",
            "./n1305456000000771.jpg\n",
            "./n1305456000000773.jpg\n",
            "./n1305456000000774.jpg\n",
            "./n1305456000000777.jpg\n",
            "./n1305456000000778.jpg\n",
            "./n1305456000000784.jpg\n",
            "./n1305456000000787.jpg\n",
            "./n1305456000000788.jpg\n",
            "./n1305456000000789.jpg\n",
            "./n1305456000000790.jpg\n",
            "./n1305456000000791.jpg\n",
            "./n1305456000000792.jpg\n",
            "./n1305456000000794.jpg\n",
            "./n1305456000000795.jpg\n",
            "./n1305456000000797.jpg\n",
            "./n1305456000000800.jpg\n",
            "./n1305456000000801.jpg\n",
            "./n1305456000000802.jpg\n",
            "./n1305456000000804.jpg\n",
            "./n1305456000000805.jpg\n",
            "./n1305456000000806.jpg\n",
            "./n1305456000000807.jpg\n",
            "./n1305456000000809.jpg\n",
            "./n1305456000000811.jpg\n",
            "./n1305456000000812.jpg\n",
            "./n1305456000000813.jpg\n",
            "./n1305456000000814.jpg\n",
            "./n1305456000000817.jpg\n",
            "./n1305456000000820.jpg\n",
            "./n1305456000000823.jpg\n",
            "./n1305456000000824.jpg\n",
            "./n1305456000000826.jpg\n",
            "./n1305456000000827.jpg\n",
            "./n1305456000000833.jpg\n",
            "./n1305456000000834.jpg\n",
            "./n1305456000000835.jpg\n",
            "./n1305456000000838.jpg\n",
            "./n1305456000000844.jpg\n",
            "./n1305456000000847.jpg\n",
            "./n1305456000000848.jpg\n",
            "./n1305456000000849.jpg\n",
            "./n1305456000000850.jpg\n",
            "./n1305456000000852.jpg\n",
            "./n1305456000000854.jpg\n",
            "./n1305456000000855.jpg\n",
            "./n1305456000000856.jpg\n",
            "./n1305456000000858.jpg\n",
            "./n1305456000000860.jpg\n",
            "./n1305456000000861.jpg\n",
            "./n1305456000000863.jpg\n",
            "./n1305456000000866.jpg\n",
            "./n1305456000000867.jpg\n",
            "./n1305456000000868.jpg\n",
            "./n1305456000000874.jpg\n",
            "./n1305456000000875.jpg\n",
            "./n1305456000000880.jpg\n",
            "./n1305456000000883.jpg\n",
            "./n1305456000000885.jpg\n",
            "./n1305456000000886.jpg\n",
            "./n1305456000000889.jpg\n",
            "./n1305456000000895.jpg\n",
            "./n1305456000000897.jpg\n",
            "./n1305456000000898.jpg\n",
            "./n1305456000000899.jpg\n",
            "./n1305456000000901.jpg\n",
            "./n1305456000000903.jpg\n",
            "./n1305456000000905.jpg\n",
            "./n1305456000000906.jpg\n",
            "./n1305456000000908.jpg\n",
            "./n1305456000000909.jpg\n",
            "./n1305456000000912.jpg\n",
            "./n1305456000000914.jpg\n",
            "./n1305456000000916.jpg\n",
            "./n1305456000000917.jpg\n",
            "./n1305456000000918.jpg\n",
            "./n1305456000000919.jpg\n",
            "./n1305456000000920.jpg\n",
            "./n1305456000000921.jpg\n",
            "./n1305456000000926.jpg\n",
            "./n1305456000000928.jpg\n",
            "./n1305456000000930.jpg\n",
            "./n1305456000000931.jpg\n",
            "./n1305456000000937.jpg\n",
            "./n1305456000000938.jpg\n",
            "./n1305456000000941.jpg\n",
            "./n1305456000000942.jpg\n",
            "./n1305456000000943.jpg\n",
            "./n1305456000000944.jpg\n",
            "./n1305456000000945.jpg\n",
            "./n1305456000000947.jpg\n",
            "./n1305456000000950.jpg\n",
            "./n1305456000000953.jpg\n",
            "./n1305456000000956.jpg\n",
            "./n1305456000000962.jpg\n",
            "./n1305456000000964.jpg\n",
            "./n1305456000000965.jpg\n",
            "./n1305456000000972.jpg\n",
            "./n1305456000000973.jpg\n",
            "./n1305456000000976.jpg\n",
            "./n1305456000000977.jpg\n",
            "./n1305456000000982.jpg\n",
            "./n1305456000000983.jpg\n",
            "./n1305456000000984.jpg\n",
            "./n1305456000000985.jpg\n",
            "./n1305456000000986.jpg\n",
            "./n1305456000000987.jpg\n",
            "./n1305456000000988.jpg\n",
            "./n1305456000000990.jpg\n",
            "./n1305456000000993.jpg\n",
            "./n1305456000000997.jpg\n",
            "./n1305456000000999.jpg\n",
            "./n1305456000001005.jpg\n",
            "./n1305456000001006.jpg\n",
            "./n1305456000001007.jpg\n",
            "./n1305456000001011.jpg\n",
            "./n1305456000001012.jpg\n",
            "./n1305456000001013.jpg\n",
            "./n1305456000001016.jpg\n",
            "./n1305456000001018.jpg\n",
            "./n1305456000001020.jpg\n",
            "./n1305456000001021.jpg\n",
            "./n1305456000001025.jpg\n",
            "./n1305456000001026.jpg\n",
            "./n1305456000001027.jpg\n",
            "./n1305456000001036.jpg\n",
            "./n1305456000001038.jpg\n",
            "./n1305456000001039.jpg\n",
            "./n1305456000001043.jpg\n",
            "./n1305456000001045.jpg\n",
            "./n1305456000001047.jpg\n",
            "./n1305456000001048.jpg\n",
            "./n1305456000001049.jpg\n",
            "./n1305456000001050.jpg\n",
            "./n1305456000001051.jpg\n",
            "./n1305456000001053.jpg\n",
            "./n1305456000001055.jpg\n",
            "./n1305456000001057.jpg\n",
            "./n1305456000001058.jpg\n",
            "./n1305456000001059.jpg\n",
            "./n1305456000001061.jpg\n",
            "./n1305456000001062.jpg\n",
            "./n1305456000001064.jpg\n",
            "./n1305456000001066.jpg\n",
            "./n1305456000001067.jpg\n",
            "./n1305456000001068.jpg\n",
            "./n1305456000001070.jpg\n",
            "./n1305456000001072.jpg\n",
            "./n1305456000001079.jpg\n",
            "./n1305456000001080.jpg\n",
            "./n1305456000001081.jpg\n",
            "./n1305456000001084.jpg\n",
            "./n1305456000001086.jpg\n",
            "./n1305456000001087.jpg\n",
            "./n1305456000001088.jpg\n",
            "./n1305456000001092.jpg\n",
            "./n1305456000001094.jpg\n",
            "./n1305456000001104.jpg\n",
            "./n1305456000001105.jpg\n",
            "./n1305456000001107.jpg\n",
            "./n1305456000001108.jpg\n",
            "./n1305456000001112.jpg\n",
            "./n1305456000001115.jpg\n",
            "./n1305456000001116.jpg\n",
            "./n1305456000001117.jpg\n",
            "./n1305456000001118.jpg\n",
            "./n1305456000001119.jpg\n",
            "./n1305456000001121.jpg\n",
            "./n1305456000001122.jpg\n",
            "./n1305456000001125.jpg\n",
            "./n1305456000001126.jpg\n",
            "./n1305456000001136.jpg\n",
            "./n1305456000001138.jpg\n",
            "./n1305456000001139.jpg\n",
            "./n1305456000001141.jpg\n",
            "./n1305456000001142.jpg\n",
            "./n1305456000001143.jpg\n",
            "./n1305456000001144.jpg\n",
            "./n1305456000001145.jpg\n",
            "./n1305456000001149.jpg\n",
            "./n1305456000001152.jpg\n",
            "./n1305456000001156.jpg\n",
            "./n1305456000001159.jpg\n",
            "./n1305456000001160.jpg\n",
            "./n1305456000001163.jpg\n",
            "./n1305456000001164.jpg\n",
            "./n1305456000001167.jpg\n",
            "./n1305456000001172.jpg\n",
            "./n1305456000001177.jpg\n",
            "./n1305456000001178.jpg\n",
            "./n1305456000001180.jpg\n",
            "./n1305456000001181.jpg\n",
            "./n1305456000001182.jpg\n",
            "./n1305456000001187.jpg\n",
            "./n1305456000001188.jpg\n",
            "./n1305456000001191.jpg\n",
            "./n1305456000001194.jpg\n",
            "./n1305456000001197.jpg\n",
            "./n1305456000001198.jpg\n",
            "./n1305456000001199.jpg\n",
            "./n1305456000001201.jpg\n",
            "./n1305456000001206.jpg\n",
            "./n1305456000001209.jpg\n",
            "./n1305456000001210.jpg\n",
            "./n1305456000001212.jpg\n",
            "./n1305456000001214.jpg\n",
            "./n1305456000001216.jpg\n",
            "./n1305456000001220.jpg\n",
            "./n1305456000001222.jpg\n",
            "./n1305456000001223.jpg\n",
            "./n1305456000001225.jpg\n",
            "./n1305456000001226.jpg\n",
            "./n1305456000001227.jpg\n",
            "./n1305456000001228.jpg\n",
            "./n1305456000001229.jpg\n",
            "./n1305456000001230.jpg\n",
            "./n1305456000001231.jpg\n",
            "./n1305456000001232.jpg\n",
            "./n1305456000001235.jpg\n",
            "./n1305456000001239.jpg\n",
            "./n1305456000001240.jpg\n",
            "./n1305456000001247.jpg\n",
            "./n1305456000001248.jpg\n",
            "./n1305456000001249.jpg\n",
            "./n1305456000001252.jpg\n",
            "./n1305456000001255.jpg\n",
            "./n1305456000001259.jpg\n",
            "./n1305456000001261.jpg\n",
            "./n1305456000001262.jpg\n",
            "./n1305456000001263.jpg\n",
            "./n1305456000001266.jpg\n",
            "./n1305456000001270.jpg\n",
            "./n1305456000001271.jpg\n",
            "./n1305456000001273.jpg\n",
            "./n1305456000001277.jpg\n",
            "./n1305456000001279.jpg\n",
            "./n1305456000001280.jpg\n",
            "./n1305456000001282.jpg\n",
            "./n1305456000001288.jpg\n",
            "./n1305456000001292.jpg\n",
            "./n1305456000001293.jpg\n",
            "./n1313361300000001.jpg\n",
            "./n1313361300000002.jpg\n",
            "./n1313361300000004.jpg\n",
            "./n1313361300000007.jpg\n",
            "./n1313361300000008.jpg\n",
            "./n1313361300000009.jpg\n",
            "./n1313361300000012.jpg\n",
            "./n1313361300000014.jpg\n",
            "./n1313361300000015.jpg\n",
            "./n1313361300000017.jpg\n",
            "./n1313361300000018.jpg\n",
            "./n1313361300000021.jpg\n",
            "./n1313361300000022.jpg\n",
            "./n1313361300000024.jpg\n",
            "./n1313361300000028.jpg\n",
            "./n1313361300000033.jpg\n",
            "./n1313361300000035.jpg\n",
            "./n1313361300000040.jpg\n",
            "./n1313361300000041.jpg\n",
            "./n1313361300000042.jpg\n",
            "./n1313361300000044.jpg\n",
            "./n1313361300000045.jpg\n",
            "./n1313361300000046.jpg\n",
            "./n1313361300000047.jpg\n",
            "./n1313361300000048.jpg\n",
            "./n1313361300000049.jpg\n",
            "./n1313361300000050.jpg\n",
            "./n1313361300000051.jpg\n",
            "./n1313361300000052.jpg\n",
            "./n1313361300000054.jpg\n",
            "./n1313361300000055.jpg\n",
            "./n1313361300000056.jpg\n",
            "./n1313361300000059.jpg\n",
            "./n1313361300000062.jpg\n",
            "./n1313361300000063.jpg\n",
            "./n1313361300000064.jpg\n",
            "./n1313361300000065.jpg\n",
            "./n1313361300000066.jpg\n",
            "./n1313361300000067.jpg\n",
            "./n1313361300000070.jpg\n",
            "./n1313361300000076.jpg\n",
            "./n1313361300000077.jpg\n",
            "./n1313361300000078.jpg\n",
            "./n1313361300000079.jpg\n",
            "./n1313361300000083.jpg\n",
            "./n1313361300000085.jpg\n",
            "./n1313361300000086.jpg\n",
            "./n1313361300000087.jpg\n",
            "./n1313361300000088.jpg\n",
            "./n1313361300000090.jpg\n",
            "./n1313361300000091.jpg\n",
            "./n1313361300000094.jpg\n",
            "./n1313361300000095.jpg\n",
            "./n1313361300000096.jpg\n",
            "./n1313361300000097.jpg\n",
            "./n1313361300000098.jpg\n",
            "./n1313361300000101.jpg\n",
            "./n1313361300000102.jpg\n",
            "./n1313361300000104.jpg\n",
            "./n1313361300000106.jpg\n",
            "./n1313361300000110.jpg\n",
            "./n1313361300000112.jpg\n",
            "./n1313361300000113.jpg\n",
            "./n1313361300000116.jpg\n",
            "./n1313361300000117.jpg\n",
            "./n1313361300000123.jpg\n",
            "./n1313361300000124.jpg\n",
            "./n1313361300000128.jpg\n",
            "./n1313361300000130.jpg\n",
            "./n1313361300000131.jpg\n",
            "./n1313361300000137.jpg\n",
            "./n1313361300000139.jpg\n",
            "./n1313361300000143.jpg\n",
            "./n1313361300000144.jpg\n",
            "./n1313361300000147.jpg\n",
            "./n1313361300000148.jpg\n",
            "./n1313361300000150.jpg\n",
            "./n1313361300000152.jpg\n",
            "./n1313361300000153.jpg\n",
            "./n1313361300000154.jpg\n",
            "./n1313361300000155.jpg\n",
            "./n1313361300000157.jpg\n",
            "./n1313361300000158.jpg\n",
            "./n1313361300000159.jpg\n",
            "./n1313361300000160.jpg\n",
            "./n1313361300000161.jpg\n",
            "./n1313361300000162.jpg\n",
            "./n1313361300000165.jpg\n",
            "./n1313361300000166.jpg\n",
            "./n1313361300000168.jpg\n",
            "./n1313361300000170.jpg\n",
            "./n1313361300000171.jpg\n",
            "./n1313361300000175.jpg\n",
            "./n1313361300000177.jpg\n",
            "./n1313361300000181.jpg\n",
            "./n1313361300000183.jpg\n",
            "./n1313361300000189.jpg\n",
            "./n1313361300000192.jpg\n",
            "./n1313361300000193.jpg\n",
            "./n1313361300000196.jpg\n",
            "./n1313361300000200.jpg\n",
            "./n1313361300000201.jpg\n",
            "./n1313361300000202.jpg\n",
            "./n1313361300000206.jpg\n",
            "./n1313361300000209.jpg\n",
            "./n1313361300000213.jpg\n",
            "./n1313361300000214.jpg\n",
            "./n1313361300000215.jpg\n",
            "./n1313361300000216.jpg\n",
            "./n1313361300000218.jpg\n",
            "./n1313361300000219.jpg\n",
            "./n1313361300000220.jpg\n",
            "./n1313361300000224.jpg\n",
            "./n1313361300000227.jpg\n",
            "./n1313361300000228.jpg\n",
            "./n1313361300000231.jpg\n",
            "./n1313361300000232.jpg\n",
            "./n1313361300000234.jpg\n",
            "./n1313361300000235.jpg\n",
            "./n1313361300000237.jpg\n",
            "./n1313361300000240.jpg\n",
            "./n1313361300000244.jpg\n",
            "./n1313361300000247.jpg\n",
            "./n1313361300000249.jpg\n",
            "./n1313361300000251.jpg\n",
            "./n1313361300000256.jpg\n",
            "./n1313361300000257.jpg\n",
            "./n1313361300000258.jpg\n",
            "./n1313361300000259.jpg\n",
            "./n1313361300000263.jpg\n",
            "./n1313361300000264.jpg\n",
            "./n1313361300000265.jpg\n",
            "./n1313361300000270.jpg\n",
            "./n1313361300000271.jpg\n",
            "./n1313361300000272.jpg\n",
            "./n1313361300000274.jpg\n",
            "./n1313361300000276.jpg\n",
            "./n1313361300000278.jpg\n",
            "./n1313361300000279.jpg\n",
            "./n1313361300000281.jpg\n",
            "./n1313361300000282.jpg\n",
            "./n1313361300000283.jpg\n",
            "./n1313361300000284.jpg\n",
            "./n1313361300000287.jpg\n",
            "./n1313361300000288.jpg\n",
            "./n1313361300000293.jpg\n",
            "./n1313361300000294.jpg\n",
            "./n1313361300000295.jpg\n",
            "./n1313361300000298.jpg\n",
            "./n1313361300000299.jpg\n",
            "./n1313361300000301.jpg\n",
            "./n1313361300000302.jpg\n",
            "./n1313361300000303.jpg\n",
            "./n1313361300000310.jpg\n",
            "./n1313361300000311.jpg\n",
            "./n1313361300000313.jpg\n",
            "./n1313361300000314.jpg\n",
            "./n1313361300000315.jpg\n",
            "./n1313361300000317.jpg\n",
            "./n1313361300000318.jpg\n",
            "./n1313361300000320.jpg\n",
            "./n1313361300000321.jpg\n",
            "./n1313361300000322.jpg\n",
            "./n1313361300000324.jpg\n",
            "./n1313361300000326.jpg\n",
            "./n1313361300000327.jpg\n",
            "./n1313361300000328.jpg\n",
            "./n1313361300000329.jpg\n",
            "./n1313361300000330.jpg\n",
            "./n1313361300000332.jpg\n",
            "./n1313361300000333.jpg\n",
            "./n1313361300000334.jpg\n",
            "./n1313361300000335.jpg\n",
            "./n1313361300000337.jpg\n",
            "./n1313361300000339.jpg\n",
            "./n1313361300000340.jpg\n",
            "./n1313361300000344.jpg\n",
            "./n1313361300000347.jpg\n",
            "./n1313361300000357.jpg\n",
            "./n1313361300000358.jpg\n",
            "./n1313361300000359.jpg\n",
            "./n1313361300000362.jpg\n",
            "./n1313361300000365.jpg\n",
            "./n1313361300000368.jpg\n",
            "./n1313361300000370.jpg\n",
            "./n1313361300000371.jpg\n",
            "./n1313361300000375.jpg\n",
            "./n1313361300000376.jpg\n",
            "./n1313361300000379.jpg\n",
            "./n1313361300000382.jpg\n",
            "./n1313361300000384.jpg\n",
            "./n1313361300000385.jpg\n",
            "./n1313361300000386.jpg\n",
            "./n1313361300000389.jpg\n",
            "./n1313361300000391.jpg\n",
            "./n1313361300000397.jpg\n",
            "./n1313361300000399.jpg\n",
            "./n1313361300000400.jpg\n",
            "./n1313361300000404.jpg\n",
            "./n1313361300000407.jpg\n",
            "./n1313361300000409.jpg\n",
            "./n1313361300000410.jpg\n",
            "./n1313361300000411.jpg\n",
            "./n1313361300000416.jpg\n",
            "./n1313361300000418.jpg\n",
            "./n1313361300000419.jpg\n",
            "./n1313361300000420.jpg\n",
            "./n1313361300000421.jpg\n",
            "./n1313361300000427.jpg\n",
            "./n1313361300000428.jpg\n",
            "./n1313361300000429.jpg\n",
            "./n1313361300000430.jpg\n",
            "./n1313361300000431.jpg\n",
            "./n1313361300000432.jpg\n",
            "./n1313361300000435.jpg\n",
            "./n1313361300000438.jpg\n",
            "./n1313361300000440.jpg\n",
            "./n1313361300000443.jpg\n",
            "./n1313361300000444.jpg\n",
            "./n1313361300000447.jpg\n",
            "./n1313361300000449.jpg\n",
            "./n1313361300000450.jpg\n",
            "./n1313361300000451.jpg\n",
            "./n1313361300000452.jpg\n",
            "./n1313361300000458.jpg\n",
            "./n1313361300000459.jpg\n",
            "./n1313361300000467.jpg\n",
            "./n1313361300000470.jpg\n",
            "./n1313361300000471.jpg\n",
            "./n1313361300000473.jpg\n",
            "./n1313361300000474.jpg\n",
            "./n1313361300000477.jpg\n",
            "./n1313361300000481.jpg\n",
            "./n1313361300000482.jpg\n",
            "./n1313361300000484.jpg\n",
            "./n1313361300000487.jpg\n",
            "./n1313361300000490.jpg\n",
            "./n1313361300000491.jpg\n",
            "./n1313361300000492.jpg\n",
            "./n1313361300000494.jpg\n",
            "./n1313361300000497.jpg\n",
            "./n1313361300000498.jpg\n",
            "./n1313361300000499.jpg\n",
            "./n1313361300000500.jpg\n",
            "./n1313361300000502.jpg\n",
            "./n1313361300000507.jpg\n",
            "./n1313361300000511.jpg\n",
            "./n1313361300000517.jpg\n",
            "./n1313361300000518.jpg\n",
            "./n1313361300000521.jpg\n",
            "./n1313361300000522.jpg\n",
            "./n1313361300000523.jpg\n",
            "./n1313361300000528.jpg\n",
            "./n1313361300000532.jpg\n",
            "./n1313361300000533.jpg\n",
            "./n1313361300000534.jpg\n",
            "./n1313361300000537.jpg\n",
            "./n1313361300000540.jpg\n",
            "./n1313361300000541.jpg\n",
            "./n1313361300000542.jpg\n",
            "./n1313361300000543.jpg\n",
            "./n1313361300000544.jpg\n",
            "./n1313361300000545.jpg\n",
            "./n1313361300000546.jpg\n",
            "./n1313361300000547.jpg\n",
            "./n1313361300000549.jpg\n",
            "./n1313361300000550.jpg\n",
            "./n1313361300000551.jpg\n",
            "./n1313361300000556.jpg\n",
            "./n1313361300000557.jpg\n",
            "./n1313361300000561.jpg\n",
            "./n1313361300000562.jpg\n",
            "./n1313361300000563.jpg\n",
            "./n1313361300000565.jpg\n",
            "./n1313361300000569.jpg\n",
            "./n1313361300000570.jpg\n",
            "./n1313361300000571.jpg\n",
            "./n1313361300000572.jpg\n",
            "./n1313361300000574.jpg\n",
            "./n1313361300000575.jpg\n",
            "./n1313361300000576.jpg\n",
            "./n1313361300000580.jpg\n",
            "./n1313361300000584.jpg\n",
            "./n1313361300000587.jpg\n",
            "./n1313361300000588.jpg\n",
            "./n1313361300000591.jpg\n",
            "./n1313361300000592.jpg\n",
            "./n1313361300000596.jpg\n",
            "./n1313361300000598.jpg\n",
            "./n1313361300000599.jpg\n",
            "./n1313361300000605.jpg\n",
            "./n1313361300000606.jpg\n",
            "./n1313361300000608.jpg\n",
            "./n1313361300000610.jpg\n",
            "./n1313361300000611.jpg\n",
            "./n1313361300000614.jpg\n",
            "./n1313361300000617.jpg\n",
            "./n1313361300000618.jpg\n",
            "./n1313361300000622.jpg\n",
            "./n1313361300000623.jpg\n",
            "./n1313361300000624.jpg\n",
            "./n1313361300000625.jpg\n",
            "./n1313361300000629.jpg\n",
            "./n1313361300000630.jpg\n",
            "./n1313361300000634.jpg\n",
            "./n1313361300000635.jpg\n",
            "./n1313361300000639.jpg\n",
            "./n1313361300000642.jpg\n",
            "./n1313361300000650.jpg\n",
            "./n1313361300000651.jpg\n",
            "./n1313361300000652.jpg\n",
            "./n1313361300000654.jpg\n",
            "./n1313361300000655.jpg\n",
            "./n1313361300000657.jpg\n",
            "./n1313361300000658.jpg\n",
            "./n1313361300000659.jpg\n",
            "./n1313361300000661.jpg\n",
            "./n1313361300000663.jpg\n",
            "./n1313361300000664.jpg\n",
            "./n1313361300000667.jpg\n",
            "./n1313361300000668.jpg\n",
            "./n1313361300000669.jpg\n",
            "./n1313361300000672.jpg\n",
            "./n1313361300000673.jpg\n",
            "./n1313361300000675.jpg\n",
            "./n1313361300000679.jpg\n",
            "./n1313361300000680.jpg\n",
            "./n1313361300000681.jpg\n",
            "./n1313361300000682.jpg\n",
            "./n1313361300000684.jpg\n",
            "./n1313361300000685.jpg\n",
            "./n1313361300000687.jpg\n",
            "./n1313361300000689.jpg\n",
            "./n1313361300000691.jpg\n",
            "./n1313361300000693.jpg\n",
            "./n1313361300000697.jpg\n",
            "./n1313361300000699.jpg\n",
            "./n1313361300000701.jpg\n",
            "./n1313361300000702.jpg\n",
            "./n1313361300000704.jpg\n",
            "./n1313361300000705.jpg\n",
            "./n1313361300000708.jpg\n",
            "./n1313361300000713.jpg\n",
            "./n1313361300000714.jpg\n",
            "./n1313361300000715.jpg\n",
            "./n1313361300000717.jpg\n",
            "./n1313361300000720.jpg\n",
            "./n1313361300000721.jpg\n",
            "./n1313361300000724.jpg\n",
            "./n1313361300000726.jpg\n",
            "./n1313361300000727.jpg\n",
            "./n1313361300000728.jpg\n",
            "./n1313361300000730.jpg\n",
            "./n1313361300000731.jpg\n",
            "./n1313361300000732.jpg\n",
            "./n1313361300000733.jpg\n",
            "./n1313361300000734.jpg\n",
            "./n1313361300000735.jpg\n",
            "./n1313361300000736.jpg\n",
            "./n1313361300000738.jpg\n",
            "./n1313361300000739.jpg\n",
            "./n1313361300000743.jpg\n",
            "./n1313361300000747.jpg\n",
            "./n1313361300000749.jpg\n",
            "./n1313361300000751.jpg\n",
            "./n1313361300000752.jpg\n",
            "./n1313361300000755.jpg\n",
            "./n1313361300000758.jpg\n",
            "./n1313361300000764.jpg\n",
            "./n1313361300000768.jpg\n",
            "./n1313361300000769.jpg\n",
            "./n1313361300000771.jpg\n",
            "./n1313361300000775.jpg\n",
            "./n1313361300000778.jpg\n",
            "./n1313361300000779.jpg\n",
            "./n1313361300000780.jpg\n",
            "./n1313361300000793.jpg\n",
            "./n1313361300000795.jpg\n",
            "./n1313361300000796.jpg\n",
            "./n1313361300000797.jpg\n",
            "./n1313361300000801.jpg\n",
            "./n1313361300000802.jpg\n",
            "./n1313361300000805.jpg\n",
            "./n1313361300000809.jpg\n",
            "./n1313361300000815.jpg\n",
            "./n1313361300000818.jpg\n",
            "./n1313361300000820.jpg\n",
            "./n1313361300000825.jpg\n",
            "./n1313361300000830.jpg\n",
            "./n1313361300000831.jpg\n",
            "./n1313361300000834.jpg\n",
            "./n1313361300000835.jpg\n",
            "./n1313361300000836.jpg\n",
            "./n1313361300000838.jpg\n",
            "./n1313361300000841.jpg\n",
            "./n1313361300000842.jpg\n",
            "./n1313361300000843.jpg\n",
            "./n1313361300000846.jpg\n",
            "./n1313361300000847.jpg\n",
            "./n1313361300000848.jpg\n",
            "./n1313361300000849.jpg\n",
            "./n1313361300000851.jpg\n",
            "./n1313361300000852.jpg\n",
            "./n1313361300000855.jpg\n",
            "./n1313361300000857.jpg\n",
            "./n1313361300000862.jpg\n",
            "./n1313361300000864.jpg\n",
            "./n1313361300000865.jpg\n",
            "./n1313361300000866.jpg\n",
            "./n1313361300000867.jpg\n",
            "./n1313361300000871.jpg\n",
            "./n1313361300000872.jpg\n",
            "./n1313361300000873.jpg\n",
            "./n1313361300000880.jpg\n",
            "./n1313361300000882.jpg\n",
            "./n1313361300000884.jpg\n",
            "./n1313361300000885.jpg\n",
            "./n1313361300000886.jpg\n",
            "./n1313361300000890.jpg\n",
            "./n1313361300000892.jpg\n",
            "./n1313361300000894.jpg\n",
            "./n1313361300000898.jpg\n",
            "./n1313361300000899.jpg\n",
            "./n1313361300000901.jpg\n",
            "./n1313361300000902.jpg\n",
            "./n1313361300000911.jpg\n",
            "./n1313361300000912.jpg\n",
            "./n1313361300000913.jpg\n",
            "./n1313361300000915.jpg\n",
            "./n1313361300000918.jpg\n",
            "./n1313361300000921.jpg\n",
            "./n1313361300000922.jpg\n",
            "./n1313361300000923.jpg\n",
            "./n1313361300000929.jpg\n",
            "./n1313361300000930.jpg\n",
            "./n1313361300000932.jpg\n",
            "./n1313361300000934.jpg\n",
            "./n1313361300000938.jpg\n",
            "./n1313361300000939.jpg\n",
            "./n1313361300000940.jpg\n",
            "./n1313361300000947.jpg\n",
            "./n1313361300000949.jpg\n",
            "./n1313361300000950.jpg\n",
            "./n1313361300000951.jpg\n",
            "./n1313361300000952.jpg\n",
            "./n1313361300000953.jpg\n",
            "./n1313361300000956.jpg\n",
            "./n1313361300000961.jpg\n",
            "./n1313361300000962.jpg\n",
            "./n1313361300000963.jpg\n",
            "./n1313361300000964.jpg\n",
            "./n1313361300000965.jpg\n",
            "./n1313361300000966.jpg\n",
            "./n1313361300000974.jpg\n",
            "./n1313361300000979.jpg\n",
            "./n1313361300000980.jpg\n",
            "./n1313361300000982.jpg\n",
            "./n1313361300000984.jpg\n",
            "./n1313361300000985.jpg\n",
            "./n1313361300000989.jpg\n",
            "./n1313361300000991.jpg\n",
            "./n1313361300000992.jpg\n",
            "./n1313361300000993.jpg\n",
            "./n1313361300000995.jpg\n",
            "./n1313361300001001.jpg\n",
            "./n1313361300001002.jpg\n",
            "./n1313361300001003.jpg\n",
            "./n1313361300001006.jpg\n",
            "./n1313361300001010.jpg\n",
            "./n1313361300001013.jpg\n",
            "./n1313361300001014.jpg\n",
            "./n1313361300001017.jpg\n",
            "./n1313361300001019.jpg\n",
            "./n1313361300001022.jpg\n",
            "./n1313361300001023.jpg\n",
            "./n1313361300001027.jpg\n",
            "./n1313361300001029.jpg\n",
            "./n1313361300001030.jpg\n",
            "./n1313361300001033.jpg\n",
            "./n1313361300001034.jpg\n",
            "./n1313361300001035.jpg\n",
            "./n1313361300001039.jpg\n",
            "./n1313361300001041.jpg\n",
            "./n1313361300001043.jpg\n",
            "./n1313361300001047.jpg\n",
            "./n1313361300001048.jpg\n",
            "./n1313361300001051.jpg\n",
            "./n1313361300001053.jpg\n",
            "./n1313361300001054.jpg\n",
            "./n1313361300001060.jpg\n",
            "./n1313361300001063.jpg\n",
            "./n1313361300001064.jpg\n",
            "./n1313361300001066.jpg\n",
            "./n1313361300001067.jpg\n",
            "./n1313361300001068.jpg\n",
            "./n1313361300001073.jpg\n",
            "./n1313361300001078.jpg\n",
            "./n1313361300001079.jpg\n",
            "./n1313361300001081.jpg\n",
            "./n1313361300001082.jpg\n",
            "./n1313361300001085.jpg\n",
            "./n1313361300001088.jpg\n",
            "./n1313361300001089.jpg\n",
            "./n1313361300001090.jpg\n",
            "./n1313361300001094.jpg\n",
            "./n1313361300001095.jpg\n",
            "./n1313361300001098.jpg\n",
            "./n1313361300001100.jpg\n",
            "./n1313361300001101.jpg\n",
            "./n1313361300001102.jpg\n",
            "./n1313361300001110.jpg\n",
            "./n1313361300001111.jpg\n",
            "./n1313361300001112.jpg\n",
            "./n1313361300001113.jpg\n",
            "./n1313361300001117.jpg\n",
            "./n1313361300001118.jpg\n",
            "./n1313361300001119.jpg\n",
            "./n1313361300001120.jpg\n",
            "./n1313361300001122.jpg\n",
            "./n1313361300001125.jpg\n",
            "./n1313361300001126.jpg\n",
            "./n1313361300001127.jpg\n",
            "./n1313361300001130.jpg\n",
            "./n1313361300001131.jpg\n",
            "./n1313361300001134.jpg\n",
            "./n1313361300001136.jpg\n",
            "./n1313361300001137.jpg\n",
            "./n1313361300001144.jpg\n",
            "./n1313361300001145.jpg\n",
            "./n1313361300001146.jpg\n",
            "./n1313361300001147.jpg\n",
            "./n1313361300001148.jpg\n",
            "./n1313361300001149.jpg\n",
            "./n1313361300001150.jpg\n",
            "./n1313361300001153.jpg\n",
            "./n1313361300001154.jpg\n",
            "./n1313361300001157.jpg\n",
            "./n1313361300001158.jpg\n",
            "./n1313361300001160.jpg\n",
            "./n1313361300001161.jpg\n",
            "./n1313361300001162.jpg\n",
            "./n1313361300001165.jpg\n",
            "./n1313361300001167.jpg\n",
            "./n1313361300001168.jpg\n",
            "./n1313361300001170.jpg\n",
            "./n1313361300001171.jpg\n",
            "./n1313361300001172.jpg\n",
            "./n1313361300001173.jpg\n",
            "./n1313361300001179.jpg\n",
            "./n1313361300001181.jpg\n",
            "./n1313361300001182.jpg\n",
            "./n1313361300001187.jpg\n",
            "./n1313361300001188.jpg\n",
            "./n1313361300001190.jpg\n",
            "./n1313361300001191.jpg\n",
            "./n1313361300001192.jpg\n",
            "./n1313361300001194.jpg\n",
            "./n1313361300001196.jpg\n",
            "./n1313361300001198.jpg\n",
            "./n1313361300001200.jpg\n",
            "./n1313361300001201.jpg\n",
            "./n1313361300001203.jpg\n",
            "./n1313361300001206.jpg\n",
            "./n1313361300001207.jpg\n",
            "./n1313361300001208.jpg\n",
            "./n1313361300001212.jpg\n",
            "./n1313361300001214.jpg\n",
            "./n1313361300001221.jpg\n",
            "./n1313361300001222.jpg\n",
            "./n1313361300001224.jpg\n",
            "./n1313361300001227.jpg\n",
            "./n1313361300001228.jpg\n",
            "./n1313361300001231.jpg\n",
            "./n1313361300001232.jpg\n",
            "./n1313361300001239.jpg\n",
            "./n1313361300001240.jpg\n",
            "./n1313361300001241.jpg\n",
            "./n1313361300001242.jpg\n",
            "./n1313361300001243.jpg\n",
            "./n1313361300001249.jpg\n",
            "./n1313361300001251.jpg\n",
            "./n1313361300001252.jpg\n",
            "./n1313361300001262.jpg\n",
            "./n1313361300001265.jpg\n",
            "./n1313361300001267.jpg\n",
            "./n1313361300001269.jpg\n",
            "./n1313361300001270.jpg\n",
            "./n1313361300001271.jpg\n",
            "./n1313361300001272.jpg\n",
            "./n1313361300001275.jpg\n",
            "./n1313361300001276.jpg\n",
            "./n1313361300001279.jpg\n",
            "./n1313361300001282.jpg\n",
            "./n1313361300001285.jpg\n",
            "./n1313361300001286.jpg\n",
            "./n1313361300001288.jpg\n",
            "./n1313361300001290.jpg\n",
            "./n1313361300001296.jpg\n",
            "./n1313361300001297.jpg\n",
            "./n1313361300001299.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Base path where your files are located\n",
        "base_path = \"/content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/\"\n",
        "\n",
        "# Output path for organized dataset\n",
        "output_path = os.path.join(base_path, 'organized')\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Read CSV files\n",
        "print(\"Reading CSV files...\")\n",
        "train_df = pd.read_csv(os.path.join(base_path, 'train.csv'))\n",
        "val_df = pd.read_csv(os.path.join(base_path, 'val.csv'))\n",
        "test_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\n",
        "\n",
        "# Process each split\n",
        "for split_name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f\"\\nProcessing {split_name} split...\")\n",
        "\n",
        "    # Get unique classes\n",
        "    classes = df['label'].unique()\n",
        "    print(f\"Found {len(classes)} classes in {split_name}\")\n",
        "\n",
        "    # Create directories for each class\n",
        "    for class_name in tqdm(classes, desc=\"Creating class directories\"):\n",
        "        class_dir = os.path.join(output_path, split_name, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        # Get all images for this class\n",
        "        class_images = df[df['label'] == class_name]['filename'].tolist()\n",
        "\n",
        "        # Move images to appropriate directory\n",
        "        for img_name in class_images:\n",
        "            src_path = os.path.join(base_path, 'images', img_name)\n",
        "            dst_path = os.path.join(class_dir, img_name)\n",
        "\n",
        "            if os.path.exists(src_path):\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "            else:\n",
        "                print(f\"Warning: {src_path} not found\")\n",
        "\n",
        "print(\"\\nOrganization complete!\")\n",
        "\n",
        "# Print some statistics\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    split_path = os.path.join(output_path, split_name)\n",
        "    if os.path.exists(split_path):\n",
        "        num_classes = len(os.listdir(split_path))\n",
        "        total_images = sum(len(os.listdir(os.path.join(split_path, class_name)))\n",
        "                         for class_name in os.listdir(split_path))\n",
        "        print(f\"\\n{split_name} split statistics:\")\n",
        "        print(f\"Number of classes: {num_classes}\")\n",
        "        print(f\"Total images: {total_images}\")"
      ],
      "metadata": {
        "id": "b8CqNHW6fnKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/organized/test /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/test\n",
        "!mv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/organized/val /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/val\n",
        "!mv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/organized/train /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/train"
      ],
      "metadata": {
        "id": "41d3wfP-hfBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/organized /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images"
      ],
      "metadata": {
        "id": "rdnQY7mygLQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqgfzU5ElCLi"
      },
      "source": [
        "# MAML Classifier Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oanV_B5TD5Zj"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class GradientDescentLearningRule(nn.Module):\n",
        "    \"\"\"Simple (stochastic) gradient descent learning rule.\n",
        "    For a scalar error function `E(p[0], p_[1] ... )` of some set of\n",
        "    potentially multidimensional parameters this attempts to find a local\n",
        "    minimum of the loss function by applying updates to each parameter of the\n",
        "    form\n",
        "        p[i] := p[i] - learning_rate * dE/dp[i]\n",
        "    With `learning_rate` a positive scaling parameter.\n",
        "    The error function used in successive applications of these updates may be\n",
        "    a stochastic estimator of the true error function (e.g. when the error with\n",
        "    respect to only a subset of data-points is calculated) in which case this\n",
        "    will correspond to a stochastic gradient descent learning rule.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, learning_rate=1e-3):\n",
        "        \"\"\"Creates a new learning rule object.\n",
        "        Args:\n",
        "            learning_rate: A postive scalar to scale gradient updates to the\n",
        "                parameters by. This needs to be carefully set - if too large\n",
        "                the learning dynamic will be unstable and may diverge, while\n",
        "                if set too small learning will proceed very slowly.\n",
        "        \"\"\"\n",
        "        super(GradientDescentLearningRule, self).__init__()\n",
        "        assert learning_rate > 0., 'learning_rate should be positive.'\n",
        "        self.learning_rate = torch.ones(1) * learning_rate\n",
        "        self.learning_rate.to(device)\n",
        "\n",
        "    def update_params(self, names_weights_dict, names_grads_wrt_params_dict, num_step, tau=0.9):\n",
        "        \"\"\"Applies a single gradient descent update to all parameters.\n",
        "        All parameter updates are performed using in-place operations and so\n",
        "        nothing is returned.\n",
        "        Args:\n",
        "            grads_wrt_params: A list of gradients of the scalar loss function\n",
        "                with respect to each of the parameters passed to `initialise`\n",
        "                previously, with this list expected to be in the same order.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            key: names_weights_dict[key]\n",
        "            - self.learning_rate * names_grads_wrt_params_dict[key]\n",
        "            for key in names_weights_dict.keys()\n",
        "        }\n",
        "\n",
        "\n",
        "class LSLRGradientDescentLearningRule(nn.Module):\n",
        "    \"\"\"Simple (stochastic) gradient descent learning rule.\n",
        "    For a scalar error function `E(p[0], p_[1] ... )` of some set of\n",
        "    potentially multidimensional parameters this attempts to find a local\n",
        "    minimum of the loss function by applying updates to each parameter of the\n",
        "    form\n",
        "        p[i] := p[i] - learning_rate * dE/dp[i]\n",
        "    With `learning_rate` a positive scaling parameter.\n",
        "    The error function used in successive applications of these updates may be\n",
        "    a stochastic estimator of the true error function (e.g. when the error with\n",
        "    respect to only a subset of data-points is calculated) in which case this\n",
        "    will correspond to a stochastic gradient descent learning rule.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, total_num_inner_loop_steps, use_learnable_learning_rates, init_learning_rate=1e-3):\n",
        "        \"\"\"Creates a new learning rule object.\n",
        "        Args:\n",
        "            init_learning_rate: A postive scalar to scale gradient updates to the\n",
        "                parameters by. This needs to be carefully set - if too large\n",
        "                the learning dynamic will be unstable and may diverge, while\n",
        "                if set too small learning will proceed very slowly.\n",
        "        \"\"\"\n",
        "        super(LSLRGradientDescentLearningRule, self).__init__()\n",
        "        print(init_learning_rate)\n",
        "        assert init_learning_rate > 0., 'learning_rate should be positive.'\n",
        "\n",
        "        self.init_learning_rate = torch.ones(1) * init_learning_rate\n",
        "        self.init_learning_rate.to(device)\n",
        "        self.total_num_inner_loop_steps = total_num_inner_loop_steps\n",
        "        self.use_learnable_learning_rates = use_learnable_learning_rates\n",
        "\n",
        "    def initialise(self, names_weights_dict):\n",
        "        self.names_learning_rates_dict = nn.ParameterDict()\n",
        "        for idx, (key, param) in enumerate(names_weights_dict.items()):\n",
        "            self.names_learning_rates_dict[key.replace(\".\", \"-\")] = nn.Parameter(\n",
        "                data=torch.ones(self.total_num_inner_loop_steps + 1) * self.init_learning_rate,\n",
        "                requires_grad=self.use_learnable_learning_rates)\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        # for key, param in self.names_learning_rates_dict.items():\n",
        "        #     param.fill_(self.init_learning_rate)\n",
        "        pass\n",
        "\n",
        "    def update_params(self, names_weights_dict, names_grads_wrt_params_dict, num_step, tau=0.1):\n",
        "        \"\"\"Applies a single gradient descent update to all parameters.\n",
        "        All parameter updates are performed using in-place operations and so\n",
        "        nothing is returned.\n",
        "        Args:\n",
        "            grads_wrt_params: A list of gradients of the scalar loss function\n",
        "                with respect to each of the parameters passed to `initialise`\n",
        "                previously, with this list expected to be in the same order.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            key: names_weights_dict[key]\n",
        "            - self.names_learning_rates_dict[key.replace(\".\", \"-\")][num_step]\n",
        "            * names_grads_wrt_params_dict[key]\n",
        "            for key in names_grads_wrt_params_dict.keys()\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t22ksVdiybq"
      },
      "outputs": [],
      "source": [
        "class MAMLFewShotClassifier(nn.Module):\n",
        "    def __init__(self, im_shape, device, args):\n",
        "        \"\"\"\n",
        "        Initializes a MAML few shot learning system\n",
        "        :param im_shape: The images input size, in batch, c, h, w shape\n",
        "        :param device: The device to use to use the model on.\n",
        "        :param args: A namedtuple of arguments specifying various hyperparameters.\n",
        "        \"\"\"\n",
        "        super(MAMLFewShotClassifier, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.batch_size = args.batch_size\n",
        "        self.use_cuda = args.use_cuda\n",
        "        self.im_shape = im_shape\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.rng = set_torch_seed(seed=args.seed)\n",
        "        self.classifier = VGGReLUNormNetwork(im_shape=self.im_shape, num_output_classes=self.args.\n",
        "                                             num_classes_per_set,\n",
        "                                             args=args, device=device, meta_classifier=True).to(device=self.device)\n",
        "        self.task_learning_rate = args.task_learning_rate\n",
        "\n",
        "        self.inner_loop_optimizer = LSLRGradientDescentLearningRule(device=device,\n",
        "                                                                    init_learning_rate=self.task_learning_rate,\n",
        "                                                                    total_num_inner_loop_steps=self.args.number_of_training_steps_per_iter,\n",
        "                                                                    use_learnable_learning_rates=self.args.learnable_per_layer_per_step_inner_loop_learning_rate)\n",
        "        self.inner_loop_optimizer.initialise(\n",
        "            names_weights_dict=self.get_inner_loop_parameter_dict(params=self.classifier.named_parameters()))\n",
        "\n",
        "        print(\"Inner Loop parameters\")\n",
        "        for key, value in self.inner_loop_optimizer.named_parameters():\n",
        "            print(key, value.shape)\n",
        "\n",
        "        self._noise_size = 0.01\n",
        "        self._importance_noise_size = 0\n",
        "\n",
        "        self.use_cuda = args.use_cuda\n",
        "        self.device = device\n",
        "        self.args = args\n",
        "        self.to(device)\n",
        "        print(\"Outer Loop parameters\")\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                print(name, param.shape, param.device, param.requires_grad)\n",
        "\n",
        "\n",
        "        self.optimizer = optim.Adam(self.trainable_parameters(), lr=args.meta_learning_rate, amsgrad=False)\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=self.optimizer, T_max=self.args.total_epochs,\n",
        "                                                              eta_min=self.args.min_learning_rate)\n",
        "\n",
        "        self.device = torch.device('cpu')\n",
        "        if torch.cuda.is_available():\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                self.to(torch.cuda.current_device())\n",
        "                self.classifier = nn.DataParallel(module=self.classifier)\n",
        "            else:\n",
        "                self.to(torch.cuda.current_device())\n",
        "\n",
        "            self.device = torch.cuda.current_device()\n",
        "\n",
        "    def get_per_step_loss_importance_vector(self):\n",
        "        \"\"\"\n",
        "        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n",
        "        loss towards the optimization loss.\n",
        "        :return: A tensor to be used to compute the weighted average of the loss, useful for\n",
        "        the MSL (Multi Step Loss) mechanism.\n",
        "        \"\"\"\n",
        "        loss_weights = np.ones(shape=(self.args.number_of_training_steps_per_iter)) * (\n",
        "                1.0 / self.args.number_of_training_steps_per_iter)\n",
        "        decay_rate = 1.0 / self.args.number_of_training_steps_per_iter / self.args.multi_step_loss_num_epochs\n",
        "        min_value_for_non_final_losses = 0.03 / self.args.number_of_training_steps_per_iter\n",
        "        for i in range(len(loss_weights) - 1):\n",
        "            curr_value = np.maximum(loss_weights[i] - (self.current_epoch * decay_rate), min_value_for_non_final_losses)\n",
        "            loss_weights[i] = curr_value\n",
        "\n",
        "        curr_value = np.minimum(\n",
        "            loss_weights[-1] + (self.current_epoch * (self.args.number_of_training_steps_per_iter - 1) * decay_rate),\n",
        "            1.0 - ((self.args.number_of_training_steps_per_iter - 1) * min_value_for_non_final_losses))\n",
        "        loss_weights[-1] = curr_value\n",
        "\n",
        "        # loss_weights += torch.randn_like(torch.Tensor(loss_weights)) * self._importance_noise_size\n",
        "        loss_weights = torch.Tensor(loss_weights).to(device=self.device)\n",
        "\n",
        "        return loss_weights\n",
        "\n",
        "    def get_inner_loop_parameter_dict(self, params):\n",
        "        \"\"\"\n",
        "        Returns a dictionary with the parameters to use for inner loop updates.\n",
        "        :param params: A dictionary of the network's parameters.\n",
        "        :return: A dictionary of the parameters to use for the inner loop optimization process.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            name: param.to(device=self.device)\n",
        "            for name, param in params\n",
        "            if param.requires_grad\n",
        "            and (\n",
        "                not self.args.enable_inner_loop_optimizable_bn_params\n",
        "                and \"norm_layer\" not in name\n",
        "                or self.args.enable_inner_loop_optimizable_bn_params\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def apply_inner_loop_update(self, loss, names_weights_copy, use_second_order, current_step_idx):\n",
        "        \"\"\"\n",
        "        Applies an inner loop update given current step's loss, the weights to update, a flag indicating whether to use\n",
        "        second order derivatives and the current step's index.\n",
        "        :param loss: Current step's loss with respect to the support set.\n",
        "        :param names_weights_copy: A dictionary with names to parameters to update.\n",
        "        :param use_second_order: A boolean flag of whether to use second order derivatives.\n",
        "        :param current_step_idx: Current step's index.\n",
        "        :return: A dictionary with the updated weights (name, param)\n",
        "        \"\"\"\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        if num_gpus > 1:\n",
        "            self.classifier.module.zero_grad(params=names_weights_copy)\n",
        "        else:\n",
        "            self.classifier.zero_grad(params=names_weights_copy)\n",
        "\n",
        "        grads = torch.autograd.grad(loss, names_weights_copy.values(),\n",
        "                                    create_graph=use_second_order, allow_unused=True)\n",
        "        names_grads_copy = dict(zip(names_weights_copy.keys(), grads))\n",
        "\n",
        "        names_weights_copy = {key: value[0] for key, value in names_weights_copy.items()}\n",
        "\n",
        "        for key, grad in names_grads_copy.items():\n",
        "            if grad is None:\n",
        "                print('Grads not found for inner loop parameter', key)\n",
        "            names_grads_copy[key] = names_grads_copy[key].sum(dim=0)\n",
        "\n",
        "\n",
        "        names_weights_copy = self.inner_loop_optimizer.update_params(names_weights_dict=names_weights_copy,\n",
        "                                                                     names_grads_wrt_params_dict=names_grads_copy,\n",
        "                                                                     num_step=current_step_idx)\n",
        "\n",
        "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "        names_weights_copy = {\n",
        "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
        "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
        "            name, value in names_weights_copy.items()}\n",
        "\n",
        "\n",
        "        return names_weights_copy\n",
        "\n",
        "    def get_across_task_loss_metrics(self, total_losses, total_accuracies):\n",
        "        losses = {'loss': torch.mean(torch.stack(total_losses))}\n",
        "\n",
        "        losses['accuracy'] = np.mean(total_accuracies)\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def forward(self, data_batch, epoch, use_second_order, use_multi_step_loss_optimization, num_steps, training_phase):\n",
        "        \"\"\"\n",
        "        Runs a forward outer loop pass on the batch of tasks using the MAML/++ framework.\n",
        "        :param data_batch: A data batch containing the support and target sets.\n",
        "        :param epoch: Current epoch's index\n",
        "        :param use_second_order: A boolean saying whether to use second order derivatives.\n",
        "        :param use_multi_step_loss_optimization: Whether to optimize on the outer loop using just the last step's\n",
        "        target loss (True) or whether to use multi step loss which improves the stability of the system (False)\n",
        "        :param num_steps: Number of inner loop steps.\n",
        "        :param training_phase: Whether this is a training phase (True) or an evaluation phase (False)\n",
        "        :return: A dictionary with the collected losses of the current outer forward propagation.\n",
        "        \"\"\"\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set = data_batch\n",
        "\n",
        "        [b, ncs, spc] = y_support_set.shape\n",
        "\n",
        "        self.num_classes_per_set = ncs\n",
        "\n",
        "        total_losses = []\n",
        "        total_accuracies = []\n",
        "        per_task_target_preds = [[] for i in range(len(x_target_set))]\n",
        "        self.classifier.zero_grad()\n",
        "        task_accuracies = []\n",
        "        for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
        "                              y_support_set,\n",
        "                              x_target_set,\n",
        "                              y_target_set)):\n",
        "            task_losses = []\n",
        "            per_step_loss_importance_vectors = self.get_per_step_loss_importance_vector()\n",
        "            names_weights_copy = self.get_inner_loop_parameter_dict(self.classifier.named_parameters())\n",
        "\n",
        "            num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "\n",
        "            names_weights_copy = {\n",
        "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
        "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
        "                name, value in names_weights_copy.items()}\n",
        "\n",
        "            n, s, c, h, w = x_target_set_task.shape\n",
        "\n",
        "            x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
        "            y_support_set_task = y_support_set_task.view(-1)\n",
        "            x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
        "            y_target_set_task = y_target_set_task.view(-1)\n",
        "\n",
        "            for num_step in range(num_steps):\n",
        "\n",
        "                support_loss, support_preds = self.net_forward(\n",
        "                    x=x_support_set_task,\n",
        "                    y=y_support_set_task,\n",
        "                    weights=names_weights_copy,\n",
        "                    backup_running_statistics=num_step == 0,\n",
        "                    training=True,\n",
        "                    num_step=num_step,\n",
        "                )\n",
        "\n",
        "\n",
        "                names_weights_copy = self.apply_inner_loop_update(loss=support_loss,\n",
        "                                                                  names_weights_copy=names_weights_copy,\n",
        "                                                                  use_second_order=use_second_order,\n",
        "                                                                  current_step_idx=num_step)\n",
        "\n",
        "                if use_multi_step_loss_optimization and training_phase and epoch < self.args.multi_step_loss_num_epochs:\n",
        "                    target_loss, target_preds = self.net_forward(x=x_target_set_task,\n",
        "                                                                 y=y_target_set_task, weights=names_weights_copy,\n",
        "                                                                 backup_running_statistics=False, training=True,\n",
        "                                                                 num_step=num_step)\n",
        "\n",
        "                    task_losses.append(per_step_loss_importance_vectors[num_step] * target_loss)\n",
        "                elif num_step == (self.args.number_of_training_steps_per_iter - 1):\n",
        "                    target_loss, target_preds = self.net_forward(x=x_target_set_task,\n",
        "                                                                 y=y_target_set_task, weights=names_weights_copy,\n",
        "                                                                 backup_running_statistics=False, training=True,\n",
        "                                                                 num_step=num_step)\n",
        "                    task_losses.append(target_loss)\n",
        "\n",
        "            per_task_target_preds[task_id] = target_preds.detach().cpu().numpy()\n",
        "            _, predicted = torch.max(target_preds.data, 1)\n",
        "\n",
        "            accuracy = predicted.float().eq(y_target_set_task.data.float()).cpu().float()\n",
        "            task_losses = torch.sum(torch.stack(task_losses))\n",
        "            total_losses.append(task_losses)\n",
        "            total_accuracies.extend(accuracy)\n",
        "\n",
        "            if not training_phase:\n",
        "                self.classifier.restore_backup_stats()\n",
        "\n",
        "        losses = self.get_across_task_loss_metrics(total_losses=total_losses,\n",
        "                                                   total_accuracies=total_accuracies)\n",
        "\n",
        "        for idx, item in enumerate(per_step_loss_importance_vectors):\n",
        "            losses['loss_importance_vector_{}'.format(idx)] = item.detach().cpu().numpy()\n",
        "\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def net_forward(self, x, y, weights, backup_running_statistics, training, num_step):\n",
        "        \"\"\"\n",
        "        A base model forward pass on some data points x. Using the parameters in the weights dictionary. Also requires\n",
        "        boolean flags indicating whether to reset the running statistics at the end of the run (if at evaluation phase).\n",
        "        A flag indicating whether this is the training session and an int indicating the current step's number in the\n",
        "        inner loop.\n",
        "        :param x: A data batch of shape b, c, h, w\n",
        "        :param y: A data targets batch of shape b, n_classes\n",
        "        :param weights: A dictionary containing the weights to pass to the network.\n",
        "        :param backup_running_statistics: A flag indicating whether to reset the batch norm running statistics to their\n",
        "         previous values after the run (only for evaluation)\n",
        "        :param training: A flag indicating whether the current process phase is a training or evaluation.\n",
        "        :param num_step: An integer indicating the number of the step in the inner loop.\n",
        "        :return: the crossentropy losses with respect to the given y, the predictions of the base model.\n",
        "        \"\"\"\n",
        "        preds = self.classifier.forward(x=x, params=weights,\n",
        "                                        training=training,\n",
        "                                        backup_running_statistics=backup_running_statistics, num_step=num_step)\n",
        "\n",
        "        loss = F.cross_entropy(input=preds, target=y)\n",
        "\n",
        "        return loss, preds\n",
        "\n",
        "    def trainable_parameters(self):\n",
        "        \"\"\"\n",
        "        Returns an iterator over the trainable parameters of the model.\n",
        "        \"\"\"\n",
        "        for param in self.parameters():\n",
        "            if param.requires_grad:\n",
        "                noise = torch.randn_like(param) * self._noise_size  # TODO EXPERIMENT WITH THIS\n",
        "                print(f\"the grinch added {noise} amount of noise :)\")\n",
        "                param.data.add_(noise)\n",
        "                yield param\n",
        "\n",
        "    def train_forward_prop(self, data_batch, epoch):\n",
        "        \"\"\"\n",
        "        Runs an outer loop forward prop using the meta-model and base-model.\n",
        "        :param data_batch: A data batch containing the support set and the target set input, output pairs.\n",
        "        :param epoch: The index of the currrent epoch.\n",
        "        :return: A dictionary of losses for the current step.\n",
        "        \"\"\"\n",
        "        losses, per_task_target_preds = self.forward(data_batch=data_batch, epoch=epoch,\n",
        "                                                     use_second_order=self.args.second_order and\n",
        "                                                                      epoch > self.args.first_order_to_second_order_epoch,\n",
        "                                                     use_multi_step_loss_optimization=self.args.use_multi_step_loss_optimization,\n",
        "                                                     num_steps=self.args.number_of_training_steps_per_iter,\n",
        "                                                     training_phase=True)\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def evaluation_forward_prop(self, data_batch, epoch):\n",
        "        \"\"\"\n",
        "        Runs an outer loop evaluation forward prop using the meta-model and base-model.\n",
        "        :param data_batch: A data batch containing the support set and the target set input, output pairs.\n",
        "        :param epoch: The index of the currrent epoch.\n",
        "        :return: A dictionary of losses for the current step.\n",
        "        \"\"\"\n",
        "        losses, per_task_target_preds = self.forward(data_batch=data_batch, epoch=epoch, use_second_order=False,\n",
        "                                                     use_multi_step_loss_optimization=True,\n",
        "                                                     num_steps=self.args.number_of_evaluation_steps_per_iter,\n",
        "                                                     training_phase=False)\n",
        "\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def meta_update(self, loss):\n",
        "        \"\"\"\n",
        "        Applies an outer loop update on the meta-parameters of the model.\n",
        "        :param loss: The current crossentropy loss.\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if 'imagenet' in self.args.dataset_name:\n",
        "            for name, param in self.classifier.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    param.grad.data.clamp_(-10, 10)  # not sure if this is necessary, more experiments are needed\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def run_train_iter(self, data_batch, epoch):\n",
        "        \"\"\"\n",
        "        Runs an outer loop update step on the meta-model's parameters.\n",
        "        :param data_batch: input data batch containing the support set and target set input, output pairs\n",
        "        :param epoch: the index of the current epoch\n",
        "        :return: The losses of the ran iteration.\n",
        "        \"\"\"\n",
        "        epoch = int(epoch)\n",
        "        if epoch > 1:\n",
        "          self.scheduler.step(epoch=epoch)\n",
        "        if self.current_epoch != epoch:\n",
        "            self.current_epoch = epoch\n",
        "\n",
        "        if not self.training:\n",
        "            self.train()\n",
        "\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set = data_batch\n",
        "\n",
        "        x_support_set = torch.Tensor(x_support_set).float().to(device=self.device)\n",
        "        x_target_set = torch.Tensor(x_target_set).float().to(device=self.device)\n",
        "        y_support_set = torch.Tensor(y_support_set).long().to(device=self.device)\n",
        "        y_target_set = torch.Tensor(y_target_set).long().to(device=self.device)\n",
        "\n",
        "        data_batch = (x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        losses, per_task_target_preds = self.train_forward_prop(data_batch=data_batch, epoch=epoch)\n",
        "\n",
        "        self.meta_update(loss=losses['loss'])\n",
        "        losses['learning_rate'] = self.scheduler.get_last_lr()[0]\n",
        "        self.optimizer.zero_grad()\n",
        "        self.zero_grad()\n",
        "\n",
        "        self._noise_size *= 0.9999\n",
        "        print(f\"noise size: {self._noise_size}\")\n",
        "\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def run_validation_iter(self, data_batch):\n",
        "        \"\"\"\n",
        "        Runs an outer loop evaluation step on the meta-model's parameters.\n",
        "        :param data_batch: input data batch containing the support set and target set input, output pairs\n",
        "        :param epoch: the index of the current epoch\n",
        "        :return: The losses of the ran iteration.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.training:\n",
        "            self.eval()\n",
        "\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set = data_batch\n",
        "\n",
        "        x_support_set = torch.Tensor(x_support_set).float().to(device=self.device)\n",
        "        x_target_set = torch.Tensor(x_target_set).float().to(device=self.device)\n",
        "        y_support_set = torch.Tensor(y_support_set).long().to(device=self.device)\n",
        "        y_target_set = torch.Tensor(y_target_set).long().to(device=self.device)\n",
        "\n",
        "        data_batch = (x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        losses, per_task_target_preds = self.evaluation_forward_prop(data_batch=data_batch, epoch=self.current_epoch)\n",
        "\n",
        "        # losses['loss'].backward() # uncomment if you get the weird memory error\n",
        "        # self.zero_grad()\n",
        "        # self.optimizer.zero_grad()\n",
        "\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def save_model(self, model_save_dir, state):\n",
        "        \"\"\"\n",
        "        Save the network parameter state and experiment state dictionary.\n",
        "        :param model_save_dir: The directory to store the state at.\n",
        "        :param state: The state containing the experiment state and the network. It's in the form of a dictionary\n",
        "        object.\n",
        "        \"\"\"\n",
        "        state['network'] = self.state_dict()\n",
        "        state['optimizer'] = self.optimizer.state_dict()\n",
        "        torch.save(state, f=model_save_dir)\n",
        "\n",
        "    def load_model(self, model_save_dir, model_name, model_idx):\n",
        "        \"\"\"\n",
        "        Load checkpoint and return the state dictionary containing the network state params and experiment state.\n",
        "        :param model_save_dir: The directory from which to load the files.\n",
        "        :param model_name: The model_name to be loaded from the direcotry.\n",
        "        :param model_idx: The index of the model (i.e. epoch number or 'latest' for the latest saved model of the current\n",
        "        experiment)\n",
        "        :return: A dictionary containing the experiment state and the saved model parameters.\n",
        "        \"\"\"\n",
        "        filepath = os.path.join(model_save_dir, \"{}_{}\".format(model_name, model_idx))\n",
        "        state = torch.load(filepath)\n",
        "        state_dict_loaded = state['network']\n",
        "        self.optimizer.load_state_dict(state['optimizer'])\n",
        "        self.load_state_dict(state_dict=state_dict_loaded)\n",
        "        return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTJUQiySnDQi"
      },
      "source": [
        "# Layer Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w0JKoFtCnWZZ"
      },
      "outputs": [],
      "source": [
        "import numbers\n",
        "from copy import copy\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_ZTbS3zjnOD5"
      },
      "outputs": [],
      "source": [
        "def extract_top_level_dict(current_dict):\n",
        "    \"\"\"\n",
        "    Builds a graph dictionary from the passed depth_keys, value pair. Useful for dynamically passing external params\n",
        "    :param depth_keys: A list of strings making up the name of a variable. Used to make a graph for that params tree.\n",
        "    :param value: Param value\n",
        "    :param key_exists: If none then assume new dict, else load existing dict and add new key->value pairs to it.\n",
        "    :return: A dictionary graph of the params already added to the graph.\n",
        "    \"\"\"\n",
        "    output_dict = {}\n",
        "    for key in current_dict.keys():\n",
        "        name = key.replace(\"layer_dict.\", \"\")\n",
        "        name = name.replace(\"layer_dict.\", \"\")\n",
        "        name = name.replace(\"block_dict.\", \"\")\n",
        "        name = name.replace(\"module-\", \"\")\n",
        "        top_level = name.split(\".\")[0]\n",
        "        sub_level = \".\".join(name.split(\".\")[1:])\n",
        "\n",
        "        if top_level in output_dict:\n",
        "            new_item = {key: value for key, value in output_dict[top_level].items()}\n",
        "            new_item[sub_level] = current_dict[key]\n",
        "            output_dict[top_level] = new_item\n",
        "\n",
        "        elif sub_level == \"\":\n",
        "            output_dict[top_level] = current_dict[key]\n",
        "        else:\n",
        "            output_dict[top_level] = {sub_level: current_dict[key]}\n",
        "    #print(current_dict.keys(), output_dict.keys())\n",
        "    return output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y1N7ERJmnZpx"
      },
      "outputs": [],
      "source": [
        "class MetaConv2dLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, use_bias, groups=1, dilation_rate=1):\n",
        "        \"\"\"\n",
        "        A MetaConv2D layer. Applies the same functionality of a standard Conv2D layer with the added functionality of\n",
        "        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n",
        "        weights instead of the internal ones stored in the conv layer. Useful for inner loop optimization in the meta\n",
        "        learning setting.\n",
        "        :param in_channels: Number of input channels\n",
        "        :param out_channels: Number of output channels\n",
        "        :param kernel_size: Convolutional kernel size\n",
        "        :param stride: Convolutional stride\n",
        "        :param padding: Convolution padding\n",
        "        :param use_bias: Boolean indicating whether to use a bias or not.\n",
        "        \"\"\"\n",
        "        super(MetaConv2dLayer, self).__init__()\n",
        "        num_filters = out_channels\n",
        "        self.stride = int(stride)\n",
        "        self.padding = int(padding)\n",
        "        self.dilation_rate = int(dilation_rate)\n",
        "        self.use_bias = use_bias\n",
        "        self.groups = int(groups)\n",
        "        self.weight = nn.Parameter(torch.empty(num_filters, in_channels, kernel_size, kernel_size))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(num_filters))\n",
        "\n",
        "    def forward(self, x, params=None):\n",
        "        \"\"\"\n",
        "        Applies a conv2D forward pass. If params are not None will use the passed params as the conv weights and biases\n",
        "        :param x: Input image batch.\n",
        "        :param params: If none, then conv layer will use the stored self.weights and self.bias, if they are not none\n",
        "        then the conv layer will use the passed params as its parameters.\n",
        "        :return: The output of a convolutional function.\n",
        "        \"\"\"\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "            if self.use_bias:\n",
        "                (weight, bias) = params[\"weight\"], params[\"bias\"]\n",
        "            else:\n",
        "                (weight) = params[\"weight\"]\n",
        "                bias = None\n",
        "        elif self.use_bias:\n",
        "            weight, bias = self.weight, self.bias\n",
        "        else:\n",
        "            weight = self.weight\n",
        "            bias = None\n",
        "\n",
        "        return F.conv2d(\n",
        "            input=x,\n",
        "            weight=weight,\n",
        "            bias=bias,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding,\n",
        "            dilation=self.dilation_rate,\n",
        "            groups=self.groups,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EvrE9o8Vnf6q"
      },
      "outputs": [],
      "source": [
        "class MetaLinearLayer(nn.Module):\n",
        "    def __init__(self, input_shape, num_filters, use_bias):\n",
        "        \"\"\"\n",
        "        A MetaLinear layer. Applies the same functionality of a standard linearlayer with the added functionality of\n",
        "        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n",
        "        weights instead of the internal ones stored in the linear layer. Useful for inner loop optimization in the meta\n",
        "        learning setting.\n",
        "        :param input_shape: The shape of the input data, in the form (b, f)\n",
        "        :param num_filters: Number of output filters\n",
        "        :param use_bias: Whether to use biases or not.\n",
        "        \"\"\"\n",
        "        super(MetaLinearLayer, self).__init__()\n",
        "        b, c = input_shape\n",
        "\n",
        "        self.use_bias = use_bias\n",
        "        self.weights = nn.Parameter(torch.ones(num_filters, c))\n",
        "        # nn.init.xavier_uniform_(self.weights) TODO CHANGE BACK TEST GENE (it works kinda)\n",
        "        if self.use_bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(num_filters))\n",
        "\n",
        "    def forward(self, x, params=None):\n",
        "        \"\"\"\n",
        "        Forward propagates by applying a linear function (Wx + b). If params are none then internal params are used.\n",
        "        Otherwise passed params will be used to execute the function.\n",
        "        :param x: Input data batch, in the form (b, f)\n",
        "        :param params: A dictionary containing 'weights' and 'bias'. If params are none then internal params are used.\n",
        "        Otherwise the external are used.\n",
        "        :return: The result of the linear function.\n",
        "        \"\"\"\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "            if self.use_bias:\n",
        "                (weight, bias) = params[\"weights\"], params[\"bias\"]\n",
        "            else:\n",
        "                (weight) = params[\"weights\"]\n",
        "                bias = None\n",
        "        elif self.use_bias:\n",
        "            weight, bias = self.weights, self.bias\n",
        "        else:\n",
        "            weight = self.weights\n",
        "            bias = None\n",
        "        return F.linear(input=x, weight=weight, bias=bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b4UV1OiDnkU7"
      },
      "outputs": [],
      "source": [
        "class MetaBatchNormLayer(nn.Module):\n",
        "    def __init__(self, num_features, device, args, eps=1e-5, momentum=0.1, affine=True,\n",
        "                 track_running_stats=True, meta_batch_norm=True, no_learnable_params=False,\n",
        "                 use_per_step_bn_statistics=False):\n",
        "        \"\"\"\n",
        "        A MetaBatchNorm layer. Applies the same functionality of a standard BatchNorm layer with the added functionality of\n",
        "        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n",
        "        weights instead of the internal ones stored in the conv layer. Useful for inner loop optimization in the meta\n",
        "        learning setting. Also has the additional functionality of being able to store per step running stats and per step beta and gamma.\n",
        "        :param num_features:\n",
        "        :param device:\n",
        "        :param args:\n",
        "        :param eps:\n",
        "        :param momentum:\n",
        "        :param affine:\n",
        "        :param track_running_stats:\n",
        "        :param meta_batch_norm:\n",
        "        :param no_learnable_params:\n",
        "        :param use_per_step_bn_statistics:\n",
        "        \"\"\"\n",
        "        super(MetaBatchNormLayer, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "\n",
        "        self.affine = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.meta_batch_norm = meta_batch_norm\n",
        "        self.num_features = num_features\n",
        "        self.device = device\n",
        "        self.use_per_step_bn_statistics = use_per_step_bn_statistics\n",
        "        self.args = args\n",
        "        self.learnable_gamma = self.args.learnable_bn_gamma\n",
        "        self.learnable_beta = self.args.learnable_bn_beta\n",
        "\n",
        "        if use_per_step_bn_statistics:\n",
        "            self.running_mean = nn.Parameter(torch.zeros(args.number_of_training_steps_per_iter, num_features),\n",
        "                                             requires_grad=False)\n",
        "            self.running_var = nn.Parameter(torch.ones(args.number_of_training_steps_per_iter, num_features),\n",
        "                                            requires_grad=False)\n",
        "            self.bias = nn.Parameter(torch.zeros(args.number_of_training_steps_per_iter, num_features),\n",
        "                                     requires_grad=self.learnable_beta)\n",
        "            self.weight = nn.Parameter(torch.ones(args.number_of_training_steps_per_iter, num_features),\n",
        "                                       requires_grad=self.learnable_gamma)\n",
        "        else:\n",
        "            self.running_mean = nn.Parameter(torch.zeros(num_features), requires_grad=False)\n",
        "            self.running_var = nn.Parameter(torch.zeros(num_features), requires_grad=False)\n",
        "            self.bias = nn.Parameter(torch.zeros(num_features),\n",
        "                                     requires_grad=self.learnable_beta)\n",
        "            self.weight = nn.Parameter(torch.ones(num_features),\n",
        "                                       requires_grad=self.learnable_gamma)\n",
        "\n",
        "        if self.args.enable_inner_loop_optimizable_bn_params:\n",
        "            self.bias = nn.Parameter(torch.zeros(num_features),\n",
        "                                     requires_grad=self.learnable_beta)\n",
        "            self.weight = nn.Parameter(torch.ones(num_features),\n",
        "                                       requires_grad=self.learnable_gamma)\n",
        "\n",
        "        self.backup_running_mean = torch.zeros(self.running_mean.shape)\n",
        "        self.backup_running_var = torch.ones(self.running_var.shape)\n",
        "\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def forward(self, input, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "        Forward propagates by applying a bach norm function. If params are none then internal params are used.\n",
        "        Otherwise passed params will be used to execute the function.\n",
        "        :param input: input data batch, size either can be any.\n",
        "        :param num_step: The current inner loop step being taken. This is used when we are learning per step params and\n",
        "         collecting per step batch statistics. It indexes the correct object to use for the current time-step\n",
        "        :param params: A dictionary containing 'weight' and 'bias'.\n",
        "        :param training: Whether this is currently the training or evaluation phase.\n",
        "        :param backup_running_statistics: Whether to backup the running statistics. This is used\n",
        "        at evaluation time, when after the pass is complete we want to throw away the collected validation stats.\n",
        "        :return: The result of the batch norm operation.\n",
        "        \"\"\"\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "            (weight, bias) = params[\"weight\"], params[\"bias\"]\n",
        "            #print(num_step, params['weight'])\n",
        "        else:\n",
        "            #print(num_step, \"no params\")\n",
        "            weight, bias = self.weight, self.bias\n",
        "\n",
        "        if self.use_per_step_bn_statistics:\n",
        "            running_mean = self.running_mean[num_step]\n",
        "            running_var = self.running_var[num_step]\n",
        "            if (\n",
        "                params is None\n",
        "                and not self.args.enable_inner_loop_optimizable_bn_params\n",
        "            ):\n",
        "                bias = self.bias[num_step]\n",
        "                weight = self.weight[num_step]\n",
        "        else:\n",
        "            running_mean = None\n",
        "            running_var = None\n",
        "\n",
        "\n",
        "        if backup_running_statistics and self.use_per_step_bn_statistics:\n",
        "            self.backup_running_mean.data = copy(self.running_mean.data)\n",
        "            self.backup_running_var.data = copy(self.running_var.data)\n",
        "\n",
        "        momentum = self.momentum\n",
        "\n",
        "        return F.batch_norm(input, running_mean, running_var, weight, bias,\n",
        "                              training=True, momentum=momentum, eps=self.eps)\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        \"\"\"\n",
        "        Resets batch statistics to their backup values which are collected after each forward pass.\n",
        "        \"\"\"\n",
        "        if self.use_per_step_bn_statistics:\n",
        "            self.running_mean = nn.Parameter(self.backup_running_mean.to(device=self.device), requires_grad=False)\n",
        "            self.running_var = nn.Parameter(self.backup_running_var.to(device=self.device), requires_grad=False)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n",
        "               'track_running_stats={track_running_stats}'.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6nEWVSSLnna2"
      },
      "outputs": [],
      "source": [
        "class MetaLayerNormLayer(nn.Module):\n",
        "    def __init__(self, input_feature_shape, eps=1e-5, elementwise_affine=True):\n",
        "        \"\"\"\n",
        "        A MetaLayerNorm layer. A layer that applies the same functionality as a layer norm layer with the added\n",
        "        capability of being able to receive params at inference time to use instead of the internal ones. As well as\n",
        "        being able to use its own internal weights.\n",
        "        :param input_feature_shape: The input shape without the batch dimension, e.g. c, h, w\n",
        "        :param eps: Epsilon to use for protection against overflows\n",
        "        :param elementwise_affine: Whether to learn a multiplicative interaction parameter 'w' in addition to\n",
        "        the biases.\n",
        "        \"\"\"\n",
        "        super(MetaLayerNormLayer, self).__init__()\n",
        "        if isinstance(input_feature_shape, numbers.Integral):\n",
        "            input_feature_shape = (input_feature_shape,)\n",
        "        self.normalized_shape = torch.Size(input_feature_shape)\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(*input_feature_shape), requires_grad=False)\n",
        "            self.bias = nn.Parameter(torch.Tensor(*input_feature_shape))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Reset parameters to their initialization values.\n",
        "        \"\"\"\n",
        "        if self.elementwise_affine:\n",
        "            self.weight.data.fill_(1)\n",
        "            self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "            Forward propagates by applying a layer norm function. If params are none then internal params are used.\n",
        "            Otherwise passed params will be used to execute the function.\n",
        "            :param input: input data batch, size either can be any.\n",
        "            :param num_step: The current inner loop step being taken. This is used when we are learning per step params and\n",
        "             collecting per step batch statistics. It indexes the correct object to use for the current time-step\n",
        "            :param params: A dictionary containing 'weight' and 'bias'.\n",
        "            :param training: Whether this is currently the training or evaluation phase.\n",
        "            :param backup_running_statistics: Whether to backup the running statistics. This is used\n",
        "            at evaluation time, when after the pass is complete we want to throw away the collected validation stats.\n",
        "            :return: The result of the batch norm operation.\n",
        "        \"\"\"\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "            bias = params[\"bias\"]\n",
        "        else:\n",
        "            bias = self.bias\n",
        "            #print('no inner loop params', self)\n",
        "\n",
        "        return F.layer_norm(\n",
        "            input, self.normalized_shape, self.weight, bias, self.eps)\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        pass\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{normalized_shape}, eps={eps}, ' \\\n",
        "               'elementwise_affine={elementwise_affine}'.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "j1Jj9kodnrOV"
      },
      "outputs": [],
      "source": [
        "class MetaConvNormLayerReLU(nn.Module):\n",
        "    def __init__(self, input_shape, num_filters, kernel_size, stride, padding, use_bias, args, normalization=True,\n",
        "                 meta_layer=True, no_bn_learnable_params=False, device=None):\n",
        "        \"\"\"\n",
        "           Initializes a BatchNorm->Conv->ReLU layer which applies those operation in that order.\n",
        "           :param args: A named tuple containing the system's hyperparameters.\n",
        "           :param device: The device to run the layer on.\n",
        "           :param normalization: The type of normalization to use 'batch_norm' or 'layer_norm'\n",
        "           :param meta_layer: Whether this layer will require meta-layer capabilities such as meta-batch norm,\n",
        "           meta-conv etc.\n",
        "           :param input_shape: The image input shape in the form (b, c, h, w)\n",
        "           :param num_filters: number of filters for convolutional layer\n",
        "           :param kernel_size: the kernel size of the convolutional layer\n",
        "           :param stride: the stride of the convolutional layer\n",
        "           :param padding: the bias of the convolutional layer\n",
        "           :param use_bias: whether the convolutional layer utilizes a bias\n",
        "        \"\"\"\n",
        "        super(MetaConvNormLayerReLU, self).__init__()\n",
        "        self.normalization = normalization\n",
        "        self.use_per_step_bn_statistics = args.per_step_bn_statistics\n",
        "        self.input_shape = input_shape\n",
        "        self.args = args\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "        self.meta_layer = meta_layer\n",
        "        self.no_bn_learnable_params = no_bn_learnable_params\n",
        "        self.device = device\n",
        "        self.layer_dict = nn.ModuleDict()\n",
        "        self.build_block()\n",
        "\n",
        "    def build_block(self):\n",
        "\n",
        "        x = torch.zeros(self.input_shape)\n",
        "\n",
        "        out = x\n",
        "\n",
        "        self.conv = MetaConv2dLayer(in_channels=out.shape[1], out_channels=self.num_filters,\n",
        "                                    kernel_size=self.kernel_size,\n",
        "                                    stride=self.stride, padding=self.padding, use_bias=self.use_bias)\n",
        "\n",
        "\n",
        "\n",
        "        out = self.conv(out)\n",
        "\n",
        "        if self.normalization:\n",
        "            if self.args.norm_layer == \"batch_norm\":\n",
        "                self.norm_layer = MetaBatchNormLayer(out.shape[1], track_running_stats=True,\n",
        "                                                     meta_batch_norm=self.meta_layer,\n",
        "                                                     no_learnable_params=self.no_bn_learnable_params,\n",
        "                                                     device=self.device,\n",
        "                                                     use_per_step_bn_statistics=self.use_per_step_bn_statistics,\n",
        "                                                     args=self.args)\n",
        "            elif self.args.norm_layer == \"layer_norm\":\n",
        "                self.norm_layer = MetaLayerNormLayer(input_feature_shape=out.shape[1:])\n",
        "\n",
        "            out = self.norm_layer(out, num_step=0)\n",
        "\n",
        "        out = F.leaky_relu(out)\n",
        "\n",
        "        print(out.shape)\n",
        "\n",
        "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "            Forward propagates by applying the function. If params are none then internal params are used.\n",
        "            Otherwise passed params will be used to execute the function.\n",
        "            :param input: input data batch, size either can be any.\n",
        "            :param num_step: The current inner loop step being taken. This is used when we are learning per step params and\n",
        "             collecting per step batch statistics. It indexes the correct object to use for the current time-step\n",
        "            :param params: A dictionary containing 'weight' and 'bias'.\n",
        "            :param training: Whether this is currently the training or evaluation phase.\n",
        "            :param backup_running_statistics: Whether to backup the running statistics. This is used\n",
        "            at evaluation time, when after the pass is complete we want to throw away the collected validation stats.\n",
        "            :return: The result of the batch norm operation.\n",
        "        \"\"\"\n",
        "        batch_norm_params = None\n",
        "        conv_params = None\n",
        "        activation_function_pre_params = None\n",
        "\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "\n",
        "            if self.normalization:\n",
        "                if 'norm_layer' in params:\n",
        "                    batch_norm_params = params['norm_layer']\n",
        "\n",
        "                if 'activation_function_pre' in params:\n",
        "                    activation_function_pre_params = params['activation_function_pre']\n",
        "\n",
        "            conv_params = params['conv']\n",
        "\n",
        "        out = x\n",
        "\n",
        "\n",
        "        out = self.conv(out, params=conv_params)\n",
        "\n",
        "        if self.normalization:\n",
        "            out = self.norm_layer.forward(out, num_step=num_step,\n",
        "                                          params=batch_norm_params, training=training,\n",
        "                                          backup_running_statistics=backup_running_statistics)\n",
        "\n",
        "        out = F.leaky_relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        \"\"\"\n",
        "        Restore stored statistics from the backup, replacing the current ones.\n",
        "        \"\"\"\n",
        "        if self.normalization:\n",
        "            self.norm_layer.restore_backup_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cl6609dBntk4"
      },
      "outputs": [],
      "source": [
        "class MetaNormLayerConvReLU(nn.Module):\n",
        "    def __init__(self, input_shape, num_filters, kernel_size, stride, padding, use_bias, args, normalization=True,\n",
        "                 meta_layer=True, no_bn_learnable_params=False, device=None):\n",
        "        \"\"\"\n",
        "           Initializes a BatchNorm->Conv->ReLU layer which applies those operation in that order.\n",
        "           :param args: A named tuple containing the system's hyperparameters.\n",
        "           :param device: The device to run the layer on.\n",
        "           :param normalization: The type of normalization to use 'batch_norm' or 'layer_norm'\n",
        "           :param meta_layer: Whether this layer will require meta-layer capabilities such as meta-batch norm,\n",
        "           meta-conv etc.\n",
        "           :param input_shape: The image input shape in the form (b, c, h, w)\n",
        "           :param num_filters: number of filters for convolutional layer\n",
        "           :param kernel_size: the kernel size of the convolutional layer\n",
        "           :param stride: the stride of the convolutional layer\n",
        "           :param padding: the bias of the convolutional layer\n",
        "           :param use_bias: whether the convolutional layer utilizes a bias\n",
        "        \"\"\"\n",
        "        super(MetaNormLayerConvReLU, self).__init__()\n",
        "        self.normalization = normalization\n",
        "        self.use_per_step_bn_statistics = args.per_step_bn_statistics\n",
        "        self.input_shape = input_shape\n",
        "        self.args = args\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "        self.meta_layer = meta_layer\n",
        "        self.no_bn_learnable_params = no_bn_learnable_params\n",
        "        self.device = device\n",
        "        self.layer_dict = nn.ModuleDict()\n",
        "        self.build_block()\n",
        "\n",
        "    def build_block(self):\n",
        "\n",
        "        x = torch.zeros(self.input_shape)\n",
        "\n",
        "        out = x\n",
        "        if self.normalization:\n",
        "            if self.args.norm_layer == \"batch_norm\":\n",
        "                self.norm_layer = MetaBatchNormLayer(self.input_shape[1], track_running_stats=True,\n",
        "                                                     meta_batch_norm=self.meta_layer,\n",
        "                                                     no_learnable_params=self.no_bn_learnable_params,\n",
        "                                                     device=self.device,\n",
        "                                                     use_per_step_bn_statistics=self.use_per_step_bn_statistics,\n",
        "                                                     args=self.args)\n",
        "            elif self.args.norm_layer == \"layer_norm\":\n",
        "                self.norm_layer = MetaLayerNormLayer(input_feature_shape=out.shape[1:])\n",
        "\n",
        "            out = self.norm_layer.forward(out, num_step=0)\n",
        "        self.conv = MetaConv2dLayer(in_channels=out.shape[1], out_channels=self.num_filters,\n",
        "                                    kernel_size=self.kernel_size,\n",
        "                                    stride=self.stride, padding=self.padding, use_bias=self.use_bias)\n",
        "\n",
        "\n",
        "        self.layer_dict['activation_function_pre'] = nn.LeakyReLU()\n",
        "\n",
        "\n",
        "        out = self.layer_dict['activation_function_pre'].forward(self.conv.forward(out))\n",
        "        print(out.shape)\n",
        "\n",
        "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "            Forward propagates by applying the function. If params are none then internal params are used.\n",
        "            Otherwise passed params will be used to execute the function.\n",
        "            :param input: input data batch, size either can be any.\n",
        "            :param num_step: The current inner loop step being taken. This is used when we are learning per step params and\n",
        "             collecting per step batch statistics. It indexes the correct object to use for the current time-step\n",
        "            :param params: A dictionary containing 'weight' and 'bias'.\n",
        "            :param training: Whether this is currently the training or evaluation phase.\n",
        "            :param backup_running_statistics: Whether to backup the running statistics. This is used\n",
        "            at evaluation time, when after the pass is complete we want to throw away the collected validation stats.\n",
        "            :return: The result of the batch norm operation.\n",
        "        \"\"\"\n",
        "        batch_norm_params = None\n",
        "\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "\n",
        "            if self.normalization and 'norm_layer' in params:\n",
        "                batch_norm_params = params['norm_layer']\n",
        "\n",
        "            conv_params = params['conv']\n",
        "        else:\n",
        "            conv_params = None\n",
        "            #print('no inner loop params', self)\n",
        "\n",
        "        out = x\n",
        "\n",
        "        if self.normalization:\n",
        "            out = self.norm_layer.forward(out, num_step=num_step,\n",
        "                                          params=batch_norm_params, training=training,\n",
        "                                          backup_running_statistics=backup_running_statistics)\n",
        "\n",
        "        out = self.conv.forward(out, params=conv_params)\n",
        "        out = self.layer_dict['activation_function_pre'].forward(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        \"\"\"\n",
        "        Restore stored statistics from the backup, replacing the current ones.\n",
        "        \"\"\"\n",
        "        if self.normalization:\n",
        "            self.norm_layer.restore_backup_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LiAJAkvInHY-"
      },
      "outputs": [],
      "source": [
        "class VGGReLUNormNetwork(nn.Module):\n",
        "    def __init__(self, im_shape, num_output_classes, args, device, meta_classifier=True):\n",
        "        \"\"\"\n",
        "        Builds a multilayer convolutional network. It also provides functionality for passing external parameters to be\n",
        "        used at inference time. Enables inner loop optimization readily.\n",
        "        :param im_shape: The input image batch shape.\n",
        "        :param num_output_classes: The number of output classes of the network.\n",
        "        :param args: A named tuple containing the system's hyperparameters.\n",
        "        :param device: The device to run this on.\n",
        "        :param meta_classifier: A flag indicating whether the system's meta-learning (inner-loop) functionalities should\n",
        "        be enabled.\n",
        "        \"\"\"\n",
        "        super(VGGReLUNormNetwork, self).__init__()\n",
        "        b, c, self.h, self.w = im_shape\n",
        "        self.device = device\n",
        "        self.total_layers = 0\n",
        "        self.args = args\n",
        "        self.upscale_shapes = []\n",
        "        self.cnn_filters = args.cnn_num_filters\n",
        "        self.input_shape = list(im_shape)\n",
        "        self.num_stages = args.num_stages\n",
        "        self.num_output_classes = num_output_classes\n",
        "\n",
        "        if args.max_pooling:\n",
        "            print(\"Using max pooling\")\n",
        "            self.conv_stride = 1\n",
        "        else:\n",
        "            print(\"Using strided convolutions\")\n",
        "            self.conv_stride = 2\n",
        "        self.meta_classifier = meta_classifier\n",
        "\n",
        "        self.build_network()\n",
        "        print(\"meta network params\")\n",
        "        for name, param in self.named_parameters():\n",
        "            print(name, param.shape)\n",
        "\n",
        "    def build_network(self):\n",
        "        \"\"\"\n",
        "        Builds the network before inference is required by creating some dummy inputs with the same input as the\n",
        "        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n",
        "        sets output shapes for each layer.\n",
        "        \"\"\"\n",
        "        x = torch.zeros(self.input_shape)\n",
        "        out = x\n",
        "        self.layer_dict = nn.ModuleDict()\n",
        "        self.upscale_shapes.append(x.shape)\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            self.layer_dict['conv{}'.format(i)] = MetaConvNormLayerReLU(input_shape=out.shape,\n",
        "                                                                        num_filters=self.cnn_filters,\n",
        "                                                                        kernel_size=3, stride=self.conv_stride,\n",
        "                                                                        padding=self.args.conv_padding,\n",
        "                                                                        use_bias=True, args=self.args,\n",
        "                                                                        normalization=True,\n",
        "                                                                        meta_layer=self.meta_classifier,\n",
        "                                                                        no_bn_learnable_params=False,\n",
        "                                                                        device=self.device)\n",
        "            out = self.layer_dict['conv{}'.format(i)](out, training=True, num_step=0)\n",
        "\n",
        "            if self.args.max_pooling:\n",
        "                out = F.max_pool2d(input=out, kernel_size=(2, 2), stride=2, padding=0)\n",
        "\n",
        "\n",
        "        if not self.args.max_pooling:\n",
        "            out = F.avg_pool2d(out, out.shape[2])\n",
        "\n",
        "        self.encoder_features_shape = list(out.shape)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "\n",
        "        self.layer_dict['linear'] = MetaLinearLayer(input_shape=(out.shape[0], np.prod(out.shape[1:])),\n",
        "                                                    num_filters=self.num_output_classes, use_bias=True)\n",
        "\n",
        "        out = self.layer_dict['linear'](out)\n",
        "        print(\"VGGNetwork build\", out.shape)\n",
        "\n",
        "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "        Forward propages through the network. If any params are passed then they are used instead of stored params.\n",
        "        :param x: Input image batch.\n",
        "        :param num_step: The current inner loop step number\n",
        "        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n",
        "         same as the layer names then they will be used instead.\n",
        "        :param training: Whether this is training (True) or eval time.\n",
        "        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n",
        "        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n",
        "        :return: Logits of shape b, num_output_classes.\n",
        "        \"\"\"\n",
        "        param_dict = {}\n",
        "\n",
        "        if params is not None:\n",
        "            params = {key: value[0] for key, value in params.items()}\n",
        "            param_dict = extract_top_level_dict(current_dict=params)\n",
        "\n",
        "        # print('top network', param_dict.keys())\n",
        "        for name, param in self.layer_dict.named_parameters():\n",
        "            path_bits = name.split(\".\")\n",
        "            layer_name = path_bits[0]\n",
        "            if layer_name not in param_dict:\n",
        "                param_dict[layer_name] = None\n",
        "\n",
        "        out = x\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            out = self.layer_dict['conv{}'.format(i)](out, params=param_dict['conv{}'.format(i)], training=training,\n",
        "                                                      backup_running_statistics=backup_running_statistics,\n",
        "                                                      num_step=num_step)\n",
        "            if self.args.max_pooling:\n",
        "                out = F.max_pool2d(input=out, kernel_size=(2, 2), stride=2, padding=0)\n",
        "\n",
        "        if not self.args.max_pooling:\n",
        "            out = F.avg_pool2d(out, out.shape[2])\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.layer_dict['linear'](out, param_dict['linear'])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def zero_grad(self, params=None):\n",
        "        if params is None:\n",
        "            for param in self.parameters():\n",
        "                if (\n",
        "                    param.requires_grad == True\n",
        "                    and param.grad is not None\n",
        "                    and torch.sum(param.grad) > 0\n",
        "                ):\n",
        "                    print(param.grad)\n",
        "                    param.grad.zero_()\n",
        "        else:\n",
        "            for name, param in params.items():\n",
        "                if (\n",
        "                    param.requires_grad == True\n",
        "                    and param.grad is not None\n",
        "                    and torch.sum(param.grad) > 0\n",
        "                ):\n",
        "                    print(param.grad)\n",
        "                    param.grad.zero_()\n",
        "                    params[name].grad = None\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        \"\"\"\n",
        "        Reset stored batch statistics from the stored backup.\n",
        "        \"\"\"\n",
        "        for i in range(self.num_stages):\n",
        "            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-uTHDDT0YGl"
      },
      "source": [
        "TODO:\n",
        "\n",
        "add more functionalities specific to MAML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoajsklRk8Os"
      },
      "source": [
        "# Experiment Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FmscGJS8cCUv"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "17o11EhPJ4NL"
      },
      "outputs": [],
      "source": [
        "def build_experiment_folder(experiment_name):\n",
        "    experiment_path = os.path.abspath(experiment_name)\n",
        "    saved_models_filepath = \"{}/{}\".format(experiment_path, \"saved_models\")\n",
        "    logs_filepath = \"{}/{}\".format(experiment_path, \"logs\")\n",
        "    samples_filepath = \"{}/{}\".format(experiment_path, \"visual_outputs\")\n",
        "\n",
        "    if not os.path.exists(experiment_path):\n",
        "        os.makedirs(experiment_path)\n",
        "    if not os.path.exists(logs_filepath):\n",
        "        os.makedirs(logs_filepath)\n",
        "    if not os.path.exists(samples_filepath):\n",
        "        os.makedirs(samples_filepath)\n",
        "    if not os.path.exists(saved_models_filepath):\n",
        "        os.makedirs(saved_models_filepath)\n",
        "\n",
        "    outputs = (saved_models_filepath, logs_filepath, samples_filepath)\n",
        "    outputs = (os.path.abspath(item) for item in outputs)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3BMXNVZ6b9qk"
      },
      "outputs": [],
      "source": [
        "def save_statistics(experiment_name, line_to_add, filename=\"summary_statistics.csv\", create=False):\n",
        "    summary_filename = \"{}/{}\".format(experiment_name, filename)\n",
        "    if create:\n",
        "        with open(summary_filename, 'w') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(line_to_add)\n",
        "    else:\n",
        "        with open(summary_filename, 'a') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(line_to_add)\n",
        "\n",
        "    return summary_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qIFIaiMXcIOc"
      },
      "outputs": [],
      "source": [
        "def save_to_json(filename, dict_to_store):\n",
        "    with open(os.path.abspath(filename), 'w') as f:\n",
        "        json.dump(dict_to_store, fp=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nV0wkiZBk59X"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "# from utils.storage import build_experiment_folder, save_statistics, save_to_json\n",
        "import time\n",
        "import torch\n",
        "\n",
        "\n",
        "class ExperimentBuilder(object):\n",
        "    def __init__(self, args, data, model, device):\n",
        "        \"\"\"\n",
        "        Initializes an experiment builder using a named tuple (args), a data provider (data), a meta learning system\n",
        "        (model) and a device (e.g. gpu/cpu/n)\n",
        "        :param args: A namedtuple containing all experiment hyperparameters\n",
        "        :param data: A data provider of instance MetaLearningSystemDataLoader\n",
        "        :param model: A meta learning system instance\n",
        "        :param device: Device/s to use for the experiment\n",
        "        \"\"\"\n",
        "        self.args, self.device = args, device\n",
        "\n",
        "        self.model = model\n",
        "        self.saved_models_filepath, self.logs_filepath, self.samples_filepath = build_experiment_folder(\n",
        "            experiment_name=self.args.experiment_name)\n",
        "\n",
        "        self.total_losses = {}\n",
        "        self.state = {'best_val_acc': 0.0, 'best_val_iter': 0, 'current_iter': 0}\n",
        "        self.start_epoch = 0\n",
        "        self.max_models_to_save = self.args.max_models_to_save\n",
        "        self.create_summary_csv = False\n",
        "\n",
        "        if self.args.continue_from_epoch == 'from_scratch':\n",
        "            self.create_summary_csv = True\n",
        "\n",
        "        elif self.args.continue_from_epoch == 'latest':\n",
        "            checkpoint = os.path.join(self.saved_models_filepath, \"train_model_latest\")\n",
        "            print(\"attempting to find existing checkpoint\", )\n",
        "            if os.path.exists(checkpoint):\n",
        "                self.state = \\\n",
        "                    self.model.load_model(model_save_dir=self.saved_models_filepath, model_name=\"train_model\",\n",
        "                                          model_idx='latest')\n",
        "                self.start_epoch = int(self.state['current_iter'] / self.args.total_iter_per_epoch)\n",
        "\n",
        "            else:\n",
        "                self.args.continue_from_epoch = 'from_scratch'\n",
        "                self.create_summary_csv = True\n",
        "        elif int(self.args.continue_from_epoch) >= 0:\n",
        "            self.state = \\\n",
        "                self.model.load_model(model_save_dir=self.saved_models_filepath, model_name=\"train_model\",\n",
        "                                      model_idx=self.args.continue_from_epoch)\n",
        "            self.start_epoch = int(self.state['current_iter'] / self.args.total_iter_per_epoch)\n",
        "\n",
        "        self.data = data(args=args, current_iter=self.state['current_iter'])\n",
        "\n",
        "        print(\"train_seed {}, val_seed: {}, at start time\".format(self.data.dataset.seed[\"train\"],\n",
        "                                                                  self.data.dataset.seed[\"val\"]))\n",
        "        self.total_epochs_before_pause = self.args.total_epochs_before_pause\n",
        "        self.state['best_epoch'] = int(self.state['best_val_iter'] / self.args.total_iter_per_epoch)\n",
        "        self.epoch = int(self.state['current_iter'] / self.args.total_iter_per_epoch)\n",
        "        self.augment_flag = 'omniglot' in self.args.dataset_name.lower()\n",
        "        self.start_time = time.time()\n",
        "        self.epochs_done_in_this_run = 0\n",
        "        print(self.state['current_iter'], int(self.args.total_iter_per_epoch * self.args.total_epochs))\n",
        "\n",
        "    def build_summary_dict(self, total_losses, phase, summary_losses=None):\n",
        "        \"\"\"\n",
        "        Builds/Updates a summary dict directly from the metric dict of the current iteration.\n",
        "        :param total_losses: Current dict with total losses (not aggregations) from experiment\n",
        "        :param phase: Current training phase\n",
        "        :param summary_losses: Current summarised (aggregated/summarised) losses stats means, stdv etc.\n",
        "        :return: A new summary dict with the updated summary statistics information.\n",
        "        \"\"\"\n",
        "        if summary_losses is None:\n",
        "            summary_losses = {}\n",
        "\n",
        "        for key in total_losses:\n",
        "            summary_losses[\"{}_{}_mean\".format(phase, key)] = np.mean(total_losses[key])\n",
        "            summary_losses[\"{}_{}_std\".format(phase, key)] = np.std(total_losses[key])\n",
        "\n",
        "        return summary_losses\n",
        "\n",
        "    def build_loss_summary_string(self, summary_losses):\n",
        "        \"\"\"\n",
        "        Builds a progress bar summary string given current summary losses dictionary\n",
        "        :param summary_losses: Current summary statistics\n",
        "        :return: A summary string ready to be shown to humans.\n",
        "        \"\"\"\n",
        "        output_update = \"\"\n",
        "        for key, value in zip(list(summary_losses.keys()), list(summary_losses.values())):\n",
        "            if \"loss\" in key or \"accuracy\" in key:\n",
        "                value = float(value)\n",
        "                output_update += \"{}: {:.4f}, \".format(key, value)\n",
        "\n",
        "        return output_update\n",
        "\n",
        "    def merge_two_dicts(self, first_dict, second_dict):\n",
        "        \"\"\"Given two dicts, merge them into a new dict as a shallow copy.\"\"\"\n",
        "        z = first_dict.copy()\n",
        "        z.update(second_dict)\n",
        "        return z\n",
        "\n",
        "    def train_iteration(self, train_sample, sample_idx, epoch_idx, total_losses, current_iter, pbar_train):\n",
        "        \"\"\"\n",
        "        Runs a training iteration, updates the progress bar and returns the total and current epoch train losses.\n",
        "        :param train_sample: A sample from the data provider\n",
        "        :param sample_idx: The index of the incoming sample, in relation to the current training run.\n",
        "        :param epoch_idx: The epoch index.\n",
        "        :param total_losses: The current total losses dictionary to be updated.\n",
        "        :param current_iter: The current training iteration in relation to the whole experiment.\n",
        "        :param pbar_train: The progress bar of the training.\n",
        "        :return: Updates total_losses, train_losses, current_iter\n",
        "        \"\"\"\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
        "        data_batch = (x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        if sample_idx == 0:\n",
        "            print(\"shape of data\", x_support_set.shape, x_target_set.shape, y_support_set.shape,\n",
        "                  y_target_set.shape)\n",
        "\n",
        "        losses, _ = self.model.run_train_iter(data_batch=data_batch, epoch=epoch_idx)\n",
        "\n",
        "        for key, value in zip(list(losses.keys()), list(losses.values())):\n",
        "            if key not in total_losses:\n",
        "                total_losses[key] = [float(value)]\n",
        "            else:\n",
        "                total_losses[key].append(float(value))\n",
        "\n",
        "        train_losses = self.build_summary_dict(total_losses=total_losses, phase=\"train\")\n",
        "        train_output_update = self.build_loss_summary_string(losses)\n",
        "\n",
        "        pbar_train.update(1)\n",
        "        pbar_train.set_description(\"training phase {} -> {}\".format(self.epoch, train_output_update))\n",
        "\n",
        "        current_iter += 1\n",
        "\n",
        "        return train_losses, total_losses, current_iter\n",
        "\n",
        "    def evaluation_iteration(self, val_sample, total_losses, pbar_val, phase):\n",
        "        \"\"\"\n",
        "        Runs a validation iteration, updates the progress bar and returns the total and current epoch val losses.\n",
        "        :param val_sample: A sample from the data provider\n",
        "        :param total_losses: The current total losses dictionary to be updated.\n",
        "        :param pbar_val: The progress bar of the val stage.\n",
        "        :return: The updated val_losses, total_losses\n",
        "        \"\"\"\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set, seed = val_sample\n",
        "        data_batch = (\n",
        "            x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        losses, _ = self.model.run_validation_iter(data_batch=data_batch)\n",
        "        for key, value in zip(list(losses.keys()), list(losses.values())):\n",
        "            if key not in total_losses:\n",
        "                total_losses[key] = [float(value)]\n",
        "            else:\n",
        "                total_losses[key].append(float(value))\n",
        "\n",
        "        val_losses = self.build_summary_dict(total_losses=total_losses, phase=phase)\n",
        "        val_output_update = self.build_loss_summary_string(losses)\n",
        "\n",
        "        pbar_val.update(1)\n",
        "        pbar_val.set_description(\n",
        "            \"val_phase {} -> {}\".format(self.epoch, val_output_update))\n",
        "\n",
        "        return val_losses, total_losses\n",
        "\n",
        "    def test_evaluation_iteration(self, val_sample, model_idx, sample_idx, per_model_per_batch_preds, pbar_test):\n",
        "        \"\"\"\n",
        "        Runs a validation iteration, updates the progress bar and returns the total and current epoch val losses.\n",
        "        :param val_sample: A sample from the data provider\n",
        "        :param total_losses: The current total losses dictionary to be updated.\n",
        "        :param pbar_test: The progress bar of the val stage.\n",
        "        :return: The updated val_losses, total_losses\n",
        "        \"\"\"\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set, seed = val_sample\n",
        "        data_batch = (\n",
        "            x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        losses, per_task_preds = self.model.run_validation_iter(data_batch=data_batch)\n",
        "\n",
        "        per_model_per_batch_preds[model_idx].extend(list(per_task_preds))\n",
        "\n",
        "        test_output_update = self.build_loss_summary_string(losses)\n",
        "\n",
        "        pbar_test.update(1)\n",
        "        pbar_test.set_description(\n",
        "            \"test_phase {} -> {}\".format(self.epoch, test_output_update))\n",
        "\n",
        "        return per_model_per_batch_preds\n",
        "\n",
        "    def save_models(self, model, epoch, state):\n",
        "        \"\"\"\n",
        "        Saves two separate instances of the current model. One to be kept for history and reloading later and another\n",
        "        one marked as \"latest\" to be used by the system for the next epoch training. Useful when the training/val\n",
        "        process is interrupted or stopped. Leads to fault tolerant training and validation systems that can continue\n",
        "        from where they left off before.\n",
        "        :param model: Current meta learning model of any instance within the few_shot_learning_system.py\n",
        "        :param epoch: Current epoch\n",
        "        :param state: Current model and experiment state dict.\n",
        "        \"\"\"\n",
        "        model.save_model(model_save_dir=os.path.join(self.saved_models_filepath, \"train_model_{}\".format(int(epoch))),\n",
        "                         state=state)\n",
        "\n",
        "        model.save_model(model_save_dir=os.path.join(self.saved_models_filepath, \"train_model_latest\"),\n",
        "                         state=state)\n",
        "\n",
        "        print(\"saved models to\", self.saved_models_filepath)\n",
        "\n",
        "    def pack_and_save_metrics(self, start_time, create_summary_csv, train_losses, val_losses, state):\n",
        "        \"\"\"\n",
        "        Given current epochs start_time, train losses, val losses and whether to create a new stats csv file, pack stats\n",
        "        and save into a statistics csv file. Return a new start time for the new epoch.\n",
        "        :param start_time: The start time of the current epoch\n",
        "        :param create_summary_csv: A boolean variable indicating whether to create a new statistics file or\n",
        "        append results to existing one\n",
        "        :param train_losses: A dictionary with the current train losses\n",
        "        :param val_losses: A dictionary with the currrent val loss\n",
        "        :return: The current time, to be used for the next epoch.\n",
        "        \"\"\"\n",
        "        epoch_summary_losses = self.merge_two_dicts(first_dict=train_losses, second_dict=val_losses)\n",
        "\n",
        "        if 'per_epoch_statistics' not in state:\n",
        "            state['per_epoch_statistics'] = {}\n",
        "\n",
        "        for key, value in epoch_summary_losses.items():\n",
        "\n",
        "            if key not in state['per_epoch_statistics']:\n",
        "                state['per_epoch_statistics'][key] = [value]\n",
        "            else:\n",
        "                state['per_epoch_statistics'][key].append(value)\n",
        "\n",
        "        epoch_summary_string = self.build_loss_summary_string(epoch_summary_losses)\n",
        "        epoch_summary_losses[\"epoch\"] = self.epoch\n",
        "        epoch_summary_losses['epoch_run_time'] = time.time() - start_time\n",
        "\n",
        "        if create_summary_csv:\n",
        "            self.summary_statistics_filepath = save_statistics(self.logs_filepath, list(epoch_summary_losses.keys()),\n",
        "                                                               create=True)\n",
        "            self.create_summary_csv = False\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"epoch {} -> {}\".format(epoch_summary_losses[\"epoch\"], epoch_summary_string))\n",
        "\n",
        "        self.summary_statistics_filepath = save_statistics(self.logs_filepath,\n",
        "                                                           list(epoch_summary_losses.values()))\n",
        "        return start_time, state\n",
        "\n",
        "    def evaluated_test_set_using_the_best_models(self, top_n_models):\n",
        "        per_epoch_statistics = self.state['per_epoch_statistics']\n",
        "        val_acc = np.copy(per_epoch_statistics['val_accuracy_mean'])\n",
        "        val_idx = np.array([i for i in range(len(val_acc))])\n",
        "        sorted_idx = np.argsort(val_acc, axis=0).astype(dtype=np.int32)[::-1][:top_n_models]\n",
        "\n",
        "        sorted_val_acc = val_acc[sorted_idx]\n",
        "        val_idx = val_idx[sorted_idx]\n",
        "        print(sorted_idx)\n",
        "        print(sorted_val_acc)\n",
        "\n",
        "        top_n_idx = val_idx[:top_n_models]\n",
        "        per_model_per_batch_preds = [[] for i in range(top_n_models)]\n",
        "        per_model_per_batch_targets = [[] for i in range(top_n_models)]\n",
        "        test_losses = [dict() for i in range(top_n_models)]\n",
        "        for idx, model_idx in enumerate(top_n_idx):\n",
        "            self.state = \\\n",
        "                self.model.load_model(model_save_dir=self.saved_models_filepath, model_name=\"train_model\",\n",
        "                                      model_idx=model_idx + 1)\n",
        "            with tqdm.tqdm(total=int(self.args.num_evaluation_tasks / self.args.batch_size)) as pbar_test:\n",
        "                for sample_idx, test_sample in enumerate(\n",
        "                        self.data.get_test_batches(total_batches=int(self.args.num_evaluation_tasks / self.args.batch_size),\n",
        "                                                   augment_images=False)):\n",
        "                    #print(test_sample[4])\n",
        "                    per_model_per_batch_targets[idx].extend(np.array(test_sample[3]))\n",
        "                    per_model_per_batch_preds = self.test_evaluation_iteration(val_sample=test_sample,\n",
        "                                                                               sample_idx=sample_idx,\n",
        "                                                                               model_idx=idx,\n",
        "                                                                               per_model_per_batch_preds=per_model_per_batch_preds,\n",
        "                                                                               pbar_test=pbar_test)\n",
        "        # for i in range(top_n_models):\n",
        "        #     print(\"test assertion\", 0)\n",
        "        #     print(per_model_per_batch_targets[0], per_model_per_batch_targets[i])\n",
        "        #     assert np.equal(np.array(per_model_per_batch_targets[0]), np.array(per_model_per_batch_targets[i]))\n",
        "\n",
        "        per_batch_preds = np.mean(per_model_per_batch_preds, axis=0)\n",
        "        #print(per_batch_preds.shape)\n",
        "        per_batch_max = np.argmax(per_batch_preds, axis=2)\n",
        "        per_batch_targets = np.array(per_model_per_batch_targets[0]).reshape(per_batch_max.shape)\n",
        "        #print(per_batch_max)\n",
        "        accuracy = np.mean(np.equal(per_batch_targets, per_batch_max))\n",
        "        accuracy_std = np.std(np.equal(per_batch_targets, per_batch_max))\n",
        "\n",
        "        test_losses = {\"test_accuracy_mean\": accuracy, \"test_accuracy_std\": accuracy_std}\n",
        "\n",
        "        _ = save_statistics(self.logs_filepath,\n",
        "                            list(test_losses.keys()),\n",
        "                            create=True, filename=\"test_summary.csv\")\n",
        "\n",
        "        summary_statistics_filepath = save_statistics(self.logs_filepath,\n",
        "                                                      list(test_losses.values()),\n",
        "                                                      create=False, filename=\"test_summary.csv\")\n",
        "        print(test_losses)\n",
        "        print(\"saved test performance at\", summary_statistics_filepath)\n",
        "\n",
        "    def run_experiment(self):\n",
        "        \"\"\"\n",
        "        Runs a full training experiment with evaluations of the model on the val set at every epoch. Furthermore,\n",
        "        will return the test set evaluation results on the best performing validation model.\n",
        "        \"\"\"\n",
        "        with tqdm.tqdm(initial=self.state['current_iter'],\n",
        "                           total=int(self.args.total_iter_per_epoch * self.args.total_epochs)) as pbar_train:\n",
        "\n",
        "            while (self.state['current_iter'] < (self.args.total_epochs * self.args.total_iter_per_epoch)) and (self.args.evaluate_on_test_set_only == False):\n",
        "\n",
        "                for train_sample_idx, train_sample in enumerate(\n",
        "                        self.data.get_train_batches(total_batches=int(self.args.total_iter_per_epoch *\n",
        "                                                                      self.args.total_epochs) - self.state[\n",
        "                                                                      'current_iter'],\n",
        "                                                    augment_images=self.augment_flag)):\n",
        "                    # print(self.state['current_iter'], (self.args.total_epochs * self.args.total_iter_per_epoch))\n",
        "                    train_losses, total_losses, self.state['current_iter'] = self.train_iteration(\n",
        "                        train_sample=train_sample,\n",
        "                        total_losses=self.total_losses,\n",
        "                        epoch_idx=(self.state['current_iter'] /\n",
        "                                   self.args.total_iter_per_epoch),\n",
        "                        pbar_train=pbar_train,\n",
        "                        current_iter=self.state['current_iter'],\n",
        "                        sample_idx=self.state['current_iter'])\n",
        "\n",
        "                    if self.state['current_iter'] % self.args.total_iter_per_epoch == 0:\n",
        "\n",
        "                        total_losses = {}\n",
        "                        val_losses = {}\n",
        "                        with tqdm.tqdm(total=int(self.args.num_evaluation_tasks / self.args.batch_size)) as pbar_val:\n",
        "                            for _, val_sample in enumerate(\n",
        "                                    self.data.get_val_batches(total_batches=int(self.args.num_evaluation_tasks / self.args.batch_size),\n",
        "                                                              augment_images=False)):\n",
        "                                val_losses, total_losses = self.evaluation_iteration(val_sample=val_sample,\n",
        "                                                                                     total_losses=total_losses,\n",
        "                                                                                     pbar_val=pbar_val, phase='val')\n",
        "\n",
        "                            if val_losses[\"val_accuracy_mean\"] > self.state['best_val_acc']:\n",
        "                                print(\"Best validation accuracy\", val_losses[\"val_accuracy_mean\"])\n",
        "                                self.state['best_val_acc'] = val_losses[\"val_accuracy_mean\"]\n",
        "                                self.state['best_val_iter'] = self.state['current_iter']\n",
        "                                self.state['best_epoch'] = int(\n",
        "                                    self.state['best_val_iter'] / self.args.total_iter_per_epoch)\n",
        "\n",
        "\n",
        "                        self.epoch += 1\n",
        "                        self.state = self.merge_two_dicts(first_dict=self.merge_two_dicts(first_dict=self.state,\n",
        "                                                                                          second_dict=train_losses),\n",
        "                                                          second_dict=val_losses)\n",
        "\n",
        "                        self.save_models(model=self.model, epoch=self.epoch, state=self.state)\n",
        "\n",
        "                        self.start_time, self.state = self.pack_and_save_metrics(start_time=self.start_time,\n",
        "                                                                                 create_summary_csv=self.create_summary_csv,\n",
        "                                                                                 train_losses=train_losses,\n",
        "                                                                                 val_losses=val_losses,\n",
        "                                                                                 state=self.state)\n",
        "\n",
        "                        self.total_losses = {}\n",
        "\n",
        "                        self.epochs_done_in_this_run += 1\n",
        "\n",
        "                        save_to_json(filename=os.path.join(self.logs_filepath, \"summary_statistics.json\"),\n",
        "                                     dict_to_store=self.state['per_epoch_statistics'])\n",
        "\n",
        "                        if self.epochs_done_in_this_run >= self.total_epochs_before_pause:\n",
        "                            print(\"train_seed {}, val_seed: {}, at pause time\".format(self.data.dataset.seed[\"train\"],\n",
        "                                                                                      self.data.dataset.seed[\"val\"]))\n",
        "                            sys.exit()\n",
        "            self.evaluated_test_set_using_the_best_models(top_n_models=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcgAQA_LvTZc"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6C_DqsYZvVpv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm\n",
        "import concurrent.futures\n",
        "import pickle\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# from utils.parser_utils import get_args\n",
        "\n",
        "\n",
        "class rotate_image(object):\n",
        "\n",
        "    def __init__(self, k, channels):\n",
        "        self.k = k\n",
        "        self.channels = channels\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if self.channels == 1:\n",
        "            if len(image.shape) == 3:\n",
        "                image = image[:, :, 0]\n",
        "                image = np.expand_dims(image, axis=2)\n",
        "\n",
        "            elif len(image.shape) == 4:\n",
        "                image = image[:, :, :, 0]\n",
        "                image = np.expand_dims(image, axis=3)\n",
        "\n",
        "        image = np.rot90(image, k=self.k).copy()\n",
        "        return image\n",
        "\n",
        "\n",
        "class torch_rotate_image(object):\n",
        "\n",
        "    def __init__(self, k, channels):\n",
        "        self.k = k\n",
        "        self.channels = channels\n",
        "\n",
        "    def __call__(self, image):\n",
        "        rotate = transforms.RandomRotation(degrees=self.k * 90)\n",
        "        if image.shape[-1] == 1:\n",
        "            image = image[:, :, 0]\n",
        "        image = Image.fromarray(image)\n",
        "        image = rotate(image)\n",
        "        image = np.array(image)\n",
        "        if len(image.shape) == 2:\n",
        "            image = np.expand_dims(image, axis=2)\n",
        "        return image\n",
        "\n",
        "\n",
        "def augment_image(image, k, channels, augment_bool, args, dataset_name):\n",
        "    transform_train, transform_evaluation = get_transforms_for_dataset(dataset_name=dataset_name,\n",
        "                                                                       args=args, k=k)\n",
        "    if len(image.shape) > 3:\n",
        "        images = [item for item in image]\n",
        "        output_images = []\n",
        "        for image in images:\n",
        "            if augment_bool is True:\n",
        "                for transform_current in transform_train:\n",
        "                    image = transform_current(image)\n",
        "            else:\n",
        "                for transform_current in transform_evaluation:\n",
        "                    image = transform_current(image)\n",
        "            output_images.append(image)\n",
        "        image = torch.stack(output_images)\n",
        "    elif augment_bool is True:\n",
        "        # meanstd transformation\n",
        "        for transform_current in transform_train:\n",
        "            image = transform_current(image)\n",
        "    else:\n",
        "        for transform_current in transform_evaluation:\n",
        "            image = transform_current(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def get_transforms_for_dataset(dataset_name, args, k):\n",
        "    if \"cifar10\" in dataset_name or \"cifar100\" in dataset_name:\n",
        "        transform_train = [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(args.classification_mean, args.classification_std)]\n",
        "\n",
        "        transform_evaluate = [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(args.classification_mean, args.classification_std)]\n",
        "\n",
        "    elif 'omniglot' in dataset_name:\n",
        "\n",
        "        transform_train = [rotate_image(k=k, channels=args.image_channels), transforms.ToTensor()]\n",
        "        transform_evaluate = [transforms.ToTensor()]\n",
        "\n",
        "\n",
        "    elif 'imagenet' in dataset_name:\n",
        "\n",
        "        transform_train = [transforms.Compose([\n",
        "\n",
        "            transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])]\n",
        "\n",
        "        transform_evaluate = [transforms.Compose([\n",
        "\n",
        "            transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])]\n",
        "\n",
        "    return transform_train, transform_evaluate\n",
        "\n",
        "\n",
        "class FewShotLearningDatasetParallel(Dataset):\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        A data provider class inheriting from Pytorch's Dataset class. It takes care of creating task sets for\n",
        "        our few-shot learning model training and evaluation\n",
        "        :param args: Arguments in the form of a Bunch object. Includes all hyperparameters necessary for the\n",
        "        data-provider. For transparency and readability reasons to explicitly set as self.object_name all arguments\n",
        "        required for the data provider, such that the reader knows exactly what is necessary for the data provider/\n",
        "        \"\"\"\n",
        "        self.data_path = args.dataset_path\n",
        "        self.dataset_name = args.dataset_name\n",
        "        self.data_loaded_in_memory = False\n",
        "        self.image_height, self.image_width, self.image_channel = args.image_height, args.image_width, args.image_channels\n",
        "        self.args = args\n",
        "        self.indexes_of_folders_indicating_class = args.indexes_of_folders_indicating_class\n",
        "        self.reverse_channels = args.reverse_channels\n",
        "        self.labels_as_int = args.labels_as_int\n",
        "        self.train_val_test_split = args.train_val_test_split\n",
        "        self.current_set_name = \"train\"\n",
        "        self.num_target_samples = args.num_target_samples\n",
        "        self.reset_stored_filepaths = args.reset_stored_filepaths\n",
        "        val_rng = np.random.RandomState(seed=args.val_seed)\n",
        "        val_seed = val_rng.randint(1, 999999)\n",
        "        train_rng = np.random.RandomState(seed=args.train_seed)\n",
        "        train_seed = train_rng.randint(1, 999999)\n",
        "        test_rng = np.random.RandomState(seed=args.val_seed)\n",
        "        test_seed = test_rng.randint(1, 999999)\n",
        "        args.val_seed = val_seed\n",
        "        args.train_seed = train_seed\n",
        "        args.test_seed = test_seed\n",
        "        self.init_seed = {\"train\": args.train_seed, \"val\": args.val_seed, 'test': args.val_seed}\n",
        "        self.seed = {\"train\": args.train_seed, \"val\": args.val_seed, 'test': args.val_seed}\n",
        "        self.num_of_gpus = args.num_of_gpus\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.train_index = 0\n",
        "        self.val_index = 0\n",
        "        self.test_index = 0\n",
        "\n",
        "        self.augment_images = False\n",
        "        self.num_samples_per_class = args.num_samples_per_class\n",
        "        self.num_classes_per_set = args.num_classes_per_set\n",
        "\n",
        "        self.rng = np.random.RandomState(seed=self.seed['val'])\n",
        "        self.datasets = self.load_dataset()\n",
        "\n",
        "        self.indexes = {\"train\": 0, \"val\": 0, 'test': 0}\n",
        "        self.dataset_size_dict = {\n",
        "            \"train\": {key: len(self.datasets['train'][key]) for key in list(self.datasets['train'].keys())},\n",
        "            \"val\": {key: len(self.datasets['val'][key]) for key in list(self.datasets['val'].keys())},\n",
        "            'test': {key: len(self.datasets['test'][key]) for key in list(self.datasets['test'].keys())}}\n",
        "        self.label_set = self.get_label_set()\n",
        "        self.data_length = {name: np.sum([len(self.datasets[name][key])\n",
        "                                          for key in self.datasets[name]]) for name in self.datasets.keys()}\n",
        "\n",
        "        print(\"data\", self.data_length)\n",
        "        self.observed_seed_set = None\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"\n",
        "        Loads a dataset's dictionary files and splits the data according to the train_val_test_split variable stored\n",
        "        in the args object.\n",
        "        :return: Three sets, the training set, validation set and test sets (referred to as the meta-train,\n",
        "        meta-val and meta-test in the paper)\n",
        "        \"\"\"\n",
        "        rng = np.random.RandomState(seed=self.seed['val'])\n",
        "\n",
        "        if self.args.sets_are_pre_split == True:\n",
        "            print(\"Loading pre-split data\")\n",
        "            data_image_paths, index_to_label_name_dict_file, label_to_index = self.load_datapaths()\n",
        "            dataset_splits = {}\n",
        "            for key, value in data_image_paths.items():\n",
        "                key = self.get_label_from_index(index=key)\n",
        "                bits = key.split(\"/\")\n",
        "                set_name = bits[0]\n",
        "                class_label = bits[1]\n",
        "                if set_name not in dataset_splits:\n",
        "                    dataset_splits[set_name] = {class_label: value}\n",
        "                else:\n",
        "                    dataset_splits[set_name][class_label] = value\n",
        "        else:\n",
        "            data_image_paths, index_to_label_name_dict_file, label_to_index = self.load_datapaths()\n",
        "            total_label_types = len(data_image_paths)\n",
        "            num_classes_idx = np.arange(len(data_image_paths.keys()), dtype=np.int32)\n",
        "            rng.shuffle(num_classes_idx)\n",
        "            keys = list(data_image_paths.keys())\n",
        "            values = list(data_image_paths.values())\n",
        "            new_keys = [keys[idx] for idx in num_classes_idx]\n",
        "            new_values = [values[idx] for idx in num_classes_idx]\n",
        "            data_image_paths = dict(zip(new_keys, new_values))\n",
        "            # data_image_paths = self.shuffle(data_image_paths)\n",
        "            x_train_id, x_val_id, x_test_id = int(self.train_val_test_split[0] * total_label_types), \\\n",
        "                                              int(np.sum(self.train_val_test_split[:2]) * total_label_types), \\\n",
        "                                              int(total_label_types)\n",
        "            # print(x_train_id, x_val_id, x_test_id)\n",
        "            # print(\"DATA IMAGE PATH FIRST KEY\")\n",
        "            test_first_class_key = list(data_image_paths.keys())[0]\n",
        "            # print(test_first_class_key)\n",
        "            # print(data_image_paths[test_first_class_key])\n",
        "            x_train_classes = (class_key for class_key in list(data_image_paths.keys())[:x_train_id])\n",
        "            x_val_classes = (class_key for class_key in list(data_image_paths.keys())[x_train_id:x_val_id])\n",
        "            x_test_classes = (class_key for class_key in list(data_image_paths.keys())[x_val_id:x_test_id])\n",
        "            x_train, x_val, x_test = {class_key: data_image_paths[class_key] for class_key in x_train_classes}, \\\n",
        "                                     {class_key: data_image_paths[class_key] for class_key in x_val_classes}, \\\n",
        "                                     {class_key: data_image_paths[class_key] for class_key in x_test_classes},\n",
        "            dataset_splits = {\"train\": x_train, \"val\":x_val , \"test\": x_test}\n",
        "\n",
        "        if self.args.load_into_memory is True:\n",
        "\n",
        "            print(\"Loading data into RAM\")\n",
        "            x_loaded = {\"train\": [], \"val\": [], \"test\": []}\n",
        "\n",
        "            for set_key, set_value in dataset_splits.items():\n",
        "                print(\"Currently loading into memory the {} set\".format(set_key))\n",
        "                # print(\"Set value is {}\".format(set_value))\n",
        "                x_loaded[set_key] = {key: np.zeros(len(value), ) for key, value in set_value.items()}\n",
        "                # for class_key, class_value in set_value.items():\n",
        "                with tqdm.tqdm(total=len(set_value)) as pbar_memory_load:\n",
        "                    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "                        # Process the list of files, but split the work across the process pool to use all CPUs!\n",
        "                        for (class_label, class_images_loaded) in executor.map(self.load_parallel_batch, (set_value.items())):\n",
        "                            x_loaded[set_key][class_label] = class_images_loaded\n",
        "                            pbar_memory_load.update(1)\n",
        "\n",
        "            dataset_splits = x_loaded\n",
        "            self.data_loaded_in_memory = True\n",
        "\n",
        "        return dataset_splits\n",
        "\n",
        "    def load_datapaths(self):\n",
        "        \"\"\"\n",
        "        If saved json dictionaries of the data are available, then this method loads the dictionaries such that the\n",
        "        data is ready to be read. If the json dictionaries do not exist, then this method calls get_data_paths()\n",
        "        which will build the json dictionary containing the class to filepath samples, and then store them.\n",
        "        :return: data_image_paths: dict containing class to filepath list pairs.\n",
        "                 index_to_label_name_dict_file: dict containing numerical indexes mapped to the human understandable\n",
        "                 string-names of the class\n",
        "                 label_to_index: dictionary containing human understandable string mapped to numerical indexes\n",
        "        \"\"\"\n",
        "        dataset_dir = config[\"dataset_path\"]\n",
        "        data_path_file = \"{}/{}.json\".format(dataset_dir, self.dataset_name)\n",
        "        self.index_to_label_name_dict_file = \"{}/map_to_label_name_{}.json\".format(dataset_dir, self.dataset_name)\n",
        "        # print(self.index_to_label_name_dict_file)\n",
        "        self.label_name_to_map_dict_file = \"{}/label_name_to_map_{}.json\".format(dataset_dir, self.dataset_name)\n",
        "        # print(self.label_name_to_map_dict_file)\n",
        "\n",
        "        if not os.path.exists(data_path_file):\n",
        "            self.reset_stored_filepaths = True\n",
        "\n",
        "        if self.reset_stored_filepaths == True:\n",
        "            if os.path.exists(data_path_file):\n",
        "                os.remove(data_path_file)\n",
        "            self.reset_stored_filepaths = False\n",
        "\n",
        "        try:\n",
        "            data_image_paths = self.load_from_json(filename=data_path_file)\n",
        "            #json name difference; takes in /content/datasets...\n",
        "            #changed to datasets/... which is appended to new path\n",
        "            label_to_index = self.load_from_json(filename=self.label_name_to_map_dict_file)\n",
        "            index_to_label_name_dict_file = self.load_from_json(filename=self.index_to_label_name_dict_file)\n",
        "\n",
        "\n",
        "            # print(data_image_paths)\n",
        "            # print(index_to_label_name_dict_file)\n",
        "            # print(label_to_index)\n",
        "            return data_image_paths, index_to_label_name_dict_file, label_to_index\n",
        "        except:\n",
        "            print(\"Mapped data paths can't be found, remapping paths..\")\n",
        "            data_image_paths, code_to_label_name, label_name_to_code = self.get_data_paths()\n",
        "            self.save_to_json(dict_to_store=data_image_paths, filename=data_path_file)\n",
        "            self.save_to_json(dict_to_store=code_to_label_name, filename=self.index_to_label_name_dict_file)\n",
        "            self.save_to_json(dict_to_store=label_name_to_code, filename=self.label_name_to_map_dict_file)\n",
        "            return self.load_datapaths()\n",
        "\n",
        "    def save_to_json(self, filename, dict_to_store):\n",
        "        with open(os.path.abspath(filename), 'w') as f:\n",
        "            json.dump(dict_to_store, fp=f)\n",
        "\n",
        "    def load_from_json(self, filename):\n",
        "        with open(filename, mode=\"r\") as f:\n",
        "            load_dict = json.load(fp=f)\n",
        "\n",
        "        return load_dict\n",
        "\n",
        "    def load_test_image(self, filepath):\n",
        "        \"\"\"\n",
        "        Tests whether a target filepath contains an uncorrupted image. If image is corrupted, attempt to fix.\n",
        "        :param filepath: Filepath of image to be tested\n",
        "        :return: Return filepath of image if image exists and is uncorrupted (or attempt to fix has succeeded),\n",
        "        else return None\n",
        "        \"\"\"\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(filepath)\n",
        "        except RuntimeWarning:\n",
        "            os.system(\"convert {} -strip {}\".format(filepath, filepath))\n",
        "            print(\"converting\")\n",
        "            image = Image.open(filepath)\n",
        "        except:\n",
        "            print(\"Broken image\")\n",
        "\n",
        "        if image is not None:\n",
        "            return filepath\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_data_paths(self):\n",
        "        \"\"\"\n",
        "        Method that scans the dataset directory and generates class to image-filepath list dictionaries.\n",
        "        :return: data_image_paths: dict containing class to filepath list pairs.\n",
        "                 index_to_label_name_dict_file: dict containing numerical indexes mapped to the human understandable\n",
        "                 string-names of the class\n",
        "                 label_to_index: dictionary containing human understandable string mapped to numerical indexes\n",
        "        \"\"\"\n",
        "        print(\"Get images from\", self.data_path)\n",
        "        data_image_path_list_raw = []\n",
        "        labels = set()\n",
        "        for subdir, dir, files in os.walk(self.data_path):\n",
        "            for file in files:\n",
        "                if (\".jpeg\") in file.lower() or (\".png\") in file.lower() or (\".jpg\") in file.lower():\n",
        "                    filepath = os.path.abspath(os.path.join(subdir, file))\n",
        "                    label = self.get_label_from_path(filepath)\n",
        "                    data_image_path_list_raw.append(filepath)\n",
        "                    labels.add(label)\n",
        "\n",
        "        labels = sorted(labels)\n",
        "        idx_to_label_name = {idx: label for idx, label in enumerate(labels)}\n",
        "        label_name_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
        "        data_image_path_dict = {idx: [] for idx in list(idx_to_label_name.keys())}\n",
        "        with tqdm.tqdm(total=len(data_image_path_list_raw)) as pbar_error:\n",
        "            with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "                # Process the list of files, but split the work across the process pool to use all CPUs!\n",
        "                for image_file in executor.map(self.load_test_image, (data_image_path_list_raw)):\n",
        "                    pbar_error.update(1)\n",
        "                    if image_file is not None:\n",
        "                        label = self.get_label_from_path(image_file)\n",
        "                        data_image_path_dict[label_name_to_idx[label]].append(image_file)\n",
        "\n",
        "        return data_image_path_dict, idx_to_label_name, label_name_to_idx\n",
        "\n",
        "    def get_label_set(self):\n",
        "        \"\"\"\n",
        "        Generates a set containing all class numerical indexes\n",
        "        :return: A set containing all class numerical indexes\n",
        "        \"\"\"\n",
        "        index_to_label_name_dict_file = self.load_from_json(filename=self.index_to_label_name_dict_file)\n",
        "        return set(list(index_to_label_name_dict_file.keys()))\n",
        "\n",
        "    def get_index_from_label(self, label):\n",
        "        \"\"\"\n",
        "        Given a class's (human understandable) string, returns the numerical index of that class\n",
        "        :param label: A string of a human understandable class contained in the dataset\n",
        "        :return: An int containing the numerical index of the given class-string\n",
        "        \"\"\"\n",
        "        label_to_index = self.load_from_json(filename=self.label_name_to_map_dict_file)\n",
        "        return label_to_index[label]\n",
        "\n",
        "    def get_label_from_index(self, index):\n",
        "        \"\"\"\n",
        "        Given an index return the human understandable label mapping to it.\n",
        "        :param index: A numerical index (int)\n",
        "        :return: A human understandable label (str)\n",
        "        \"\"\"\n",
        "        index_to_label_name = self.load_from_json(filename=self.index_to_label_name_dict_file)\n",
        "        return index_to_label_name[index]\n",
        "\n",
        "    def get_label_from_path(self, filepath):\n",
        "        \"\"\"\n",
        "        Given a path of an image generate the human understandable label for that image.\n",
        "        :param filepath: The image's filepath\n",
        "        :return: A human understandable label.\n",
        "        \"\"\"\n",
        "        label_bits = filepath.split(\"/\")\n",
        "        label = \"/\".join([label_bits[idx] for idx in self.indexes_of_folders_indicating_class])\n",
        "        if self.labels_as_int:\n",
        "            label = int(label)\n",
        "        return label\n",
        "\n",
        "    def load_image(self, image_path, channels):\n",
        "        \"\"\"\n",
        "        Given an image filepath and the number of channels to keep, load an image and keep the specified channels\n",
        "        :param image_path: The image's filepath\n",
        "        :param channels: The number of channels to keep\n",
        "        :return: An image array of shape (h, w, channels), whose values range between 0.0 and 1.0.\n",
        "        \"\"\"\n",
        "        if not self.data_loaded_in_memory:\n",
        "            image = Image.open(image_path)\n",
        "            if 'omniglot' in self.dataset_name:\n",
        "                image = image.resize((self.image_height, self.image_width), resample=Image.LANCZOS)\n",
        "                image = np.array(image, np.float32)\n",
        "                if channels == 1:\n",
        "                    image = np.expand_dims(image, axis=2)\n",
        "            else:\n",
        "                image = image.resize((self.image_height, self.image_width)).convert('RGB')\n",
        "                image = np.array(image, np.float32)\n",
        "                image = image / 255.0\n",
        "        else:\n",
        "            image = image_path\n",
        "\n",
        "        return image\n",
        "\n",
        "    def load_batch(self, batch_image_paths):\n",
        "        \"\"\"\n",
        "        Load a batch of images, given a list of filepaths\n",
        "        :param batch_image_paths: A list of filepaths\n",
        "        :return: A numpy array of images of shape batch, height, width, channels\n",
        "        \"\"\"\n",
        "        image_batch = []\n",
        "\n",
        "        if self.data_loaded_in_memory:\n",
        "            for image_path in batch_image_paths:\n",
        "                image_batch.append(image_path)\n",
        "            image_batch = np.array(image_batch, dtype=np.float32)\n",
        "            #print(image_batch.shape)\n",
        "        else:\n",
        "            print(\"BATCH IMAGE PATH (no content?):\")\n",
        "            print(image_path)\n",
        "            image_batch = [self.load_image(image_path=image_path, channels=self.image_channel)\n",
        "                           for image_path in batch_image_paths]\n",
        "            image_batch = np.array(image_batch, dtype=np.float32)\n",
        "            image_batch = self.preprocess_data(image_batch)\n",
        "\n",
        "        return image_batch\n",
        "\n",
        "    def load_parallel_batch(self, inputs):\n",
        "        \"\"\"\n",
        "        Load a batch of images, given a list of filepaths\n",
        "        :param batch_image_paths: A list of filepaths\n",
        "        :return: A numpy array of images of shape batch, height, width, channels\n",
        "        \"\"\"\n",
        "        class_label, batch_image_paths = inputs\n",
        "        image_batch = []\n",
        "\n",
        "        if self.data_loaded_in_memory:\n",
        "            for image_path in batch_image_paths:\n",
        "                image_batch.append(np.copy(image_path))\n",
        "            image_batch = np.array(image_batch, dtype=np.float32)\n",
        "        else:\n",
        "            #with tqdm.tqdm(total=1) as load_pbar:\n",
        "            image_batch = [self.load_image(image_path=image_path, channels=self.image_channel)\n",
        "                           for image_path in batch_image_paths]\n",
        "                #load_pbar.update(1)\n",
        "\n",
        "            image_batch = np.array(image_batch, dtype=np.float32)\n",
        "            image_batch = self.preprocess_data(image_batch)\n",
        "\n",
        "        return class_label, image_batch\n",
        "\n",
        "    def preprocess_data(self, x):\n",
        "        \"\"\"\n",
        "        Preprocesses data such that their shapes match the specified structures\n",
        "        :param x: A data batch to preprocess\n",
        "        :return: A preprocessed data batch\n",
        "        \"\"\"\n",
        "        x_shape = x.shape\n",
        "        x = np.reshape(x, (-1, x_shape[-3], x_shape[-2], x_shape[-1]))\n",
        "        if self.reverse_channels is True:\n",
        "            reverse_photos = np.ones(shape=x.shape)\n",
        "            for channel in range(x.shape[-1]):\n",
        "                reverse_photos[:, :, :, x.shape[-1] - 1 - channel] = x[:, :, :, channel]\n",
        "            x = reverse_photos\n",
        "        x = x.reshape(x_shape)\n",
        "        return x\n",
        "\n",
        "    def reconstruct_original(self, x):\n",
        "        \"\"\"\n",
        "        Applies the reverse operations that preprocess_data() applies such that the data returns to their original form\n",
        "        :param x: A batch of data to reconstruct\n",
        "        :return: A reconstructed batch of data\n",
        "        \"\"\"\n",
        "        x = x * 255.0\n",
        "        return x\n",
        "\n",
        "    def shuffle(self, x, rng):\n",
        "        \"\"\"\n",
        "        Shuffles the data batch along it's first axis\n",
        "        :param x: A data batch\n",
        "        :return: A shuffled data batch\n",
        "        \"\"\"\n",
        "        indices = np.arange(len(x))\n",
        "        rng.shuffle(indices)\n",
        "        x = x[indices]\n",
        "        return x\n",
        "\n",
        "    def get_set(self, dataset_name, seed, augment_images=False):\n",
        "        \"\"\"\n",
        "        Generates a task-set to be used for training or evaluation\n",
        "        :param set_name: The name of the set to use, e.g. \"train\", \"val\" etc.\n",
        "        :return: A task-set containing an image and label support set, and an image and label target set.\n",
        "        \"\"\"\n",
        "        #seed = seed % self.args.total_unique_tasks\n",
        "        rng = np.random.RandomState(seed)\n",
        "\n",
        "        # print(self.dataset_size_dict)\n",
        "        selected_classes = rng.choice(list(self.dataset_size_dict[dataset_name].keys()),\n",
        "                                      size=self.num_classes_per_set, replace=False)\n",
        "        rng.shuffle(selected_classes)\n",
        "        k_list = rng.randint(0, 4, size=self.num_classes_per_set)\n",
        "        k_dict = {selected_class: k_item for (selected_class, k_item) in zip(selected_classes, k_list)}\n",
        "        episode_labels = [i for i in range(self.num_classes_per_set)]\n",
        "        class_to_episode_label = {selected_class: episode_label for (selected_class, episode_label) in\n",
        "                                  zip(selected_classes, episode_labels)}\n",
        "\n",
        "        x_images = []\n",
        "        y_labels = []\n",
        "\n",
        "        for class_entry in selected_classes:\n",
        "            choose_samples_list = rng.choice(self.dataset_size_dict[dataset_name][class_entry],\n",
        "                                             size=self.num_samples_per_class + self.num_target_samples, replace=False)\n",
        "            class_image_samples = []\n",
        "            class_labels = []\n",
        "            for sample in choose_samples_list:\n",
        "                choose_samples = self.datasets[dataset_name][class_entry][sample]\n",
        "                x_class_data = self.load_batch([choose_samples])[0]\n",
        "                k = k_dict[class_entry]\n",
        "                x_class_data = augment_image(image=x_class_data, k=k,\n",
        "                                             channels=self.image_channel, augment_bool=augment_images,\n",
        "                                             dataset_name=self.dataset_name, args=self.args)\n",
        "                class_image_samples.append(x_class_data)\n",
        "                class_labels.append(int(class_to_episode_label[class_entry]))\n",
        "            class_image_samples = torch.stack(class_image_samples)\n",
        "            x_images.append(class_image_samples)\n",
        "            y_labels.append(class_labels)\n",
        "\n",
        "        x_images = torch.stack(x_images)\n",
        "        y_labels = np.array(y_labels, dtype=np.float32)\n",
        "\n",
        "        support_set_images = x_images[:, :self.num_samples_per_class]\n",
        "        support_set_labels = y_labels[:, :self.num_samples_per_class]\n",
        "        target_set_images = x_images[:, self.num_samples_per_class:]\n",
        "        target_set_labels = y_labels[:, self.num_samples_per_class:]\n",
        "\n",
        "        return support_set_images, target_set_images, support_set_labels, target_set_labels, seed\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_length[self.current_set_name]\n",
        "\n",
        "    def length(self, set_name):\n",
        "        self.switch_set(set_name=set_name)\n",
        "        return len(self)\n",
        "\n",
        "    def set_augmentation(self, augment_images):\n",
        "        self.augment_images = augment_images\n",
        "\n",
        "    def switch_set(self, set_name, current_iter=None):\n",
        "        self.current_set_name = set_name\n",
        "        if set_name == \"train\":\n",
        "            self.update_seed(dataset_name=set_name, seed=self.init_seed[set_name] + current_iter)\n",
        "\n",
        "    def update_seed(self, dataset_name, seed=100):\n",
        "        self.seed[dataset_name] = seed\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        support_set_images, target_set_image, support_set_labels, target_set_label, seed = \\\n",
        "            self.get_set(self.current_set_name, seed=self.seed[self.current_set_name] + idx,\n",
        "                         augment_images=self.augment_images)\n",
        "\n",
        "        return support_set_images, target_set_image, support_set_labels, target_set_label, seed\n",
        "\n",
        "    def reset_seed(self):\n",
        "        self.seed = self.init_seed\n",
        "\n",
        "\n",
        "class MetaLearningSystemDataLoader(object):\n",
        "    def __init__(self, args, current_iter=0):\n",
        "        \"\"\"\n",
        "        Initializes a meta learning system dataloader. The data loader uses the Pytorch DataLoader class to parallelize\n",
        "        batch sampling and preprocessing.\n",
        "        :param args: An arguments NamedTuple containing all the required arguments.\n",
        "        :param current_iter: Current iter of experiment. Is used to make sure the data loader continues where it left\n",
        "        of previously.\n",
        "        \"\"\"\n",
        "        self.num_of_gpus = args.num_of_gpus\n",
        "        self.batch_size = args.batch_size\n",
        "        self.samples_per_iter = args.samples_per_iter\n",
        "        self.num_workers = args.num_dataprovider_workers\n",
        "        self.total_train_iters_produced = 0\n",
        "        self.dataset = FewShotLearningDatasetParallel(args=args)\n",
        "        self.batches_per_iter = args.samples_per_iter\n",
        "        self.full_data_length = self.dataset.data_length\n",
        "        self.continue_from_iter(current_iter=current_iter)\n",
        "        self.args = args\n",
        "\n",
        "    def get_dataloader(self):\n",
        "        \"\"\"\n",
        "        Returns a data loader with the correct set (train, val or test), continuing from the current iter.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return DataLoader(self.dataset, batch_size=(self.num_of_gpus * self.batch_size * self.samples_per_iter),\n",
        "                          shuffle=False, num_workers=self.num_workers, drop_last=True)\n",
        "\n",
        "    def continue_from_iter(self, current_iter):\n",
        "        \"\"\"\n",
        "        Makes sure the data provider is aware of where we are in terms of training iterations in the experiment.\n",
        "        :param current_iter:\n",
        "        \"\"\"\n",
        "        self.total_train_iters_produced += (current_iter * (self.num_of_gpus * self.batch_size * self.samples_per_iter))\n",
        "\n",
        "    def get_train_batches(self, total_batches=-1, augment_images=False):\n",
        "        \"\"\"\n",
        "        Returns a training batches data_loader\n",
        "        :param total_batches: The number of batches we want the data loader to sample\n",
        "        :param augment_images: Whether we want the images to be augmented.\n",
        "        \"\"\"\n",
        "        if total_batches == -1:\n",
        "            self.dataset.data_length = self.full_data_length\n",
        "        else:\n",
        "            self.dataset.data_length[\"train\"] = total_batches * self.dataset.batch_size\n",
        "        self.dataset.switch_set(set_name=\"train\", current_iter=self.total_train_iters_produced)\n",
        "        self.dataset.set_augmentation(augment_images=augment_images)\n",
        "        self.total_train_iters_produced += (self.num_of_gpus * self.batch_size * self.samples_per_iter)\n",
        "        for sample_id, sample_batched in enumerate(self.get_dataloader()):\n",
        "            yield sample_batched\n",
        "\n",
        "\n",
        "    def get_val_batches(self, total_batches=-1, augment_images=False):\n",
        "        \"\"\"\n",
        "        Returns a validation batches data_loader\n",
        "        :param total_batches: The number of batches we want the data loader to sample\n",
        "        :param augment_images: Whether we want the images to be augmented.\n",
        "        \"\"\"\n",
        "        if total_batches == -1:\n",
        "            self.dataset.data_length = self.full_data_length\n",
        "        else:\n",
        "            self.dataset.data_length['val'] = total_batches * self.dataset.batch_size\n",
        "        self.dataset.switch_set(set_name=\"val\")\n",
        "        self.dataset.set_augmentation(augment_images=augment_images)\n",
        "        for sample_id, sample_batched in enumerate(self.get_dataloader()):\n",
        "            yield sample_batched\n",
        "\n",
        "\n",
        "    def get_test_batches(self, total_batches=-1, augment_images=False):\n",
        "        \"\"\"\n",
        "        Returns a testing batches data_loader\n",
        "        :param total_batches: The number of batches we want the data loader to sample\n",
        "        :param augment_images: Whether we want the images to be augmented.\n",
        "        \"\"\"\n",
        "        if total_batches == -1:\n",
        "            self.dataset.data_length = self.full_data_length\n",
        "        else:\n",
        "            self.dataset.data_length['test'] = total_batches * self.dataset.batch_size\n",
        "        self.dataset.switch_set(set_name='test')\n",
        "        self.dataset.set_augmentation(augment_images=augment_images)\n",
        "        for sample_id, sample_batched in enumerate(self.get_dataloader()):\n",
        "            yield sample_batched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27S6f3VZp0jo"
      },
      "source": [
        "# Train MAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wXc8OJUvwZp2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "  \"batch_size\":8,\n",
        "  \"image_height\":28,\n",
        "  \"image_width\":28,\n",
        "  \"image_channels\":1,\n",
        "  \"gpu_to_use\":0,\n",
        "  \"num_dataprovider_workers\":4,\n",
        "  \"max_models_to_save\":5,\n",
        "  \"dataset_name\":\"mini_imagenet\",\n",
        "  \"dataset_path\":\"/content/HowToTrainYourMAMLPytorch/datasets\",\n",
        "  \"reset_stored_paths\":False,\n",
        "  \"experiment_name\":\"mini-imagenet_5_2_0.01_48_5_2\",\n",
        "  \"train_seed\": 2, \"val_seed\": 0,\n",
        "  \"train_val_test_split\": [0.70918052988, 0.03080714725, 0.2606284658],\n",
        "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
        "  \"load_from_npz_files\": False,\n",
        "  \"sets_are_pre_split\": False,\n",
        "  \"load_into_memory\": True,\n",
        "  \"init_inner_loop_learning_rate\": 0.1,\n",
        "  \"train_in_stages\": False,\n",
        "  \"multi_step_loss_num_epochs\": 10,\n",
        "  \"minimum_per_task_contribution\": 0.01,\n",
        "  \"num_evaluation_tasks\":600,\n",
        "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
        "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
        "\n",
        "  \"total_epochs\": 150,\n",
        "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
        "  \"evaluate_on_test_set_only\": False,\n",
        "  \"max_pooling\": True,\n",
        "  \"per_step_bn_statistics\": True,\n",
        "  \"learnable_batch_norm_momentum\": False,\n",
        "  \"evalute_on_test_set_only\": False,\n",
        "  \"learnable_bn_gamma\": True,\n",
        "  \"learnable_bn_beta\": True,\n",
        "\n",
        "  \"weight_decay\": 0.0,\n",
        "  \"dropout_rate_value\":0.0,\n",
        "  \"min_learning_rate\":0.00001,\n",
        "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 150,\n",
        "  \"first_order_to_second_order_epoch\":-1,\n",
        "\n",
        "  \"norm_layer\":\"batch_norm\",\n",
        "  \"cnn_num_filters\":64,\n",
        "  \"num_stages\":4,\n",
        "  \"conv_padding\": True,\n",
        "  \"number_of_training_steps_per_iter\":5,\n",
        "  \"number_of_evaluation_steps_per_iter\":5,\n",
        "  \"cnn_blocks_per_stage\":1,\n",
        "  \"num_classes_per_set\":5,\n",
        "  \"num_samples_per_class\":5,\n",
        "  \"num_target_samples\": 1,\n",
        "\n",
        "  \"second_order\": True,\n",
        "  \"use_multi_step_loss_optimization\":True,\n",
        "\n",
        "\n",
        "  # \"seed\": 2,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "with open(\"omniglot_maml++-omniglot_5_8_0.1_64_20_2.json\", \"w\") as outfile:\n",
        "    json.dump(config, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "D0LK35JzzHRM"
      },
      "outputs": [],
      "source": [
        "# from torch import cuda\n",
        "\n",
        "\n",
        "# def get_args():\n",
        "#     import argparse\n",
        "#     import os\n",
        "#     import torch\n",
        "#     import json\n",
        "#     parser = argparse.ArgumentParser(description='Welcome to the MAML++ training and inference system')\n",
        "\n",
        "#     parser.add_argument('--batch_size', nargs=\"?\", type=int, default=32, help='Batch_size for experiment')\n",
        "#     parser.add_argument('--image_height', nargs=\"?\", type=int, default=28)\n",
        "#     parser.add_argument('--image_width', nargs=\"?\", type=int, default=28)\n",
        "#     parser.add_argument('--image_channels', nargs=\"?\", type=int, default=1)\n",
        "#     parser.add_argument('--reset_stored_filepaths', type=str, default=\"False\")\n",
        "#     parser.add_argument('--reverse_channels', type=str, default=\"False\")\n",
        "#     parser.add_argument('--num_of_gpus', type=int, default=1)\n",
        "#     parser.add_argument('--indexes_of_folders_indicating_class', nargs='+', default=[-2, -3])\n",
        "#     parser.add_argument('--train_val_test_split', nargs='+', default=[0.73982737361, 0.26, 0.13008631319])\n",
        "#     parser.add_argument('--samples_per_iter', nargs=\"?\", type=int, default=1)\n",
        "#     parser.add_argument('--labels_as_int', type=str, default=\"False\")\n",
        "#     parser.add_argument('--seed', type=int, default=104)\n",
        "\n",
        "#     parser.add_argument('--gpu_to_use', type=int)\n",
        "#     parser.add_argument('--num_dataprovider_workers', nargs=\"?\", type=int, default=4)\n",
        "#     parser.add_argument('--max_models_to_save', nargs=\"?\", type=int, default=5)\n",
        "#     parser.add_argument('--dataset_name', type=str, default=\"omniglot_dataset\")\n",
        "#     parser.add_argument('--dataset_path', type=str, default=\"datasets/omniglot_dataset\")\n",
        "#     parser.add_argument('--reset_stored_paths', type=str, default=\"False\")\n",
        "#     parser.add_argument('--experiment_name', nargs=\"?\", type=str, )\n",
        "#     parser.add_argument('--architecture_name', nargs=\"?\", type=str)\n",
        "#     parser.add_argument('--continue_from_epoch', nargs=\"?\", type=str, default='latest', help='Continue from checkpoint of epoch')\n",
        "#     parser.add_argument('--dropout_rate_value', type=float, default=0.3, help='Dropout_rate_value')\n",
        "#     parser.add_argument('--num_target_samples', type=int, default=15, help='Dropout_rate_value')\n",
        "#     parser.add_argument('--second_order', type=str, default=\"False\", help='Dropout_rate_value')\n",
        "#     parser.add_argument('--total_epochs', type=int, default=200, help='Number of epochs per experiment')\n",
        "#     parser.add_argument('--total_iter_per_epoch', type=int, default=500, help='Number of iters per epoch')\n",
        "#     parser.add_argument('--min_learning_rate', type=float, default=0.00001, help='Min learning rate')\n",
        "#     parser.add_argument('--meta_learning_rate', type=float, default=0.001, help='Learning rate of overall MAML system')\n",
        "#     parser.add_argument('--meta_opt_bn', type=str, default=\"False\")\n",
        "#     parser.add_argument('--task_learning_rate', type=float, default=0.1, help='Learning rate per task gradient step')\n",
        "\n",
        "#     parser.add_argument('--norm_layer', type=str, default=\"batch_norm\")\n",
        "#     parser.add_argument('--max_pooling', type=str, default=\"False\")\n",
        "#     parser.add_argument('--per_step_bn_statistics', type=str, default=\"False\")\n",
        "#     parser.add_argument('--num_classes_per_set', type=int, default=20, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--cnn_num_blocks', type=int, default=4, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--number_of_training_steps_per_iter', type=int, default=1, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--number_of_evaluation_steps_per_iter', type=int, default=1, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--cnn_num_filters', type=int, default=64, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--cnn_blocks_per_stage', type=int, default=1,\n",
        "#                         help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--num_samples_per_class', type=int, default=1, help='Number of samples per set to sample')\n",
        "#     parser.add_argument('--name_of_args_json_file', type=str, default=\"None\")\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "#     args_dict = vars(args)\n",
        "#     if args.name_of_args_json_file is not \"None\":\n",
        "#         args_dict = extract_args_from_json(args.name_of_args_json_file, args_dict)\n",
        "\n",
        "#     for key in list(args_dict.keys()):\n",
        "\n",
        "#         if str(args_dict[key]).lower() == \"true\":\n",
        "#             args_dict[key] = True\n",
        "#         elif str(args_dict[key]).lower() == \"false\":\n",
        "#             args_dict[key] = False\n",
        "#         if key == \"dataset_path\":\n",
        "#             args_dict[key] = os.path.join(os.environ['DATASET_DIR'], args_dict[key])\n",
        "#             print(key, os.path.join(os.environ['DATASET_DIR'], args_dict[key]))\n",
        "\n",
        "#         print(key, args_dict[key], type(args_dict[key]))\n",
        "\n",
        "#     args = Bunch(args_dict)\n",
        "\n",
        "\n",
        "#     args.use_cuda = torch.cuda.is_available()\n",
        "#     if torch.cuda.is_available():  # checks whether a cuda gpu is available and whether the gpu flag is True\n",
        "#         device = torch.cuda.current_device()\n",
        "\n",
        "#         print(\"use GPU\", device)\n",
        "#         print(\"GPU ID {}\".format(torch.cuda.current_device()))\n",
        "\n",
        "#     else:\n",
        "#         print(\"use CPU\")\n",
        "#         device = torch.device('cpu')  # sets the device to be CPU\n",
        "\n",
        "\n",
        "#     return args, device\n",
        "\n",
        "\n",
        "\n",
        "# class Bunch(object):\n",
        "#   def __init__(self, adict):\n",
        "#     self.__dict__.update(adict)\n",
        "\n",
        "# def extract_args_from_json(json_file_path, args_dict):\n",
        "#     import json\n",
        "#     summary_filename = json_file_path\n",
        "#     with open(summary_filename) as f:\n",
        "#         summary_dict = json.load(fp=f)\n",
        "\n",
        "#     for key in summary_dict.keys():\n",
        "#         if \"continue_from\" not in key and \"gpu_to_use\" not in key:\n",
        "#             args_dict[key] = summary_dict[key]\n",
        "\n",
        "#     return args_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RcsmAhVaCUNP"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "class Bunch(object):\n",
        "    def __init__(self, adict):\n",
        "        self.__dict__.update(adict)\n",
        "\n",
        "def load_args_from_json(json_file_path):\n",
        "    def extract_args_from_json(json_file_path, args_dict):\n",
        "        with open(json_file_path) as f:\n",
        "            summary_dict = json.load(fp=f)\n",
        "        for key, value in summary_dict.items():\n",
        "            if \"continue_from\" not in key and \"gpu_to_use\" not in key:\n",
        "                args_dict[key] = value\n",
        "        return args_dict\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Welcome to the MAML++ training and inference system')\n",
        "\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch_size for experiment')\n",
        "    parser.add_argument('--image_height', type=int, default=28)\n",
        "    parser.add_argument('--image_width', type=int, default=28)\n",
        "    parser.add_argument('--image_channels', type=int, default=1)\n",
        "    parser.add_argument('--reset_stored_filepaths', type=str, default=\"False\")\n",
        "    parser.add_argument('--reverse_channels', type=str, default=\"False\")\n",
        "    parser.add_argument('--num_of_gpus', type=int, default=1)\n",
        "    parser.add_argument('--indexes_of_folders_indicating_class', nargs='+', default=[-2, -3])\n",
        "    parser.add_argument('--train_val_test_split', nargs='+', default=[0.73982737361, 0.26, 0.13008631319])\n",
        "    parser.add_argument('--samples_per_iter', type=int, default=1)\n",
        "    parser.add_argument('--labels_as_int', type=str, default=\"False\")\n",
        "    parser.add_argument('--seed', type=int, default=104)\n",
        "\n",
        "    parser.add_argument('--gpu_to_use', type=int)\n",
        "    parser.add_argument('--num_dataprovider_workers', type=int, default=4)\n",
        "    parser.add_argument('--max_models_to_save', type=int, default=5)\n",
        "    parser.add_argument('--dataset_name', type=str, default=\"omniglot_dataset\")\n",
        "    parser.add_argument('--dataset_path', type=str, default=\"datasets/omniglot_dataset\")\n",
        "    parser.add_argument('--reset_stored_paths', type=str, default=\"False\")\n",
        "    parser.add_argument('--experiment_name', type=str)\n",
        "    parser.add_argument('--architecture_name', type=str)\n",
        "    parser.add_argument('--continue_from_epoch', type=str, default='latest', help='Continue from checkpoint of epoch')\n",
        "    parser.add_argument('--dropout_rate_value', type=float, default=0.3, help='Dropout_rate_value')\n",
        "    parser.add_argument('--num_target_samples', type=int, default=15, help='Dropout_rate_value')\n",
        "    parser.add_argument('--second_order', type=str, default=\"False\", help='Dropout_rate_value')\n",
        "    parser.add_argument('--total_epochs', type=int, default=200, help='Number of epochs per experiment')\n",
        "    parser.add_argument('--total_iter_per_epoch', type=int, default=500, help='Number of iters per epoch')\n",
        "    parser.add_argument('--min_learning_rate', type=float, default=0.00001, help='Min learning rate')\n",
        "    parser.add_argument('--meta_learning_rate', type=float, default=0.001, help='Learning rate of overall MAML system')\n",
        "    parser.add_argument('--meta_opt_bn', type=str, default=\"False\")\n",
        "    parser.add_argument('--task_learning_rate', type=float, default=0.1, help='Learning rate per task gradient step')\n",
        "\n",
        "    parser.add_argument('--norm_layer', type=str, default=\"batch_norm\")\n",
        "    parser.add_argument('--max_pooling', type=str, default=\"False\")\n",
        "    parser.add_argument('--per_step_bn_statistics', type=str, default=\"False\")\n",
        "    parser.add_argument('--num_classes_per_set', type=int, default=20, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--cnn_num_blocks', type=int, default=4, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--number_of_training_steps_per_iter', type=int, default=1, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--number_of_evaluation_steps_per_iter', type=int, default=1, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--cnn_num_filters', type=int, default=64, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--cnn_blocks_per_stage', type=int, default=1, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--num_samples_per_class', type=int, default=1, help='Number of samples per set to sample')\n",
        "    parser.add_argument('--name_of_args_json_file', type=str, default=\"None\")\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "    args_dict = vars(args)\n",
        "\n",
        "    # Override args with JSON file values\n",
        "    if json_file_path:\n",
        "        args_dict = extract_args_from_json(json_file_path, args_dict)\n",
        "\n",
        "    # Convert string-based booleans to actual booleans\n",
        "    for key in args_dict:\n",
        "        if isinstance(args_dict[key], str) and args_dict[key].lower() == \"true\":\n",
        "            args_dict[key] = True\n",
        "        elif isinstance(args_dict[key], str) and args_dict[key].lower() == \"false\":\n",
        "            args_dict[key] = False\n",
        "\n",
        "    # Resolve dataset path if environment variable is set\n",
        "    if \"dataset_path\" in args_dict and config[\"dataset_path\"]:\n",
        "        args_dict[\"dataset_path\"] = os.path.join(config[\"dataset_path\"], args_dict[\"dataset_path\"])\n",
        "\n",
        "    args = Bunch(args_dict)\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    args.use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if args.use_cuda else 'cpu')\n",
        "\n",
        "    return args, device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Hb1S5Yh6zf6J"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def maybe_unzip_dataset(args):\n",
        "\n",
        "    datasets = [args.dataset_name]\n",
        "    dataset_paths = [args.dataset_path]\n",
        "    done = False\n",
        "\n",
        "    for dataset_idx, dataset_path in enumerate(dataset_paths):\n",
        "        if dataset_path.endswith('/'):\n",
        "            dataset_path = dataset_path[:-1]\n",
        "        # print(dataset_path)\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(\"Not found dataset folder structure.. searching for .tar.bz2 file\")\n",
        "            zip_directory = \"{}.tar.bz2\".format(os.path.join(config[\"dataset_path\"], datasets[dataset_idx]))\n",
        "\n",
        "            assert os.path.exists(os.path.abspath(zip_directory)), \"{} dataset zip file not found\" \\\n",
        "                                                  \"place dataset in datasets folder as explained in README\".format(os.path.abspath(zip_directory))\n",
        "            print(\"Found zip file, unpacking\")\n",
        "\n",
        "            unzip_file(filepath_pack=os.path.join(config[\"dataset_path\"], \"{}.tar.bz2\".format(datasets[dataset_idx])),\n",
        "                       filepath_to_store=config[\"dataset_path\"])\n",
        "\n",
        "\n",
        "\n",
        "            args.reset_stored_filepaths = True\n",
        "\n",
        "        total_files = 0\n",
        "        for subdir, dir, files in os.walk(dataset_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(\".jpeg\") or file.lower().endswith(\".jpg\") or file.lower().endswith(\n",
        "                        \".png\") or file.lower().endswith(\".pkl\"):\n",
        "                    total_files += 1\n",
        "        print(\"count stuff________________________________________\", total_files)\n",
        "        if (total_files == 1623 * 20 and datasets[dataset_idx] == 'omniglot_dataset') or (\n",
        "                total_files == 100 * 600 and 'mini_imagenet' in datasets[dataset_idx]) or (\n",
        "                total_files == 3 and 'mini_imagenet_pkl' in datasets[dataset_idx]):\n",
        "            print(\"file count is correct\")\n",
        "            done = True\n",
        "        elif datasets[dataset_idx] not in [\n",
        "            'omniglot_dataset',\n",
        "            'mini_imagenet',\n",
        "            'mini_imagenet_pkl',\n",
        "        ]:\n",
        "            done = True\n",
        "            print(\"using new dataset\")\n",
        "\n",
        "        if not done:\n",
        "            shutil.rmtree(dataset_path, ignore_errors=True)\n",
        "            maybe_unzip_dataset(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UKVzZ6KPXvzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cfae0d-c2a9-4785-9989-d85097de1871"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "os.chdir('/content/HowToTrainYourMAMLPytorch')\n",
        "\n",
        "\n",
        "# Create parent directories\n",
        "os.makedirs('/home/antreas', exist_ok=True)\n",
        "\n",
        "# Create symbolic link from /content to /home/antreas\n",
        "os.system('ln -s /content/HowToTrainYourMAMLPytorch /home/antreas/HowToTrainYourMAMLPytorch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuRv8nXqp2zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f36d1de-b085-47de-9403-73da190f9885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using max pooling\n",
            "torch.Size([2, 48, 84, 84])\n",
            "torch.Size([2, 48, 42, 42])\n",
            "torch.Size([2, 48, 21, 21])\n",
            "torch.Size([2, 48, 10, 10])\n",
            "VGGNetwork build torch.Size([2, 5])\n",
            "meta network params\n",
            "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
            "layer_dict.conv0.conv.bias torch.Size([48])\n",
            "layer_dict.conv0.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv1.conv.bias torch.Size([48])\n",
            "layer_dict.conv1.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv2.conv.bias torch.Size([48])\n",
            "layer_dict.conv2.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv3.conv.bias torch.Size([48])\n",
            "layer_dict.conv3.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.linear.weights torch.Size([5, 1200])\n",
            "layer_dict.linear.bias torch.Size([5])\n",
            "0.1\n",
            "Inner Loop parameters\n",
            "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
            "Outer Loop parameters\n",
            "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cpu True\n",
            "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cpu True\n",
            "classifier.layer_dict.conv0.norm_layer.bias torch.Size([5, 48]) cpu True\n",
            "classifier.layer_dict.conv0.norm_layer.weight torch.Size([5, 48]) cpu True\n",
            "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cpu True\n",
            "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cpu True\n",
            "classifier.layer_dict.conv1.norm_layer.bias torch.Size([5, 48]) cpu True\n",
            "classifier.layer_dict.conv1.norm_layer.weight torch.Size([5, 48]) cpu True\n",
            "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cpu True\n",
            "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cpu True\n",
            "classifier.layer_dict.conv2.norm_layer.bias torch.Size([5, 48]) cpu True\n",
            "classifier.layer_dict.conv2.norm_layer.weight torch.Size([5, 48]) cpu True\n",
            "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cpu True\n",
            "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cpu True\n",
            "classifier.layer_dict.conv3.norm_layer.bias torch.Size([5, 48]) cpu True\n",
            "classifier.layer_dict.conv3.norm_layer.weight torch.Size([5, 48]) cpu True\n",
            "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cpu True\n",
            "classifier.layer_dict.linear.bias torch.Size([5]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cpu True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cpu True\n",
            "the grinch added tensor([[[[ 0.1376, -0.0976,  0.0653],\n",
            "          [-0.0331,  0.1308,  0.1510],\n",
            "          [ 0.1327,  0.0213, -0.1057]],\n",
            "\n",
            "         [[ 0.1067,  0.1059, -0.0642],\n",
            "          [ 0.0101,  0.1245,  0.1097],\n",
            "          [-0.0159, -0.0164, -0.0181]],\n",
            "\n",
            "         [[-0.0239,  0.0878,  0.0302],\n",
            "          [ 0.0451,  0.0353,  0.1015],\n",
            "          [ 0.0769, -0.0073,  0.0098]]],\n",
            "\n",
            "\n",
            "        [[[-0.1181,  0.0119, -0.0915],\n",
            "          [ 0.0142,  0.1404, -0.0378],\n",
            "          [-0.1744, -0.0003, -0.0147]],\n",
            "\n",
            "         [[ 0.0451,  0.1387, -0.0534],\n",
            "          [-0.0723,  0.0481, -0.0224],\n",
            "          [-0.0243, -0.0548,  0.0751]],\n",
            "\n",
            "         [[-0.0840, -0.1888, -0.0079],\n",
            "          [ 0.0186,  0.0852,  0.1136],\n",
            "          [-0.0139,  0.0238, -0.1982]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1030, -0.0785, -0.0476],\n",
            "          [-0.0247, -0.0439, -0.0289],\n",
            "          [ 0.0892, -0.1222,  0.1290]],\n",
            "\n",
            "         [[-0.0392,  0.1888, -0.0460],\n",
            "          [ 0.0923,  0.1878, -0.1017],\n",
            "          [ 0.0616,  0.1257,  0.0466]],\n",
            "\n",
            "         [[ 0.0784, -0.0193,  0.0578],\n",
            "          [-0.1306, -0.0261, -0.0620],\n",
            "          [-0.0627, -0.1613,  0.0554]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0524, -0.0668, -0.0983],\n",
            "          [ 0.0695, -0.0353,  0.0676],\n",
            "          [-0.0398, -0.1224,  0.1314]],\n",
            "\n",
            "         [[-0.0403, -0.0758,  0.1001],\n",
            "          [ 0.0671,  0.0439,  0.0376],\n",
            "          [ 0.1072, -0.0015,  0.0375]],\n",
            "\n",
            "         [[-0.0879, -0.0752,  0.1611],\n",
            "          [ 0.1068,  0.0118, -0.0156],\n",
            "          [ 0.0168, -0.1091,  0.1063]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0613, -0.0621,  0.0247],\n",
            "          [-0.0114, -0.1014,  0.0338],\n",
            "          [-0.0048,  0.0283, -0.2268]],\n",
            "\n",
            "         [[ 0.0129, -0.0562,  0.0627],\n",
            "          [-0.0205, -0.0682,  0.1829],\n",
            "          [ 0.0411,  0.0251,  0.0778]],\n",
            "\n",
            "         [[ 0.1100, -0.0790,  0.0856],\n",
            "          [ 0.1371,  0.0361,  0.0743],\n",
            "          [-0.0755, -0.1461, -0.0305]]],\n",
            "\n",
            "\n",
            "        [[[-0.1578,  0.1976,  0.0174],\n",
            "          [-0.0520, -0.1337,  0.0380],\n",
            "          [ 0.0015, -0.0322,  0.0012]],\n",
            "\n",
            "         [[ 0.1768,  0.0135,  0.0020],\n",
            "          [-0.0238, -0.0393,  0.0420],\n",
            "          [ 0.1454, -0.0111, -0.1238]],\n",
            "\n",
            "         [[-0.0399,  0.0283,  0.0878],\n",
            "          [-0.0864,  0.1425,  0.0437],\n",
            "          [-0.0644,  0.0908,  0.0135]]]]) amount of noise :)\n",
            "the grinch added tensor([ 0.1004,  0.0753, -0.0835,  0.0226, -0.0835, -0.0603,  0.0329,  0.1232,\n",
            "         0.0976, -0.0157,  0.1059, -0.0482,  0.0889,  0.0056,  0.0199,  0.1007,\n",
            "        -0.0044,  0.0256,  0.0288,  0.1015,  0.0651,  0.0600, -0.0254,  0.1252,\n",
            "         0.0388, -0.1170, -0.0027, -0.1025,  0.1121, -0.0905, -0.1415, -0.0921,\n",
            "        -0.2130,  0.0236,  0.0235, -0.1203,  0.0529,  0.1194, -0.0541, -0.0700,\n",
            "         0.0340, -0.1226,  0.0850,  0.0432, -0.0205,  0.0436, -0.0261,  0.0698]) amount of noise :)\n",
            "the grinch added tensor([[ 1.1062e-01, -1.3487e-01, -1.3790e-01,  2.0281e-01, -7.6910e-02,\n",
            "          1.5183e-01, -1.4295e-01,  2.6669e-02, -6.6743e-02,  8.3582e-03,\n",
            "          1.7151e-01, -8.1829e-02,  1.0273e-02,  7.3238e-02, -8.4068e-02,\n",
            "          1.1694e-01, -1.8909e-02, -1.4041e-02, -5.9446e-02,  1.4019e-03,\n",
            "          1.2491e-01, -1.2978e-01,  2.1462e-02, -2.1512e-02, -6.5517e-02,\n",
            "          6.0714e-04,  6.2474e-02, -1.7300e-01,  6.1971e-02, -1.6952e-01,\n",
            "         -2.2060e-02, -1.0525e-01, -3.1124e-02,  4.0726e-02, -1.0751e-01,\n",
            "         -1.0173e-01,  9.0149e-03,  3.9332e-02,  1.2104e-01,  4.6040e-02,\n",
            "         -1.2156e-01, -6.2847e-02,  2.1337e-01,  7.6762e-02, -2.1430e-02,\n",
            "         -5.3775e-02,  8.2354e-02,  1.2690e-01],\n",
            "        [ 2.5959e-01, -8.6926e-02, -7.0360e-02,  1.3038e-01,  1.0897e-01,\n",
            "          9.8417e-02, -4.9230e-02, -6.8261e-02, -3.6799e-02, -1.3922e-01,\n",
            "         -3.3077e-02,  9.0528e-02, -1.3573e-01, -2.7080e-01, -1.1491e-01,\n",
            "          1.7797e-01, -1.0383e-01, -2.5886e-01, -8.8510e-02, -2.6449e-02,\n",
            "          5.9442e-02, -7.8578e-02,  8.5038e-03,  1.5275e-01,  3.2346e-05,\n",
            "         -1.0804e-01,  2.1772e-02, -9.0841e-02, -1.7623e-02,  6.0153e-02,\n",
            "         -4.9511e-02,  9.1446e-02, -1.1091e-01,  2.7085e-02,  1.5397e-01,\n",
            "         -6.2302e-02,  1.3668e-01, -1.9744e-01, -4.1075e-02, -2.9022e-02,\n",
            "         -9.1571e-02,  4.5220e-02, -5.6143e-02, -3.0636e-02,  5.8120e-02,\n",
            "          1.8776e-01, -2.1504e-01,  5.2154e-02],\n",
            "        [ 1.7045e-02,  9.1826e-05, -2.0062e-01, -2.6115e-02, -1.7383e-01,\n",
            "          9.9578e-02,  4.8755e-02,  1.0936e-01, -1.1052e-01, -9.6769e-02,\n",
            "         -2.7810e-02, -5.5626e-02,  1.8319e-02,  3.5355e-02,  5.9844e-03,\n",
            "         -6.1839e-02, -1.3057e-01,  3.9792e-02,  6.6811e-02,  2.0340e-02,\n",
            "          2.3093e-02, -4.6745e-02,  5.8497e-03,  7.8913e-02,  3.3478e-02,\n",
            "         -1.4205e-01, -4.0435e-03, -3.2465e-02, -3.1465e-02,  1.4840e-01,\n",
            "         -1.0568e-01,  1.9079e-01, -8.9589e-02,  9.5901e-02, -1.6167e-01,\n",
            "         -1.5099e-02,  1.3914e-02, -7.4710e-02,  3.3228e-02, -1.4742e-01,\n",
            "         -2.8221e-02,  6.6668e-02,  4.6608e-02,  2.9049e-02,  1.3927e-01,\n",
            "          9.9578e-02, -1.7801e-01, -8.6750e-02],\n",
            "        [ 6.9596e-02,  5.2293e-02, -2.6688e-02,  8.9162e-02,  8.1815e-03,\n",
            "          2.1744e-02,  7.8335e-02,  1.9319e-01,  3.8901e-02, -1.3891e-01,\n",
            "          1.6980e-01,  1.1919e-02,  2.1531e-01,  1.7042e-02,  2.6329e-02,\n",
            "         -7.3408e-02, -1.6348e-01, -6.1701e-03,  1.6110e-02,  1.5183e-01,\n",
            "         -8.4726e-02,  1.6126e-01, -6.5982e-03,  1.1622e-01, -7.0485e-02,\n",
            "         -7.2641e-02, -1.5769e-02, -2.3452e-01, -4.6073e-03, -6.3538e-02,\n",
            "         -1.4969e-03, -6.3348e-02,  3.9411e-02,  1.0762e-01,  4.8724e-03,\n",
            "         -1.5014e-01, -1.3524e-03,  1.3640e-01,  6.7628e-02, -8.2034e-02,\n",
            "         -1.3531e-01,  4.9122e-02, -1.0709e-01,  2.6208e-02,  7.5442e-02,\n",
            "          1.5789e-02,  4.0071e-01,  9.1994e-02],\n",
            "        [ 2.7987e-02,  1.2712e-01, -8.9743e-02,  7.7458e-02, -1.4131e-02,\n",
            "          1.8218e-02,  3.7064e-02,  4.7804e-03,  2.4164e-02,  1.4001e-01,\n",
            "          1.2189e-02, -8.7013e-03, -8.1414e-02,  7.9299e-02, -1.0501e-01,\n",
            "          1.7523e-01, -3.1301e-02, -1.0201e-01, -2.2729e-01,  6.4841e-02,\n",
            "          2.1932e-02, -4.1540e-02,  4.4757e-02, -5.9170e-03, -1.7448e-01,\n",
            "         -4.3751e-03, -7.4077e-02, -8.9644e-02, -5.3542e-02, -7.7218e-02,\n",
            "          1.5418e-02, -2.7526e-02, -6.3472e-02, -1.3151e-01,  2.6968e-01,\n",
            "          8.8943e-02,  5.6765e-03,  1.2748e-02, -7.2702e-02,  9.8771e-02,\n",
            "         -4.1844e-03,  8.7262e-02,  8.9841e-02, -1.2092e-01,  1.1699e-01,\n",
            "          2.3020e-02,  5.8601e-02,  1.1616e-01]]) amount of noise :)\n",
            "the grinch added tensor([[ 1.3524e-03,  1.2507e-01, -1.4487e-01,  1.7096e-01,  1.1776e-01,\n",
            "          7.5951e-02,  4.3196e-02, -5.9626e-02, -7.0707e-02,  5.9640e-02,\n",
            "          7.0733e-02,  2.3309e-02,  7.9509e-02, -2.9231e-02, -1.2849e-01,\n",
            "         -1.9598e-02, -2.0086e-01,  3.2040e-02, -4.3272e-02, -1.6361e-01,\n",
            "          1.6562e-01,  6.5606e-02, -1.8754e-02, -1.0492e-01,  4.0518e-02,\n",
            "         -3.4312e-02, -1.9927e-02,  7.4296e-02,  1.3123e-01,  6.2360e-02,\n",
            "          1.1189e-01, -1.5945e-01, -1.5310e-02,  1.5181e-01,  5.9051e-02,\n",
            "         -4.3616e-02,  8.4147e-02,  7.8755e-02,  1.0095e-03,  1.3742e-01,\n",
            "         -2.4438e-02, -9.9120e-02,  2.4864e-02, -1.3641e-01,  5.1811e-02,\n",
            "          1.1382e-01,  4.5863e-02,  1.6172e-01],\n",
            "        [ 1.0564e-01, -4.0411e-03,  8.7947e-02,  1.0027e-01,  1.5847e-01,\n",
            "          2.3203e-02, -1.8528e-01,  1.6305e-01, -4.7924e-02, -5.1072e-02,\n",
            "          8.9103e-02,  2.4658e-02, -6.0456e-03,  1.1984e-01,  6.4251e-02,\n",
            "         -6.5372e-02, -8.7446e-02, -1.8154e-01,  5.9930e-02,  1.4794e-01,\n",
            "          5.5138e-02, -1.1127e-02,  1.1342e-01, -1.1555e-01,  9.3698e-02,\n",
            "         -4.0694e-02, -1.2771e-01, -1.5648e-01, -2.7559e-02,  1.1640e-02,\n",
            "         -7.2207e-02,  5.7190e-02,  8.2855e-02,  1.8240e-01, -2.7864e-02,\n",
            "         -1.5750e-01, -1.8205e-02, -2.7068e-02, -4.1397e-02, -3.3899e-02,\n",
            "          2.8462e-02, -1.7817e-01, -3.9920e-02, -1.2721e-02, -6.6072e-03,\n",
            "         -6.4590e-02,  1.3952e-01, -6.1641e-02],\n",
            "        [ 1.9684e-01,  1.0383e-01, -2.7486e-02, -2.3809e-02,  5.6445e-02,\n",
            "         -2.4511e-01, -1.4944e-01,  8.2617e-02,  3.7210e-02,  3.7393e-02,\n",
            "         -3.4404e-03,  1.5630e-01, -1.0483e-01,  1.3969e-01, -5.5363e-02,\n",
            "         -2.0158e-01,  1.5372e-01,  2.2462e-02,  1.5849e-01, -1.4683e-01,\n",
            "         -2.2624e-02, -8.4429e-02, -5.2092e-02,  1.5475e-02,  2.2870e-02,\n",
            "          1.8559e-01, -4.6820e-02,  9.6787e-02, -1.1103e-01, -1.0923e-01,\n",
            "         -4.2953e-03, -3.1411e-02, -9.6038e-03,  5.9086e-02, -8.9913e-02,\n",
            "          3.1021e-02, -9.1583e-02, -1.6911e-01,  1.5249e-01, -4.0075e-03,\n",
            "          1.9142e-01, -9.1991e-02, -2.8431e-02,  3.8441e-02,  8.3290e-02,\n",
            "          5.7105e-02, -2.1859e-02,  3.6713e-02],\n",
            "        [-3.5968e-02, -3.9868e-02, -1.0347e-04, -1.7733e-01,  5.1541e-02,\n",
            "          4.5884e-02,  1.6502e-01, -9.7985e-02, -1.2438e-01, -1.8407e-02,\n",
            "         -7.1419e-03,  5.3686e-02,  8.3774e-02,  2.2008e-01, -3.5712e-02,\n",
            "         -3.5675e-03, -7.2301e-02, -8.2130e-02,  2.0169e-01, -1.1210e-01,\n",
            "         -1.3113e-01, -1.8052e-01, -6.7023e-02,  8.2079e-02,  1.3929e-01,\n",
            "          5.1185e-02, -1.2659e-01,  5.9445e-02, -3.6495e-02,  1.7629e-01,\n",
            "          1.0197e-01,  3.6790e-02, -1.2247e-01, -1.1378e-01, -6.5574e-02,\n",
            "         -3.3291e-02,  2.3944e-01,  6.4262e-02, -2.9783e-02, -3.9666e-02,\n",
            "         -3.9973e-02,  4.5849e-02, -6.5688e-02,  2.6993e-02, -1.0899e-01,\n",
            "          6.9025e-03,  3.1948e-02,  9.5435e-03],\n",
            "        [ 1.2941e-02,  4.4097e-03,  8.2424e-02, -1.2145e-02, -1.0904e-01,\n",
            "         -8.7050e-02, -2.5196e-02,  7.6016e-03,  7.4929e-02,  4.0350e-02,\n",
            "         -6.6262e-03, -3.2671e-02, -1.5101e-01,  1.5169e-01,  1.2870e-01,\n",
            "         -8.0384e-02, -4.5685e-02, -9.4818e-03, -6.3308e-02,  1.2254e-01,\n",
            "         -8.2562e-02, -2.5711e-02, -6.6303e-02,  2.3083e-02, -2.9370e-02,\n",
            "          6.4421e-02, -1.6228e-01, -1.3831e-01, -2.1725e-01,  7.4089e-03,\n",
            "          9.3474e-02,  1.0239e-01,  6.7327e-02,  9.0032e-02,  1.7244e-01,\n",
            "          1.2019e-01,  1.4683e-01, -5.8700e-02,  1.7074e-02, -5.3486e-02,\n",
            "         -2.6384e-02,  1.3001e-01, -1.1553e-01,  2.6290e-02, -1.3436e-01,\n",
            "          1.3866e-01, -2.1287e-01,  1.8153e-01]]) amount of noise :)\n",
            "the grinch added tensor([[[[-0.0728, -0.1736, -0.1021],\n",
            "          [ 0.0462, -0.0885,  0.0150],\n",
            "          [-0.0603, -0.1971,  0.0378]],\n",
            "\n",
            "         [[ 0.0395, -0.1524, -0.0414],\n",
            "          [-0.0403,  0.0665,  0.1943],\n",
            "          [ 0.0823, -0.2587, -0.0922]],\n",
            "\n",
            "         [[-0.0828,  0.0410, -0.0092],\n",
            "          [-0.0922,  0.0299,  0.1833],\n",
            "          [-0.0919,  0.0462,  0.0481]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0637,  0.0252,  0.0968],\n",
            "          [ 0.1313,  0.0009,  0.0160],\n",
            "          [-0.0209, -0.0472, -0.1043]],\n",
            "\n",
            "         [[ 0.0400,  0.0702,  0.1959],\n",
            "          [ 0.0865,  0.1179, -0.1612],\n",
            "          [ 0.1290,  0.1000, -0.1428]],\n",
            "\n",
            "         [[-0.3474, -0.0566,  0.0944],\n",
            "          [ 0.1443,  0.0050,  0.0091],\n",
            "          [-0.0272, -0.0825,  0.0208]]],\n",
            "\n",
            "\n",
            "        [[[-0.0307, -0.0136,  0.0786],\n",
            "          [-0.1176,  0.1328,  0.1556],\n",
            "          [ 0.0078, -0.0281,  0.0408]],\n",
            "\n",
            "         [[ 0.0035, -0.1566,  0.0084],\n",
            "          [-0.0535, -0.0806,  0.1131],\n",
            "          [-0.1184,  0.0358,  0.0009]],\n",
            "\n",
            "         [[ 0.0591, -0.1322,  0.0252],\n",
            "          [-0.0431,  0.2518, -0.0087],\n",
            "          [-0.0017,  0.1125,  0.0411]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1084,  0.0527,  0.1670],\n",
            "          [-0.0166,  0.0352,  0.0980],\n",
            "          [ 0.0205, -0.0220,  0.0610]],\n",
            "\n",
            "         [[ 0.0291,  0.0467, -0.0383],\n",
            "          [ 0.0140, -0.1290, -0.0718],\n",
            "          [ 0.0749, -0.1516,  0.0067]],\n",
            "\n",
            "         [[ 0.0744, -0.0277,  0.0188],\n",
            "          [-0.0666, -0.0025, -0.0871],\n",
            "          [ 0.0188, -0.3359,  0.0443]]],\n",
            "\n",
            "\n",
            "        [[[-0.1053, -0.0546,  0.2140],\n",
            "          [ 0.0541,  0.1866, -0.0272],\n",
            "          [ 0.0236,  0.0532,  0.0350]],\n",
            "\n",
            "         [[ 0.0191, -0.0264, -0.1381],\n",
            "          [ 0.0703,  0.0674,  0.0746],\n",
            "          [ 0.0962,  0.0855, -0.0995]],\n",
            "\n",
            "         [[ 0.0967, -0.0317, -0.1701],\n",
            "          [-0.0985,  0.0034, -0.0219],\n",
            "          [ 0.0526,  0.1419,  0.2290]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1107, -0.0620, -0.0776],\n",
            "          [ 0.0263, -0.0721, -0.0538],\n",
            "          [-0.0836,  0.0503,  0.1188]],\n",
            "\n",
            "         [[-0.0971,  0.0259,  0.0543],\n",
            "          [-0.0732,  0.2221, -0.0801],\n",
            "          [ 0.1242, -0.0377, -0.0419]],\n",
            "\n",
            "         [[-0.0799, -0.0880,  0.0255],\n",
            "          [ 0.1648, -0.1521,  0.1306],\n",
            "          [ 0.0610, -0.1025, -0.0090]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0494,  0.0328,  0.0707],\n",
            "          [ 0.0301, -0.0878, -0.0593],\n",
            "          [ 0.1689,  0.1653, -0.0016]],\n",
            "\n",
            "         [[ 0.0194,  0.1337, -0.0032],\n",
            "          [-0.1278, -0.0708, -0.1173],\n",
            "          [ 0.1062,  0.0949,  0.0468]],\n",
            "\n",
            "         [[ 0.1654, -0.0263, -0.0374],\n",
            "          [ 0.0377, -0.2337, -0.2478],\n",
            "          [-0.1406, -0.1834, -0.0918]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0160, -0.1614, -0.1637],\n",
            "          [ 0.0055,  0.0783, -0.0441],\n",
            "          [ 0.0056, -0.0998, -0.0854]],\n",
            "\n",
            "         [[ 0.0583,  0.2354, -0.0182],\n",
            "          [ 0.0422, -0.0179,  0.1367],\n",
            "          [-0.0765, -0.1068, -0.0041]],\n",
            "\n",
            "         [[-0.0874, -0.0217, -0.1058],\n",
            "          [-0.0238, -0.1539, -0.0029],\n",
            "          [ 0.1261, -0.0068,  0.0118]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0564,  0.0111,  0.0676],\n",
            "          [-0.0962,  0.1412, -0.0186],\n",
            "          [-0.0522,  0.0120, -0.0795]],\n",
            "\n",
            "         [[-0.1213,  0.1285, -0.2445],\n",
            "          [ 0.0307, -0.1813,  0.1037],\n",
            "          [-0.1732, -0.0232, -0.0689]],\n",
            "\n",
            "         [[-0.0078, -0.0505, -0.0142],\n",
            "          [-0.0651, -0.1876, -0.0585],\n",
            "          [ 0.0038,  0.0314,  0.1389]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0282,  0.0152,  0.1252],\n",
            "          [-0.0451, -0.0455, -0.1082],\n",
            "          [ 0.0202, -0.0821,  0.0661]],\n",
            "\n",
            "         [[-0.1194,  0.0344,  0.0850],\n",
            "          [ 0.1169,  0.0995,  0.0110],\n",
            "          [ 0.1506,  0.1039, -0.0160]],\n",
            "\n",
            "         [[-0.1159, -0.0905,  0.0193],\n",
            "          [-0.1214,  0.0415, -0.1199],\n",
            "          [-0.0849,  0.1083, -0.1141]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0123, -0.1051,  0.0715],\n",
            "          [ 0.0394, -0.0461,  0.0129],\n",
            "          [ 0.1212,  0.1884,  0.0122]],\n",
            "\n",
            "         [[ 0.0113, -0.0150,  0.0722],\n",
            "          [-0.0045, -0.0304,  0.0855],\n",
            "          [-0.0389,  0.0109, -0.0372]],\n",
            "\n",
            "         [[-0.0969, -0.0516,  0.1260],\n",
            "          [-0.1182, -0.1278,  0.1113],\n",
            "          [ 0.0476,  0.1290, -0.1993]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0301,  0.0142, -0.0570],\n",
            "          [-0.0272,  0.0687,  0.0760],\n",
            "          [ 0.0068,  0.0469, -0.1262]],\n",
            "\n",
            "         [[-0.1781,  0.0354, -0.0262],\n",
            "          [-0.2011, -0.1385,  0.2125],\n",
            "          [-0.0785,  0.0206,  0.1868]],\n",
            "\n",
            "         [[-0.1923, -0.1991,  0.1879],\n",
            "          [-0.1006, -0.0310, -0.0348],\n",
            "          [-0.2562, -0.0090,  0.1958]]]]) amount of noise :)\n",
            "the grinch added tensor([ 0.2355, -0.0945,  0.1371,  0.0788,  0.0115, -0.1094, -0.1300,  0.1314,\n",
            "         0.0864,  0.0012, -0.0849,  0.0011, -0.0440, -0.0233, -0.0264, -0.0937,\n",
            "        -0.0115, -0.1381,  0.0808, -0.1054, -0.1725,  0.0505, -0.1114,  0.0299,\n",
            "         0.1843, -0.1453, -0.0773,  0.1675, -0.1407, -0.0601,  0.0446, -0.1516,\n",
            "         0.0784,  0.0849,  0.0088, -0.0638,  0.0567,  0.1403, -0.0384, -0.0057,\n",
            "        -0.0669,  0.0348,  0.0130,  0.0904,  0.1065, -0.1000, -0.1032, -0.0460]) amount of noise :)\n",
            "the grinch added tensor([[-0.0146,  0.1028, -0.0464,  0.1258,  0.1000,  0.1727,  0.0571,  0.0723,\n",
            "         -0.0230,  0.0500, -0.0118,  0.0897, -0.1248, -0.2319,  0.0569, -0.0772,\n",
            "          0.0178,  0.0089,  0.0539,  0.1567,  0.0817,  0.1137, -0.1409,  0.0226,\n",
            "         -0.0023,  0.0174,  0.0734, -0.0614,  0.0790,  0.0127,  0.0618,  0.0822,\n",
            "         -0.0980, -0.1822,  0.1072,  0.1355,  0.0172, -0.0196,  0.0331,  0.0981,\n",
            "          0.0603, -0.1805,  0.1779,  0.0723,  0.0199, -0.0834, -0.0583,  0.0575],\n",
            "        [ 0.0817,  0.1425,  0.0294, -0.0965, -0.0493, -0.0582, -0.0016, -0.0365,\n",
            "          0.0071,  0.0437,  0.0309, -0.0239,  0.2647,  0.0428,  0.0647,  0.0582,\n",
            "          0.0280, -0.1153,  0.0104, -0.0656, -0.1267, -0.0420, -0.0064, -0.0204,\n",
            "         -0.0258,  0.1197, -0.0360, -0.0308,  0.0122,  0.0732,  0.0157, -0.1282,\n",
            "          0.0146,  0.0265,  0.0946,  0.0626, -0.1143,  0.1206,  0.2138, -0.0100,\n",
            "          0.1606, -0.0260,  0.0217,  0.0261, -0.0602, -0.0289,  0.0056,  0.0140],\n",
            "        [-0.0361, -0.0656,  0.1243,  0.0103, -0.0201,  0.1225,  0.1877,  0.2775,\n",
            "         -0.0668,  0.0961,  0.0017,  0.0024, -0.1018,  0.2276,  0.1220, -0.0408,\n",
            "          0.1160,  0.0725, -0.0748, -0.0880, -0.0591, -0.0248,  0.0822, -0.1025,\n",
            "          0.0253, -0.0415, -0.0471,  0.0425, -0.0083, -0.1263,  0.1363,  0.0142,\n",
            "         -0.2023,  0.0361, -0.0508,  0.1147, -0.0955, -0.0361,  0.0764, -0.0980,\n",
            "         -0.0336,  0.0725, -0.1227, -0.0289,  0.0024,  0.1254,  0.0132,  0.0623],\n",
            "        [-0.0778,  0.0403,  0.1806,  0.1544, -0.1824, -0.0075, -0.0775, -0.0289,\n",
            "         -0.0307, -0.0872, -0.0914, -0.1079, -0.0075, -0.0724, -0.0164,  0.0132,\n",
            "          0.0737, -0.0302, -0.0297,  0.0569,  0.0850, -0.0241,  0.1210,  0.0853,\n",
            "         -0.0639, -0.0657, -0.1190,  0.0353, -0.1187, -0.1513, -0.1497,  0.0313,\n",
            "         -0.0026, -0.1524,  0.0180,  0.0405,  0.0809, -0.0071,  0.0701, -0.0163,\n",
            "         -0.0793, -0.1520,  0.1258, -0.0855, -0.0648, -0.1407, -0.1134,  0.0308],\n",
            "        [-0.0524, -0.1171, -0.0648,  0.1882,  0.0138,  0.0963,  0.0225, -0.1115,\n",
            "         -0.0776,  0.1233,  0.0539,  0.0069,  0.0412, -0.1982,  0.0207,  0.0343,\n",
            "          0.1200, -0.0373,  0.0393, -0.2183, -0.0900, -0.1073,  0.1047,  0.1135,\n",
            "         -0.0139,  0.2260, -0.1259,  0.1190, -0.0735,  0.0247, -0.0651,  0.0331,\n",
            "         -0.2354,  0.0540,  0.0612,  0.0556, -0.1787,  0.0125,  0.0917, -0.2081,\n",
            "          0.0532, -0.0751,  0.0322, -0.1648, -0.0205, -0.1302,  0.0943, -0.2683]]) amount of noise :)\n",
            "the grinch added tensor([[ 0.0368,  0.1374,  0.0312,  0.0426, -0.0678,  0.0909, -0.0643, -0.0640,\n",
            "          0.0077,  0.0573,  0.0803, -0.1105, -0.1654, -0.0810,  0.1637, -0.1426,\n",
            "          0.0417, -0.0915, -0.0107,  0.1998,  0.0479, -0.1909, -0.0170, -0.0683,\n",
            "         -0.0041, -0.1094,  0.0894,  0.1330,  0.1511,  0.0814, -0.1600,  0.0083,\n",
            "          0.0252, -0.0845, -0.0270,  0.0008,  0.1194,  0.0425, -0.0892, -0.0284,\n",
            "         -0.2299, -0.1590, -0.0616,  0.0784,  0.0551,  0.0907,  0.0799, -0.1358],\n",
            "        [-0.2320, -0.1131, -0.0700,  0.0554,  0.0220,  0.0548,  0.0333,  0.1181,\n",
            "          0.0041, -0.1045, -0.0873,  0.0071,  0.1601, -0.0730, -0.0019, -0.1266,\n",
            "          0.0623, -0.0920,  0.1550, -0.1590,  0.1200, -0.1325,  0.1217,  0.0989,\n",
            "         -0.0209,  0.0630,  0.1220, -0.1141, -0.0549,  0.1363, -0.0079,  0.3529,\n",
            "         -0.0199, -0.0176, -0.1326,  0.1225, -0.0096, -0.0648,  0.0577, -0.2058,\n",
            "          0.0019, -0.0807, -0.0795, -0.0581, -0.1256, -0.0096, -0.1316, -0.0898],\n",
            "        [ 0.0872,  0.0406,  0.0671,  0.0125,  0.1480,  0.1928, -0.0297, -0.0542,\n",
            "          0.0840, -0.0036,  0.0208, -0.1505, -0.1566,  0.0381,  0.0312, -0.0569,\n",
            "          0.2132,  0.0608, -0.0366,  0.0021,  0.1066, -0.0346,  0.1672, -0.1186,\n",
            "         -0.0847,  0.0928,  0.1185,  0.0768,  0.1195, -0.0639,  0.0353,  0.1069,\n",
            "          0.1389, -0.0153, -0.0692,  0.0709,  0.0741, -0.1683,  0.0480, -0.0464,\n",
            "         -0.0370,  0.0188, -0.1307,  0.1536,  0.0759,  0.0431, -0.0746,  0.0114],\n",
            "        [ 0.0944, -0.0305,  0.0069,  0.0315, -0.0323, -0.0446,  0.0085, -0.0298,\n",
            "          0.0321, -0.0460,  0.0743,  0.0867,  0.1007, -0.0013,  0.0410,  0.0792,\n",
            "          0.1852, -0.0623,  0.0429,  0.0418, -0.2428,  0.0353,  0.1453,  0.0269,\n",
            "         -0.0376, -0.0582,  0.0488,  0.0891,  0.0338, -0.0926, -0.0152,  0.0340,\n",
            "         -0.0671, -0.0373,  0.0422, -0.0170,  0.0077, -0.0042, -0.0025, -0.0782,\n",
            "          0.0661, -0.0869,  0.1204,  0.0020, -0.1160,  0.1008,  0.0594,  0.0223],\n",
            "        [ 0.1762, -0.1274,  0.0012,  0.0317,  0.0689, -0.1631, -0.1610,  0.0054,\n",
            "          0.0597,  0.0477,  0.1492,  0.0441, -0.0212, -0.0895, -0.1743,  0.1445,\n",
            "          0.2080, -0.0110, -0.1386, -0.0730, -0.0569,  0.1010,  0.0466, -0.0944,\n",
            "          0.0067,  0.0618,  0.0707, -0.0248,  0.0213,  0.0190, -0.0038, -0.1744,\n",
            "          0.0536,  0.0365,  0.0273,  0.0252, -0.2003,  0.1267, -0.0643,  0.0828,\n",
            "          0.1240, -0.2184,  0.0186, -0.0338,  0.0234, -0.0260, -0.0382,  0.0685]]) amount of noise :)\n",
            "the grinch added tensor([[[[-0.0493,  0.0609,  0.1227],\n",
            "          [-0.1152,  0.0444, -0.0586],\n",
            "          [ 0.0122, -0.0601,  0.1265]],\n",
            "\n",
            "         [[ 0.2521, -0.0787, -0.1425],\n",
            "          [-0.0110,  0.1010, -0.0212],\n",
            "          [-0.0450, -0.0592,  0.0378]],\n",
            "\n",
            "         [[ 0.0025, -0.0212, -0.0060],\n",
            "          [-0.0147, -0.0221,  0.1147],\n",
            "          [-0.0532, -0.0282, -0.0087]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1236, -0.1047, -0.0773],\n",
            "          [ 0.1383,  0.1379, -0.0381],\n",
            "          [ 0.1426, -0.1206, -0.1450]],\n",
            "\n",
            "         [[ 0.0402, -0.1024,  0.0205],\n",
            "          [-0.1328, -0.0874,  0.0263],\n",
            "          [ 0.0435,  0.1006,  0.0674]],\n",
            "\n",
            "         [[ 0.1280,  0.0172,  0.1450],\n",
            "          [-0.0222, -0.0129, -0.1778],\n",
            "          [ 0.1085, -0.0035,  0.0501]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1711,  0.0175, -0.1763],\n",
            "          [-0.1236,  0.2389, -0.0657],\n",
            "          [-0.1821,  0.0980, -0.0840]],\n",
            "\n",
            "         [[-0.0246,  0.0259, -0.0760],\n",
            "          [-0.0602, -0.1761, -0.1023],\n",
            "          [-0.0602,  0.1020, -0.0428]],\n",
            "\n",
            "         [[-0.1223, -0.1255,  0.0497],\n",
            "          [ 0.0389,  0.1021,  0.0704],\n",
            "          [ 0.0225,  0.0760, -0.0682]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0017,  0.0416, -0.0152],\n",
            "          [-0.1717,  0.0245,  0.1015],\n",
            "          [-0.0105, -0.0923, -0.0962]],\n",
            "\n",
            "         [[ 0.1604,  0.0350,  0.1316],\n",
            "          [-0.0937, -0.0371, -0.1660],\n",
            "          [-0.0201, -0.0436,  0.2327]],\n",
            "\n",
            "         [[ 0.0373, -0.0401,  0.1637],\n",
            "          [ 0.2221,  0.0370,  0.0055],\n",
            "          [-0.2028,  0.2240, -0.0433]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0103,  0.1550, -0.0461],\n",
            "          [ 0.0777,  0.2666, -0.0465],\n",
            "          [-0.0617,  0.0512,  0.2596]],\n",
            "\n",
            "         [[ 0.0013, -0.0288,  0.0448],\n",
            "          [-0.0450, -0.0687,  0.0921],\n",
            "          [ 0.0204, -0.0138, -0.0492]],\n",
            "\n",
            "         [[ 0.0803, -0.0531, -0.0089],\n",
            "          [ 0.0672,  0.0161,  0.0500],\n",
            "          [-0.0881, -0.1970,  0.1974]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1905,  0.2003, -0.0454],\n",
            "          [-0.1130,  0.1265,  0.0180],\n",
            "          [ 0.0231,  0.0317,  0.1555]],\n",
            "\n",
            "         [[ 0.0467, -0.1796, -0.0815],\n",
            "          [-0.2403, -0.1186, -0.0328],\n",
            "          [-0.0103, -0.0277,  0.0309]],\n",
            "\n",
            "         [[-0.0419,  0.1493, -0.0316],\n",
            "          [-0.0156,  0.0317, -0.1292],\n",
            "          [-0.1070,  0.0152, -0.0969]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0887,  0.0880,  0.0972],\n",
            "          [-0.0123,  0.0752, -0.0068],\n",
            "          [-0.0757,  0.1691, -0.0104]],\n",
            "\n",
            "         [[-0.0842,  0.0540,  0.0853],\n",
            "          [-0.0394, -0.0710,  0.1399],\n",
            "          [-0.0665,  0.0180,  0.1255]],\n",
            "\n",
            "         [[-0.0232, -0.0601, -0.0311],\n",
            "          [ 0.0437,  0.0106,  0.1065],\n",
            "          [ 0.0412,  0.0580, -0.0568]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0415,  0.0598,  0.0168],\n",
            "          [-0.0703, -0.1370, -0.1259],\n",
            "          [-0.0603,  0.0011, -0.1651]],\n",
            "\n",
            "         [[-0.0731, -0.0360, -0.1719],\n",
            "          [ 0.0070,  0.0465,  0.0154],\n",
            "          [-0.1080, -0.0232, -0.0181]],\n",
            "\n",
            "         [[ 0.0700,  0.0214,  0.2051],\n",
            "          [-0.0737,  0.0075,  0.1365],\n",
            "          [ 0.0022,  0.0209,  0.1080]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0149, -0.2067, -0.0555],\n",
            "          [ 0.0110, -0.1595,  0.0384],\n",
            "          [-0.0220, -0.0079, -0.0477]],\n",
            "\n",
            "         [[-0.2252, -0.0291,  0.1736],\n",
            "          [ 0.0265, -0.0509, -0.0850],\n",
            "          [ 0.0069,  0.1018, -0.1379]],\n",
            "\n",
            "         [[-0.1365,  0.1392, -0.0510],\n",
            "          [ 0.0951,  0.1421,  0.0172],\n",
            "          [ 0.0434, -0.0574,  0.0454]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1255,  0.1135, -0.0248],\n",
            "          [-0.2109, -0.0556, -0.1336],\n",
            "          [ 0.1323, -0.0452, -0.1012]],\n",
            "\n",
            "         [[-0.0326,  0.0225, -0.0594],\n",
            "          [ 0.1567, -0.0789,  0.1187],\n",
            "          [-0.3365, -0.0561,  0.1815]],\n",
            "\n",
            "         [[ 0.1732, -0.0085, -0.1008],\n",
            "          [ 0.0442, -0.0271, -0.0623],\n",
            "          [-0.0694,  0.1353,  0.0062]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1679, -0.1144,  0.0520],\n",
            "          [ 0.1140, -0.0181, -0.0774],\n",
            "          [ 0.0217,  0.1282,  0.0600]],\n",
            "\n",
            "         [[-0.1467, -0.1259,  0.0272],\n",
            "          [-0.0660,  0.0178,  0.0878],\n",
            "          [ 0.1416, -0.1739, -0.0630]],\n",
            "\n",
            "         [[-0.0926, -0.0207,  0.2943],\n",
            "          [-0.1085,  0.1034, -0.0361],\n",
            "          [ 0.1850, -0.0284,  0.0881]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0884, -0.0137, -0.0662],\n",
            "          [ 0.0338,  0.1189, -0.0058],\n",
            "          [ 0.0248,  0.1977,  0.0869]],\n",
            "\n",
            "         [[-0.2910, -0.0237, -0.0682],\n",
            "          [ 0.0683,  0.1716, -0.0862],\n",
            "          [ 0.1637, -0.0733,  0.0383]],\n",
            "\n",
            "         [[ 0.0809,  0.0737, -0.1567],\n",
            "          [ 0.1543,  0.0130,  0.0396],\n",
            "          [ 0.0394, -0.1337, -0.0150]]]]) amount of noise :)\n",
            "the grinch added tensor([ 0.0442,  0.0652, -0.1272,  0.0290,  0.0137, -0.1647,  0.0635, -0.0523,\n",
            "         0.0998, -0.0484, -0.0788, -0.0055,  0.1157,  0.1734,  0.1454,  0.1135,\n",
            "         0.1433,  0.0534,  0.2030,  0.1914,  0.1174, -0.1762,  0.0418,  0.0907,\n",
            "         0.0407,  0.0822, -0.0330, -0.0273, -0.0246, -0.0484, -0.1461, -0.0475,\n",
            "         0.1698, -0.1124,  0.1434,  0.0027, -0.1927, -0.0793, -0.0876,  0.0723,\n",
            "        -0.0970,  0.0736,  0.1571,  0.0921,  0.0120, -0.2456,  0.0405,  0.0854]) amount of noise :)\n",
            "the grinch added tensor([[-1.3946e-01,  1.5126e-01, -8.8398e-02, -9.9749e-02,  9.0212e-02,\n",
            "          1.5102e-01,  1.0788e-01, -1.2675e-01,  4.5692e-02, -1.1523e-01,\n",
            "         -3.3369e-02, -2.4261e-02,  8.6518e-02, -6.3256e-02,  8.2121e-02,\n",
            "          8.6068e-02, -1.2725e-01, -7.2735e-02,  9.1586e-02,  1.9626e-02,\n",
            "          7.0240e-02,  4.3747e-02,  3.3248e-02,  6.2010e-02,  8.7831e-03,\n",
            "         -1.3933e-02,  7.9953e-02, -4.7720e-02,  1.0882e-01,  1.7819e-01,\n",
            "         -4.5802e-03, -1.1444e-01, -1.3199e-01,  7.5767e-02,  1.7728e-01,\n",
            "          3.5873e-02,  4.6086e-02,  1.3928e-01, -4.0453e-02,  1.1781e-01,\n",
            "          1.4264e-02, -1.4251e-02,  2.0747e-02,  5.4415e-02,  9.8611e-03,\n",
            "          1.9987e-02,  9.6659e-02, -3.3779e-02],\n",
            "        [ 2.0974e-03,  8.2179e-02, -2.9854e-01, -5.1263e-03, -2.2383e-03,\n",
            "         -6.7155e-02,  2.7692e-02,  6.7858e-02, -7.8542e-02,  1.7669e-01,\n",
            "          1.6279e-01, -8.8557e-02, -1.3227e-01, -3.0303e-02,  5.2473e-02,\n",
            "         -1.5887e-01, -5.8547e-02,  1.2555e-01, -3.1689e-02,  3.1840e-02,\n",
            "          9.5828e-02, -9.4027e-02,  1.1769e-01,  6.4955e-02,  4.0671e-02,\n",
            "         -1.4254e-01,  2.1330e-01,  1.5799e-02,  1.5211e-01, -5.5223e-02,\n",
            "         -9.0978e-02,  1.2812e-01, -9.9967e-02, -1.1043e-01, -1.1229e-01,\n",
            "         -7.9725e-03, -5.8713e-02,  2.1930e-01,  1.7667e-02, -8.7055e-02,\n",
            "         -8.6047e-02,  5.0617e-02,  1.0014e-01,  1.7119e-01, -1.4696e-02,\n",
            "         -3.4310e-02, -2.2097e-01,  3.3289e-03],\n",
            "        [ 5.0222e-02, -2.9372e-01, -5.2396e-02, -1.7260e-01,  1.3595e-01,\n",
            "          4.2057e-02, -4.1941e-04, -9.3991e-02, -1.0246e-02,  6.3724e-02,\n",
            "         -9.1417e-03,  7.3867e-02, -1.2519e-01, -5.5270e-02, -3.2554e-02,\n",
            "         -1.9888e-01,  4.2803e-02, -1.5790e-02,  1.7323e-01,  1.0455e-01,\n",
            "          5.4647e-02, -1.9670e-03,  6.4476e-02,  1.5061e-01, -6.6589e-02,\n",
            "          1.3067e-01, -1.6629e-01, -1.9799e-01, -1.0440e-02, -4.0249e-02,\n",
            "          7.6773e-02,  1.4909e-01,  9.3632e-02,  5.2467e-02, -1.4646e-01,\n",
            "          2.5333e-02,  3.4793e-02, -1.3885e-01,  2.2791e-02,  1.3848e-01,\n",
            "         -1.6881e-01, -5.3487e-02,  1.6220e-01, -1.1466e-01,  1.0300e-01,\n",
            "          1.7144e-01,  1.2286e-01,  3.4041e-02],\n",
            "        [-8.2387e-02,  1.2669e-03,  5.2727e-02,  1.1166e-02,  2.9076e-02,\n",
            "         -1.4852e-01,  1.8249e-01,  9.7752e-03,  4.8855e-02, -9.2829e-02,\n",
            "          8.6285e-03,  5.7348e-02, -9.9923e-02,  8.9631e-02, -5.4049e-02,\n",
            "          2.2350e-05,  8.4140e-02, -2.8495e-02, -1.0729e-01, -5.7312e-02,\n",
            "         -2.0021e-01,  6.2205e-02,  7.8926e-02, -2.6861e-02, -1.3972e-01,\n",
            "          1.0774e-01, -4.1470e-02, -9.9520e-02,  3.6859e-02, -1.0246e-01,\n",
            "         -1.2741e-01, -1.6173e-01, -5.0319e-02,  1.2878e-02,  1.1396e-01,\n",
            "          5.0343e-02,  7.5997e-02,  1.2741e-01, -9.7764e-02,  7.1942e-02,\n",
            "          1.9409e-01,  7.7470e-02, -2.8422e-03, -9.1483e-02, -2.1865e-02,\n",
            "          1.6476e-02, -2.7928e-03,  1.1125e-02],\n",
            "        [-1.2935e-01,  9.6240e-02, -2.1994e-02, -1.3560e-02,  5.1239e-02,\n",
            "         -5.7530e-02,  1.7911e-01,  1.6157e-02,  1.5233e-01,  3.6409e-02,\n",
            "         -1.8110e-01, -2.7281e-01,  2.9300e-02,  5.6287e-02, -2.4754e-02,\n",
            "         -5.7228e-02,  6.9982e-02, -1.1528e-01, -3.1476e-04,  8.3787e-02,\n",
            "          1.2120e-02, -3.9624e-02,  1.4111e-01,  1.1863e-01, -1.1354e-01,\n",
            "          1.0852e-01,  1.6383e-02, -2.4445e-01,  5.7467e-02,  1.3468e-01,\n",
            "         -4.3810e-02,  5.0031e-02, -5.0715e-02, -1.5773e-01, -2.5389e-02,\n",
            "         -8.6721e-02, -7.1478e-02,  7.9980e-02, -8.9425e-02, -2.8055e-02,\n",
            "         -2.1756e-02, -1.3714e-01, -8.5488e-03, -1.5318e-02,  8.8892e-02,\n",
            "          3.1955e-02,  1.9229e-02, -4.7116e-03]]) amount of noise :)\n",
            "the grinch added tensor([[-5.4278e-02, -2.8690e-02, -2.1700e-03, -8.8533e-02, -1.1070e-01,\n",
            "          1.1756e-01,  7.7418e-03, -7.8471e-03, -4.0381e-02, -5.5549e-03,\n",
            "          1.3225e-01,  1.3941e-01, -2.3786e-02, -1.2745e-02,  4.6634e-02,\n",
            "          8.6127e-02,  8.6531e-02,  9.6793e-02, -3.8951e-02, -8.8433e-02,\n",
            "         -6.7865e-02, -2.0954e-01,  3.7799e-02, -4.6794e-02, -4.8352e-02,\n",
            "         -5.0124e-02,  1.0053e-01, -1.2736e-01,  2.0872e-02,  6.5584e-02,\n",
            "         -2.7682e-02, -1.4407e-01, -1.6264e-03,  6.0447e-02, -2.7373e-02,\n",
            "          4.0303e-02, -1.0159e-01, -1.3505e-01,  1.0768e-03,  5.8414e-02,\n",
            "         -1.5950e-01, -7.5173e-02,  7.2753e-02, -9.4902e-02,  1.2743e-01,\n",
            "         -1.6115e-02,  3.1980e-02, -1.6844e-01],\n",
            "        [-8.3784e-03,  1.4046e-01, -6.6366e-02, -2.7087e-02,  2.4714e-01,\n",
            "         -4.6199e-02,  6.5272e-02,  8.4825e-02,  8.4785e-02,  1.9896e-02,\n",
            "          1.6505e-01,  8.2605e-02,  9.7257e-02, -2.6036e-02,  5.7452e-02,\n",
            "         -5.5671e-02, -1.5022e-02, -1.2765e-01, -3.9727e-02, -1.1132e-01,\n",
            "          1.3650e-02, -5.9378e-02, -5.3310e-02,  6.0187e-02, -6.3647e-02,\n",
            "          2.7433e-02, -1.2388e-01, -1.5180e-02,  1.0892e-01,  2.4802e-02,\n",
            "         -2.0169e-01,  1.0481e-01, -7.4098e-02,  1.5810e-01, -1.0712e-01,\n",
            "          5.3851e-02,  8.6851e-02,  4.3430e-03,  9.1509e-02, -4.0757e-02,\n",
            "         -9.9667e-02,  3.9168e-02, -8.3423e-02,  3.3122e-02,  1.2208e-03,\n",
            "          1.0410e-01,  4.7869e-02, -2.5279e-01],\n",
            "        [-1.2879e-01, -1.7578e-01,  1.9968e-01,  9.9508e-02, -8.3079e-02,\n",
            "          1.0954e-01, -6.6377e-02, -1.1905e-01, -1.7265e-02,  1.0770e-01,\n",
            "         -8.1772e-02, -6.2784e-02,  3.6407e-02, -1.7954e-01,  1.4026e-01,\n",
            "         -7.7086e-03, -1.7702e-01, -6.7643e-02, -9.9971e-02,  1.0255e-03,\n",
            "         -1.5229e-01, -1.0937e-01,  1.3739e-01, -8.2085e-02, -1.8297e-01,\n",
            "         -1.4415e-01, -6.1348e-02, -1.1373e-01, -1.0091e-02,  3.8565e-02,\n",
            "         -6.5851e-02,  3.7023e-02,  1.4626e-01, -5.8572e-02, -3.7598e-02,\n",
            "          1.8143e-02,  1.0739e-01,  1.2961e-01, -1.7658e-02,  1.5314e-01,\n",
            "          6.7417e-02,  5.6852e-03,  1.0483e-03,  1.1902e-03,  1.5989e-01,\n",
            "         -9.9339e-02, -6.4565e-02,  5.1825e-02],\n",
            "        [ 1.2377e-01, -1.3309e-03, -1.8759e-02,  1.1434e-02, -2.8238e-02,\n",
            "         -5.4223e-02,  9.5292e-03, -9.4564e-02,  1.0415e-01,  1.7334e-03,\n",
            "         -5.9435e-02, -1.3537e-01,  1.4608e-01,  1.1772e-02, -1.4781e-01,\n",
            "         -2.3014e-02, -3.1785e-02, -4.6498e-02, -3.7599e-02,  1.4791e-01,\n",
            "         -2.4644e-02,  1.8166e-01, -9.5127e-02, -9.9041e-02,  2.9285e-02,\n",
            "          1.7672e-01,  6.2191e-02,  2.8816e-02,  2.2658e-02,  2.5013e-02,\n",
            "         -1.3514e-02,  4.2555e-02,  6.2840e-02,  4.9056e-02, -1.9887e-01,\n",
            "          7.3020e-03, -1.1916e-01, -1.0084e-01, -4.2252e-02,  4.3571e-03,\n",
            "          1.9243e-01, -1.9862e-02,  2.0240e-02, -1.4744e-02, -1.5672e-01,\n",
            "         -2.6590e-02,  1.7514e-01, -3.9695e-02],\n",
            "        [-7.0900e-02, -1.6145e-02, -5.6311e-02, -1.0227e-01,  1.8778e-01,\n",
            "          1.1242e-01, -1.6418e-01,  1.0199e-01,  3.0808e-02, -1.2356e-02,\n",
            "          5.8600e-02,  1.3796e-01,  2.4236e-02,  1.7325e-02,  4.1749e-02,\n",
            "         -2.1056e-02, -1.1996e-04,  3.3136e-02, -3.4283e-02,  1.3473e-02,\n",
            "         -3.0809e-02,  2.2147e-02,  2.9242e-02,  4.2575e-02,  2.1148e-02,\n",
            "          1.1638e-02,  9.3419e-02, -1.7035e-03,  3.8342e-02, -2.5143e-02,\n",
            "          4.5855e-02, -1.2646e-01,  8.4246e-02, -3.4966e-02,  2.9308e-01,\n",
            "          1.5260e-02,  4.4127e-02, -8.1203e-03, -8.2866e-02,  1.5318e-02,\n",
            "          5.1345e-02, -1.3153e-01,  2.7845e-03, -3.5764e-03,  8.4989e-02,\n",
            "          1.3133e-01,  1.7562e-03, -8.6188e-02]]) amount of noise :)\n",
            "the grinch added tensor([[[[ 0.0312,  0.0470,  0.0399],\n",
            "          [-0.0253, -0.0540,  0.1031],\n",
            "          [ 0.0710, -0.0499,  0.0586]],\n",
            "\n",
            "         [[ 0.1905, -0.0135, -0.0330],\n",
            "          [-0.0369, -0.1719, -0.0236],\n",
            "          [-0.0476,  0.0622,  0.1387]],\n",
            "\n",
            "         [[-0.1927, -0.0846,  0.0499],\n",
            "          [ 0.1702, -0.0161, -0.0070],\n",
            "          [ 0.0215,  0.0617, -0.1304]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1258,  0.0481, -0.1583],\n",
            "          [-0.1282,  0.1554, -0.0414],\n",
            "          [-0.1309,  0.1827, -0.1659]],\n",
            "\n",
            "         [[-0.0141,  0.0922,  0.1371],\n",
            "          [-0.1156,  0.0618,  0.1881],\n",
            "          [-0.0952,  0.1317, -0.0715]],\n",
            "\n",
            "         [[-0.0573,  0.0473, -0.1045],\n",
            "          [-0.0432,  0.0672,  0.0408],\n",
            "          [ 0.0146,  0.1090, -0.1934]]],\n",
            "\n",
            "\n",
            "        [[[-0.1464,  0.0242,  0.0951],\n",
            "          [-0.0273, -0.0446, -0.0685],\n",
            "          [-0.1255,  0.0094, -0.0113]],\n",
            "\n",
            "         [[-0.0354, -0.0429,  0.0466],\n",
            "          [-0.0762, -0.1426, -0.0200],\n",
            "          [ 0.0264, -0.0442, -0.0599]],\n",
            "\n",
            "         [[ 0.0408, -0.1085, -0.0146],\n",
            "          [-0.0364,  0.2073,  0.0030],\n",
            "          [ 0.0195, -0.1505, -0.0373]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0736, -0.0918,  0.0455],\n",
            "          [ 0.0889,  0.1210,  0.0851],\n",
            "          [ 0.0459,  0.0652, -0.0077]],\n",
            "\n",
            "         [[ 0.0615,  0.1199, -0.0128],\n",
            "          [-0.1989,  0.0815,  0.0648],\n",
            "          [ 0.1126,  0.1476,  0.0406]],\n",
            "\n",
            "         [[-0.1549, -0.0407,  0.0999],\n",
            "          [ 0.1199,  0.0858,  0.1277],\n",
            "          [ 0.1510, -0.0841,  0.1709]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0560,  0.0168,  0.0065],\n",
            "          [ 0.0149,  0.0342, -0.0757],\n",
            "          [ 0.2381, -0.1680, -0.1121]],\n",
            "\n",
            "         [[ 0.0994, -0.0034, -0.1196],\n",
            "          [ 0.1158,  0.0125, -0.0367],\n",
            "          [-0.1031,  0.1081, -0.2111]],\n",
            "\n",
            "         [[ 0.1389,  0.0708, -0.0101],\n",
            "          [ 0.1200, -0.0884, -0.0357],\n",
            "          [ 0.0925, -0.0235,  0.0237]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1612,  0.1741, -0.0730],\n",
            "          [-0.0577,  0.0610, -0.0277],\n",
            "          [-0.1436, -0.0402,  0.0075]],\n",
            "\n",
            "         [[ 0.0636, -0.1218,  0.0173],\n",
            "          [-0.0654,  0.1450, -0.0696],\n",
            "          [-0.0239, -0.0359, -0.2339]],\n",
            "\n",
            "         [[-0.0255, -0.0448, -0.0827],\n",
            "          [-0.1332, -0.0078, -0.1340],\n",
            "          [-0.0041, -0.0114, -0.1652]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0082,  0.0975,  0.0049],\n",
            "          [ 0.0038,  0.0660, -0.1282],\n",
            "          [-0.1527,  0.1021, -0.1110]],\n",
            "\n",
            "         [[-0.0422,  0.0329,  0.1605],\n",
            "          [-0.0165,  0.1290,  0.1002],\n",
            "          [ 0.1376,  0.0639, -0.1002]],\n",
            "\n",
            "         [[-0.0038,  0.0973,  0.0874],\n",
            "          [ 0.0242,  0.1639,  0.0865],\n",
            "          [ 0.0475, -0.0474,  0.0451]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0162,  0.0859,  0.0729],\n",
            "          [-0.1311, -0.1503, -0.2428],\n",
            "          [ 0.0961, -0.0766, -0.0011]],\n",
            "\n",
            "         [[ 0.0265, -0.0818, -0.1275],\n",
            "          [ 0.1127, -0.0557, -0.0740],\n",
            "          [ 0.2274, -0.0081,  0.0934]],\n",
            "\n",
            "         [[-0.0167,  0.0700, -0.0164],\n",
            "          [-0.0357,  0.0798, -0.0894],\n",
            "          [-0.0725, -0.0286, -0.1036]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0170, -0.0110,  0.0825],\n",
            "          [ 0.0537, -0.0274,  0.0207],\n",
            "          [-0.1282,  0.0067, -0.0808]],\n",
            "\n",
            "         [[-0.0279, -0.1495,  0.0340],\n",
            "          [ 0.0634,  0.1493, -0.0313],\n",
            "          [ 0.0781,  0.0126,  0.0735]],\n",
            "\n",
            "         [[ 0.0095, -0.0954,  0.0346],\n",
            "          [-0.0117,  0.2125,  0.1361],\n",
            "          [ 0.1243,  0.0343, -0.0504]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0814, -0.0331,  0.0151],\n",
            "          [-0.0285, -0.2328,  0.0657],\n",
            "          [-0.0377,  0.0924,  0.0744]],\n",
            "\n",
            "         [[-0.0863,  0.0059, -0.0502],\n",
            "          [-0.0745, -0.0502, -0.0098],\n",
            "          [ 0.2438,  0.0923, -0.0479]],\n",
            "\n",
            "         [[-0.1134,  0.0129,  0.2229],\n",
            "          [-0.1079,  0.0488,  0.0630],\n",
            "          [ 0.0944, -0.0529, -0.2590]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1622, -0.0883, -0.0623],\n",
            "          [-0.0207, -0.0217,  0.0562],\n",
            "          [ 0.0025, -0.0060, -0.0753]],\n",
            "\n",
            "         [[-0.0436, -0.0245, -0.0374],\n",
            "          [-0.1704,  0.0160,  0.0216],\n",
            "          [ 0.1191, -0.0894,  0.0330]],\n",
            "\n",
            "         [[ 0.1109, -0.1923,  0.0011],\n",
            "          [-0.0549,  0.0901,  0.0230],\n",
            "          [-0.1277, -0.0657,  0.0993]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1021, -0.1416,  0.0604],\n",
            "          [-0.0626,  0.1172, -0.0181],\n",
            "          [ 0.0768,  0.2035, -0.0137]],\n",
            "\n",
            "         [[-0.1242, -0.0907, -0.0249],\n",
            "          [ 0.0495, -0.0393,  0.0772],\n",
            "          [ 0.0311, -0.0096, -0.1908]],\n",
            "\n",
            "         [[ 0.1342, -0.0520,  0.1278],\n",
            "          [ 0.0056, -0.0989, -0.0226],\n",
            "          [ 0.1120, -0.0729,  0.0104]]]]) amount of noise :)\n",
            "the grinch added tensor([-0.0230,  0.0779, -0.0728, -0.1575,  0.0377,  0.1477, -0.0130, -0.1472,\n",
            "         0.1338,  0.0271, -0.1330, -0.1085, -0.0607,  0.0192, -0.1846,  0.0128,\n",
            "         0.1493, -0.0084,  0.0958, -0.2862,  0.0919, -0.0930, -0.1144, -0.1090,\n",
            "         0.0400, -0.1388, -0.0060,  0.1021,  0.1818,  0.0521, -0.1327, -0.0407,\n",
            "        -0.1442,  0.0456, -0.0148,  0.1045, -0.0353,  0.0522,  0.0150,  0.0081,\n",
            "         0.0655, -0.0656, -0.0189, -0.0689, -0.0448, -0.0313,  0.2265,  0.1444]) amount of noise :)\n",
            "the grinch added tensor([[-1.6676e-01, -6.8122e-02, -1.5448e-01, -4.8294e-02, -6.2209e-02,\n",
            "         -1.3401e-01,  6.0796e-02,  1.7924e-02,  1.9620e-02,  5.7455e-03,\n",
            "          7.2182e-03,  3.2587e-02, -1.1765e-01, -6.5718e-02, -1.2250e-01,\n",
            "          1.4554e-01,  1.2280e-01, -1.0198e-03, -2.0294e-02,  2.5968e-02,\n",
            "         -8.7004e-02,  4.1195e-02, -1.9929e-01,  1.1917e-01, -2.9977e-02,\n",
            "         -2.1667e-02, -1.0552e-01, -1.7941e-01,  2.6653e-02,  1.8077e-01,\n",
            "          4.5266e-02,  2.7573e-02, -1.4384e-01, -7.7335e-02, -1.1957e-01,\n",
            "          1.0708e-02, -1.6420e-01,  1.7669e-01,  2.1471e-02, -3.8628e-02,\n",
            "          1.1641e-01,  1.3635e-02, -6.0192e-02, -5.8845e-02, -2.2488e-02,\n",
            "         -4.3639e-02, -8.5897e-03, -3.2653e-02],\n",
            "        [ 6.0747e-02,  7.8775e-02, -1.0250e-02, -1.4834e-01,  1.5049e-01,\n",
            "         -1.3302e-01,  9.0443e-03,  8.3207e-02, -2.0420e-01,  6.0599e-02,\n",
            "          2.0253e-01,  1.2897e-02, -9.1797e-02,  1.9103e-01,  4.7926e-02,\n",
            "         -1.2158e-01,  4.1743e-02,  8.6391e-03, -3.7571e-02,  1.1822e-01,\n",
            "         -9.7076e-03,  1.0071e-02,  3.8090e-02, -2.0751e-01,  8.7833e-02,\n",
            "          2.2787e-01,  8.8902e-03, -7.4973e-02, -8.3872e-02, -7.7988e-02,\n",
            "          8.9518e-02,  5.2016e-02, -8.8672e-02, -6.0795e-02,  1.1528e-01,\n",
            "         -1.4195e-01,  1.4113e-01, -5.8990e-02, -1.0622e-02, -5.4771e-02,\n",
            "          2.1046e-02,  2.8625e-02, -6.7889e-03, -2.8356e-02, -3.1759e-02,\n",
            "         -8.6176e-03, -2.7913e-02,  9.1778e-02],\n",
            "        [ 3.4195e-02, -7.5191e-03, -1.1936e-01,  1.0231e-01, -7.2978e-02,\n",
            "         -1.3837e-01,  8.3943e-02, -1.8400e-01,  7.4361e-02, -1.6665e-01,\n",
            "         -8.1582e-02,  5.6969e-02,  1.2113e-01,  8.1432e-02, -5.6319e-02,\n",
            "         -6.8427e-02, -4.5457e-02, -2.6561e-02, -2.4185e-02,  9.0681e-02,\n",
            "          4.1267e-02,  4.6619e-02,  4.0551e-02, -1.0712e-01,  7.7837e-02,\n",
            "         -5.4766e-02,  6.6408e-02,  1.3818e-02, -4.1896e-02,  5.3650e-02,\n",
            "          1.2142e-01,  4.0927e-01, -1.1022e-01, -2.8661e-02, -2.4214e-02,\n",
            "         -6.0368e-03, -8.3143e-02, -1.1399e-01,  1.2961e-01,  6.6867e-02,\n",
            "          1.1550e-01, -1.2757e-02,  8.7026e-02,  5.3523e-02,  1.0672e-01,\n",
            "          6.3399e-03, -1.7449e-01,  1.3467e-01],\n",
            "        [-4.8309e-02,  3.1084e-02,  6.8441e-02, -2.1214e-01, -1.0403e-02,\n",
            "         -4.0193e-02, -2.7823e-02, -2.5295e-02, -2.2619e-01,  3.6867e-02,\n",
            "          9.1329e-03, -2.4436e-02,  5.9224e-02, -5.2335e-03,  1.5856e-02,\n",
            "         -1.2310e-01,  2.1800e-01, -1.1949e-01,  8.4661e-02,  4.2279e-02,\n",
            "          5.5487e-02,  7.7000e-02,  2.7333e-01, -1.0219e-02,  1.7564e-01,\n",
            "         -9.7771e-02, -1.8168e-02, -2.0351e-01,  3.8257e-02, -1.0850e-01,\n",
            "         -2.3275e-01,  1.0276e-01, -3.0346e-02, -1.2627e-04, -8.8081e-02,\n",
            "          3.2235e-02, -5.6438e-02, -2.1136e-02, -1.4990e-01, -8.2250e-02,\n",
            "         -4.0781e-02,  1.3537e-02,  4.7617e-02,  1.9992e-02, -5.9922e-02,\n",
            "          5.4292e-02,  9.7232e-02, -2.5989e-02],\n",
            "        [ 1.0356e-01,  7.7542e-03, -1.4539e-01, -2.2520e-02, -4.8068e-02,\n",
            "         -5.2509e-02, -1.4126e-02,  4.1104e-02, -2.4361e-01, -8.8464e-02,\n",
            "         -3.7978e-02,  7.0756e-02,  3.2676e-02, -1.3674e-02, -7.0723e-02,\n",
            "          2.5390e-02,  1.7807e-01,  7.1580e-02, -1.8015e-02, -1.1784e-01,\n",
            "         -7.5902e-03,  4.4563e-02,  1.1185e-01, -4.3499e-02, -1.5053e-01,\n",
            "          1.8540e-02, -3.7661e-03,  1.0022e-01,  2.7087e-02,  1.9254e-02,\n",
            "          2.8445e-01,  2.3400e-01, -1.4524e-01, -1.7654e-01, -1.2638e-01,\n",
            "          5.6561e-02, -1.3124e-02, -2.7938e-02, -3.8879e-02, -6.6442e-03,\n",
            "         -2.1109e-01, -6.4857e-02,  1.7423e-02, -3.0250e-02, -7.8314e-02,\n",
            "         -1.9136e-01, -1.4797e-01, -6.6672e-03]]) amount of noise :)\n",
            "the grinch added tensor([[-1.8039e-01, -6.4958e-02,  7.2929e-02, -1.0760e-01, -1.7166e-02,\n",
            "         -1.8113e-01, -3.9282e-02,  1.3931e-01, -1.3421e-01, -8.0182e-02,\n",
            "          2.6057e-02,  9.6917e-03, -1.6333e-02,  1.3408e-01,  3.5123e-02,\n",
            "          8.3200e-02, -4.9656e-02, -9.7068e-03, -5.5480e-03,  3.6555e-02,\n",
            "         -1.0021e-01, -1.2437e-01, -4.4383e-02,  6.3545e-02, -8.5795e-02,\n",
            "          9.0838e-02, -2.4073e-02,  3.8686e-02,  8.4167e-02, -1.8235e-01,\n",
            "         -1.9754e-02,  1.3631e-01,  1.3880e-02, -2.6271e-04, -3.3099e-02,\n",
            "         -1.8434e-01,  2.9031e-02, -7.4257e-02,  2.0049e-01, -4.8704e-02,\n",
            "          6.2744e-02,  2.5960e-02, -1.5664e-01,  3.9403e-02,  2.5254e-02,\n",
            "          6.9665e-02,  6.1949e-02, -6.6096e-02],\n",
            "        [ 8.4236e-02, -1.0695e-01,  1.5256e-01, -8.0113e-02,  5.4070e-02,\n",
            "          1.2736e-01,  4.7164e-02,  6.0894e-03, -6.3699e-02,  3.1832e-02,\n",
            "         -1.3095e-01,  3.2704e-02,  2.2993e-02, -1.4524e-02, -1.2371e-01,\n",
            "          4.4624e-02,  6.9176e-02,  1.2016e-01,  1.0051e-01, -6.2210e-02,\n",
            "          6.9070e-02,  7.3037e-02,  6.6885e-02, -3.4556e-02, -1.1255e-01,\n",
            "          4.5028e-02,  4.9458e-02,  1.1095e-01,  3.9906e-02,  6.6479e-02,\n",
            "         -1.3463e-01, -1.5098e-01,  1.0291e-02, -1.1364e-01, -6.6988e-02,\n",
            "          3.4021e-02, -8.2413e-02,  2.5588e-02,  2.4022e-01, -1.5496e-01,\n",
            "          6.7558e-02,  3.4443e-02,  3.0223e-02, -2.6517e-02, -1.5898e-01,\n",
            "          5.7915e-03, -4.2857e-02, -9.9857e-02],\n",
            "        [ 3.0671e-02,  1.7968e-02,  1.8095e-01,  1.3989e-01,  1.7129e-02,\n",
            "         -2.3613e-01, -3.9247e-02, -1.4221e-02,  1.7825e-01,  8.9883e-02,\n",
            "         -1.2656e-02, -7.5666e-02, -6.8727e-02, -1.9365e-01,  1.4659e-01,\n",
            "          4.4076e-02,  5.2761e-02,  1.8212e-01,  2.6068e-01, -1.7630e-01,\n",
            "         -1.5083e-01, -6.1350e-02, -4.2773e-02, -1.2265e-02, -9.8951e-02,\n",
            "         -9.7248e-02, -1.6332e-01, -3.5951e-02, -2.2163e-01,  1.8724e-01,\n",
            "         -1.5687e-01, -2.4943e-02, -5.1764e-02, -1.8856e-02,  6.1840e-02,\n",
            "         -1.3729e-02, -1.3916e-01, -3.9169e-02,  3.1246e-02, -1.7108e-01,\n",
            "         -2.2526e-02, -9.5939e-03, -9.3486e-02,  7.6781e-02,  1.6924e-01,\n",
            "         -6.9871e-02, -3.0830e-02, -6.2494e-02],\n",
            "        [ 2.6617e-02, -8.0008e-02, -3.4402e-02,  1.9440e-01,  8.3949e-02,\n",
            "         -1.4664e-02,  4.2095e-03,  8.5648e-02, -8.7694e-03,  7.0690e-02,\n",
            "          1.9454e-02,  6.0461e-02,  1.2674e-01, -3.6762e-02,  2.8005e-02,\n",
            "         -2.6744e-03,  7.3583e-02, -5.0148e-02,  1.0059e-02,  6.5324e-02,\n",
            "         -6.9307e-02, -1.3657e-02, -9.1761e-03, -8.6861e-02, -2.0634e-02,\n",
            "         -1.5198e-01,  1.2886e-01, -3.0072e-02,  2.1534e-01, -1.0544e-02,\n",
            "          3.0481e-02,  1.4428e-02, -1.5750e-01,  4.5537e-02,  3.5984e-02,\n",
            "         -1.3122e-02,  4.2533e-02,  8.6717e-02, -6.3748e-02, -5.9534e-02,\n",
            "          1.1938e-01,  2.5925e-02, -9.1655e-02, -5.8509e-02,  2.0910e-02,\n",
            "          6.1021e-02,  6.1648e-02, -1.0285e-01],\n",
            "        [-8.0466e-02,  1.5859e-01,  3.7891e-02,  6.9830e-02, -3.3696e-02,\n",
            "          1.9797e-02, -3.5294e-02,  4.2422e-02,  1.3977e-01,  7.9274e-02,\n",
            "         -2.8737e-02,  4.4194e-02, -1.6938e-01, -3.2886e-02, -9.9034e-02,\n",
            "         -5.2437e-02,  6.5768e-02,  1.1246e-02,  2.0031e-01, -1.6407e-01,\n",
            "         -8.1432e-02,  1.3127e-01,  4.7828e-02,  8.6272e-02,  3.9421e-02,\n",
            "          2.1196e-01,  1.5032e-01, -7.7455e-02,  2.1802e-02, -7.4162e-02,\n",
            "          6.8824e-02, -6.8445e-02, -1.6031e-01, -2.7455e-02,  1.6719e-01,\n",
            "          1.2408e-02,  1.3221e-01, -1.3040e-01, -7.3524e-02,  6.2537e-02,\n",
            "          1.1291e-01,  2.6272e-01, -1.4364e-01,  8.4158e-02,  8.1253e-02,\n",
            "          7.3668e-02, -1.7578e-02, -1.1303e-01]]) amount of noise :)\n",
            "the grinch added tensor([[ 0.0303, -0.0925, -0.0015,  ..., -0.0569,  0.0235,  0.0253],\n",
            "        [ 0.1974,  0.0651, -0.0251,  ..., -0.0375,  0.0203, -0.0660],\n",
            "        [ 0.0680,  0.0924,  0.1077,  ...,  0.0345,  0.2482, -0.1578],\n",
            "        [-0.0745,  0.2557,  0.0291,  ...,  0.0290,  0.1391, -0.0425],\n",
            "        [ 0.1786, -0.0913, -0.0463,  ...,  0.0560, -0.0967,  0.0061]]) amount of noise :)\n",
            "the grinch added tensor([-0.1620, -0.0204, -0.0372, -0.0067, -0.0494]) amount of noise :)\n",
            "the grinch added tensor([-0.1128, -0.0551, -0.0007, -0.1465, -0.2056,  0.0196]) amount of noise :)\n",
            "the grinch added tensor([ 0.1285,  0.0695,  0.0516,  0.0561,  0.1835, -0.1486]) amount of noise :)\n",
            "the grinch added tensor([-0.1368, -0.0624,  0.1788, -0.0214,  0.0567,  0.0062]) amount of noise :)\n",
            "the grinch added tensor([-0.1109, -0.1123,  0.0872,  0.1035, -0.1207, -0.0576]) amount of noise :)\n",
            "the grinch added tensor([ 0.0047, -0.1073,  0.0387, -0.0065,  0.0367, -0.0783]) amount of noise :)\n",
            "the grinch added tensor([ 0.1870,  0.0492,  0.0005,  0.1271,  0.1283, -0.0044]) amount of noise :)\n",
            "the grinch added tensor([-0.0310, -0.0013, -0.2095, -0.0240,  0.0079, -0.1213]) amount of noise :)\n",
            "the grinch added tensor([0.0836, 0.1477, 0.0764, 0.0128, 0.2125, 0.0487]) amount of noise :)\n",
            "the grinch added tensor([ 0.1204,  0.0665,  0.1093, -0.0914, -0.0770, -0.0128]) amount of noise :)\n",
            "the grinch added tensor([ 0.0273,  0.0262, -0.0679,  0.0199,  0.1326,  0.0989]) amount of noise :)\n",
            "attempting to find existing checkpoint\n",
            "Loading pre-split data\n",
            "Loading data into RAM\n",
            "Currently loading into memory the test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:53<00:00,  2.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently loading into memory the train set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 54/64 [02:26<00:23,  2.35s/it]"
          ]
        }
      ],
      "source": [
        "# Combines the arguments, model, data and experiment builders to run an experiment\n",
        "\n",
        "# python train_maml_system.py --name_of_args_json_file experiment_config/ --gpu_to_use 1\n",
        "# import sys\n",
        "# # Simulate command line arguments\n",
        "# sys.argv = [  # Replace with the current file name\n",
        "#            '--name_of_args_json_file', 'content/omniglot_maml++-omniglot_5_8_0.1_64_5_2.json',\n",
        "#            '--gpu_to_use', '1',  # Dataset directory\n",
        "#           #  '--experiment_name', 'omniglot_experiment',  # Experiment name\n",
        "#           #  '--architecture_name', 'maml', # You'll likely need to provide an appropriate architecture name\n",
        "#            # ... add other necessary arguments\n",
        "#            ]\n",
        "\n",
        "# args, device = get_args()\n",
        "args, device = load_args_from_json(\"/content/HowToTrainYourMAMLPytorch/experiment_config/mini-imagenet_maml++-mini-imagenet_5_2_0.01_48_5_2.json\")\n",
        "\n",
        "model = MAMLFewShotClassifier(args=args, device=device,\n",
        "                              im_shape=(2, args.image_channels,\n",
        "                                        args.image_height, args.image_width))\n",
        "# maybe_unzip_dataset(args=args)\n",
        "data = MetaLearningSystemDataLoader\n",
        "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
        "maml_system.run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZIfE7aOQ2sj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}