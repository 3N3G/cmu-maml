{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vBTJxs8IYd3",
        "outputId": "1e357f54-60da-478e-dedf-a961c54850b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HowToTrainYourMAMLPytorch'...\n",
            "remote: Enumerating objects: 36634, done.\u001b[K\n",
            "remote: Counting objects: 100% (261/261), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 36634 (delta 159), reused 210 (delta 129), pack-reused 36373 (from 1)\u001b[K\n",
            "Receiving objects: 100% (36634/36634), 18.95 MiB | 8.20 MiB/s, done.\n",
            "Resolving deltas: 100% (2204/2204), done.\n",
            "Updating files: 100% (32570/32570), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JqpuygWUiM_i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# from meta_neural_network_architectures import VGGReLUNormNetwork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KzOo4o8Tiwhg"
      },
      "outputs": [],
      "source": [
        "def set_torch_seed(seed):\n",
        "    \"\"\"\n",
        "    Sets the pytorch seeds for current experiment run\n",
        "    :param seed: The seed (int)\n",
        "    :return: A random number generator to use\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed=seed)\n",
        "    torch_seed = rng.randint(0, 999999)\n",
        "    torch.manual_seed(seed=torch_seed)\n",
        "\n",
        "    return rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq_IztyOad4I"
      },
      "source": [
        "# Get mini-imagenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFIFtdihZNL8",
        "outputId": "28499e7a-c095-46a4-babd-867edbdc68eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/zcyzhchyu/mini-imagenet\n",
            "License(s): CC0-1.0\n",
            "Downloading mini-imagenet.zip to /content\n",
            "100% 2.86G/2.86G [02:30<00:00, 20.8MB/s]\n",
            "100% 2.86G/2.86G [02:30<00:00, 20.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "!kaggle datasets download zcyzhchyu/mini-imagenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3CN6EZnvZbbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e6dc9e-d369-480f-8cb8-a9211d7c28ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/mini-imagenet.zip\n",
            "  inflating: /content/HowToTrainYourMAMLPytorch/images.tar  \n",
            "  inflating: /content/HowToTrainYourMAMLPytorch/test.csv  \n",
            "  inflating: /content/HowToTrainYourMAMLPytorch/train.csv  \n",
            "  inflating: /content/HowToTrainYourMAMLPytorch/val.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/mini-imagenet.zip -d /content/HowToTrainYourMAMLPytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_gJBmd0UhK3Y"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size\n",
        "!mv /content/HowToTrainYourMAMLPytorch/images.tar /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size\n",
        "!mv /content/HowToTrainYourMAMLPytorch/train.csv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size\n",
        "!mv /content/HowToTrainYourMAMLPytorch/val.csv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size\n",
        "!mv /content/HowToTrainYourMAMLPytorch/test.csv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3skC8Ev8ePob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fac52f-27b3-4d63-88e3-c952ee451dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "./n0679411000000879.jpg\n",
            "./n0679411000000880.jpg\n",
            "./n0679411000000881.jpg\n",
            "./n0679411000000882.jpg\n",
            "./n0679411000000885.jpg\n",
            "./n0679411000000894.jpg\n",
            "./n0679411000000895.jpg\n",
            "./n0679411000000897.jpg\n",
            "./n0679411000000898.jpg\n",
            "./n0679411000000899.jpg\n",
            "./n0679411000000900.jpg\n",
            "./n0679411000000901.jpg\n",
            "./n0679411000000902.jpg\n",
            "./n0679411000000904.jpg\n",
            "./n0679411000000905.jpg\n",
            "./n0679411000000909.jpg\n",
            "./n0679411000000912.jpg\n",
            "./n0679411000000913.jpg\n",
            "./n0679411000000916.jpg\n",
            "./n0679411000000918.jpg\n",
            "./n0679411000000919.jpg\n",
            "./n0679411000000923.jpg\n",
            "./n0679411000000926.jpg\n",
            "./n0679411000000929.jpg\n",
            "./n0679411000000931.jpg\n",
            "./n0679411000000932.jpg\n",
            "./n0679411000000933.jpg\n",
            "./n0679411000000935.jpg\n",
            "./n0679411000000936.jpg\n",
            "./n0679411000000938.jpg\n",
            "./n0679411000000939.jpg\n",
            "./n0679411000000940.jpg\n",
            "./n0679411000000941.jpg\n",
            "./n0679411000000942.jpg\n",
            "./n0679411000000944.jpg\n",
            "./n0679411000000948.jpg\n",
            "./n0679411000000949.jpg\n",
            "./n0679411000000950.jpg\n",
            "./n0679411000000955.jpg\n",
            "./n0679411000000963.jpg\n",
            "./n0679411000000964.jpg\n",
            "./n0679411000000966.jpg\n",
            "./n0679411000000967.jpg\n",
            "./n0679411000000969.jpg\n",
            "./n0679411000000972.jpg\n",
            "./n0679411000000976.jpg\n",
            "./n0679411000000978.jpg\n",
            "./n0679411000000981.jpg\n",
            "./n0679411000000983.jpg\n",
            "./n0679411000000985.jpg\n",
            "./n0679411000000989.jpg\n",
            "./n0679411000000992.jpg\n",
            "./n0679411000000993.jpg\n",
            "./n0679411000000995.jpg\n",
            "./n0679411000001000.jpg\n",
            "./n0679411000001003.jpg\n",
            "./n0679411000001005.jpg\n",
            "./n0679411000001007.jpg\n",
            "./n0679411000001008.jpg\n",
            "./n0679411000001010.jpg\n",
            "./n0679411000001011.jpg\n",
            "./n0679411000001015.jpg\n",
            "./n0679411000001017.jpg\n",
            "./n0679411000001020.jpg\n",
            "./n0679411000001031.jpg\n",
            "./n0679411000001032.jpg\n",
            "./n0679411000001035.jpg\n",
            "./n0679411000001036.jpg\n",
            "./n0679411000001037.jpg\n",
            "./n0679411000001038.jpg\n",
            "./n0679411000001039.jpg\n",
            "./n0679411000001040.jpg\n",
            "./n0679411000001044.jpg\n",
            "./n0679411000001050.jpg\n",
            "./n0679411000001051.jpg\n",
            "./n0679411000001052.jpg\n",
            "./n0679411000001053.jpg\n",
            "./n0679411000001055.jpg\n",
            "./n0679411000001061.jpg\n",
            "./n0679411000001064.jpg\n",
            "./n0679411000001065.jpg\n",
            "./n0679411000001067.jpg\n",
            "./n0679411000001068.jpg\n",
            "./n0679411000001069.jpg\n",
            "./n0679411000001071.jpg\n",
            "./n0679411000001073.jpg\n",
            "./n0679411000001074.jpg\n",
            "./n0679411000001077.jpg\n",
            "./n0679411000001078.jpg\n",
            "./n0679411000001079.jpg\n",
            "./n0679411000001080.jpg\n",
            "./n0679411000001082.jpg\n",
            "./n0679411000001083.jpg\n",
            "./n0679411000001085.jpg\n",
            "./n0679411000001087.jpg\n",
            "./n0679411000001090.jpg\n",
            "./n0679411000001092.jpg\n",
            "./n0679411000001093.jpg\n",
            "./n0679411000001094.jpg\n",
            "./n0679411000001095.jpg\n",
            "./n0679411000001096.jpg\n",
            "./n0679411000001099.jpg\n",
            "./n0679411000001102.jpg\n",
            "./n0679411000001103.jpg\n",
            "./n0679411000001105.jpg\n",
            "./n0679411000001107.jpg\n",
            "./n0679411000001108.jpg\n",
            "./n0679411000001109.jpg\n",
            "./n0679411000001110.jpg\n",
            "./n0679411000001111.jpg\n",
            "./n0679411000001113.jpg\n",
            "./n0679411000001116.jpg\n",
            "./n0679411000001121.jpg\n",
            "./n0679411000001132.jpg\n",
            "./n0679411000001134.jpg\n",
            "./n0679411000001137.jpg\n",
            "./n0679411000001138.jpg\n",
            "./n0679411000001140.jpg\n",
            "./n0679411000001144.jpg\n",
            "./n0679411000001145.jpg\n",
            "./n0679411000001146.jpg\n",
            "./n0679411000001148.jpg\n",
            "./n0679411000001149.jpg\n",
            "./n0679411000001151.jpg\n",
            "./n0679411000001153.jpg\n",
            "./n0679411000001155.jpg\n",
            "./n0679411000001158.jpg\n",
            "./n0679411000001159.jpg\n",
            "./n0679411000001161.jpg\n",
            "./n0679411000001164.jpg\n",
            "./n0679411000001167.jpg\n",
            "./n0679411000001169.jpg\n",
            "./n0679411000001170.jpg\n",
            "./n0679411000001171.jpg\n",
            "./n0679411000001173.jpg\n",
            "./n0679411000001175.jpg\n",
            "./n0679411000001176.jpg\n",
            "./n0679411000001177.jpg\n",
            "./n0679411000001180.jpg\n",
            "./n0679411000001185.jpg\n",
            "./n0679411000001186.jpg\n",
            "./n0679411000001188.jpg\n",
            "./n0679411000001191.jpg\n",
            "./n0679411000001194.jpg\n",
            "./n0679411000001195.jpg\n",
            "./n0679411000001197.jpg\n",
            "./n0679411000001198.jpg\n",
            "./n0679411000001199.jpg\n",
            "./n0679411000001201.jpg\n",
            "./n0679411000001202.jpg\n",
            "./n0679411000001203.jpg\n",
            "./n0679411000001205.jpg\n",
            "./n0679411000001206.jpg\n",
            "./n0679411000001208.jpg\n",
            "./n0679411000001209.jpg\n",
            "./n0679411000001211.jpg\n",
            "./n0679411000001212.jpg\n",
            "./n0679411000001215.jpg\n",
            "./n0679411000001218.jpg\n",
            "./n0679411000001219.jpg\n",
            "./n0679411000001220.jpg\n",
            "./n0679411000001221.jpg\n",
            "./n0679411000001222.jpg\n",
            "./n0679411000001223.jpg\n",
            "./n0679411000001224.jpg\n",
            "./n0679411000001225.jpg\n",
            "./n0679411000001226.jpg\n",
            "./n0679411000001227.jpg\n",
            "./n0679411000001234.jpg\n",
            "./n0679411000001238.jpg\n",
            "./n0679411000001239.jpg\n",
            "./n0679411000001242.jpg\n",
            "./n0679411000001243.jpg\n",
            "./n0679411000001245.jpg\n",
            "./n0679411000001247.jpg\n",
            "./n0679411000001250.jpg\n",
            "./n0679411000001254.jpg\n",
            "./n0679411000001256.jpg\n",
            "./n0679411000001257.jpg\n",
            "./n0679411000001262.jpg\n",
            "./n0679411000001264.jpg\n",
            "./n0679411000001265.jpg\n",
            "./n0679411000001266.jpg\n",
            "./n0679411000001267.jpg\n",
            "./n0679411000001268.jpg\n",
            "./n0679411000001272.jpg\n",
            "./n0679411000001276.jpg\n",
            "./n0679411000001278.jpg\n",
            "./n0679411000001282.jpg\n",
            "./n0679411000001284.jpg\n",
            "./n0679411000001286.jpg\n",
            "./n0679411000001288.jpg\n",
            "./n0679411000001289.jpg\n",
            "./n0679411000001291.jpg\n",
            "./n0679411000001294.jpg\n",
            "./n0679411000001295.jpg\n",
            "./n0679411000001296.jpg\n",
            "./n0679411000001297.jpg\n",
            "./n0679411000001299.jpg\n",
            "./n0679411000001300.jpg\n",
            "./n0758411000000003.jpg\n",
            "./n0758411000000004.jpg\n",
            "./n0758411000000007.jpg\n",
            "./n0758411000000010.jpg\n",
            "./n0758411000000013.jpg\n",
            "./n0758411000000015.jpg\n",
            "./n0758411000000021.jpg\n",
            "./n0758411000000022.jpg\n",
            "./n0758411000000023.jpg\n",
            "./n0758411000000025.jpg\n",
            "./n0758411000000026.jpg\n",
            "./n0758411000000029.jpg\n",
            "./n0758411000000030.jpg\n",
            "./n0758411000000031.jpg\n",
            "./n0758411000000034.jpg\n",
            "./n0758411000000035.jpg\n",
            "./n0758411000000038.jpg\n",
            "./n0758411000000045.jpg\n",
            "./n0758411000000046.jpg\n",
            "./n0758411000000048.jpg\n",
            "./n0758411000000051.jpg\n",
            "./n0758411000000052.jpg\n",
            "./n0758411000000054.jpg\n",
            "./n0758411000000055.jpg\n",
            "./n0758411000000059.jpg\n",
            "./n0758411000000061.jpg\n",
            "./n0758411000000066.jpg\n",
            "./n0758411000000067.jpg\n",
            "./n0758411000000071.jpg\n",
            "./n0758411000000072.jpg\n",
            "./n0758411000000076.jpg\n",
            "./n0758411000000079.jpg\n",
            "./n0758411000000080.jpg\n",
            "./n0758411000000081.jpg\n",
            "./n0758411000000086.jpg\n",
            "./n0758411000000087.jpg\n",
            "./n0758411000000088.jpg\n",
            "./n0758411000000089.jpg\n",
            "./n0758411000000090.jpg\n",
            "./n0758411000000098.jpg\n",
            "./n0758411000000102.jpg\n",
            "./n0758411000000103.jpg\n",
            "./n0758411000000104.jpg\n",
            "./n0758411000000105.jpg\n",
            "./n0758411000000108.jpg\n",
            "./n0758411000000109.jpg\n",
            "./n0758411000000110.jpg\n",
            "./n0758411000000112.jpg\n",
            "./n0758411000000115.jpg\n",
            "./n0758411000000118.jpg\n",
            "./n0758411000000120.jpg\n",
            "./n0758411000000122.jpg\n",
            "./n0758411000000123.jpg\n",
            "./n0758411000000124.jpg\n",
            "./n0758411000000125.jpg\n",
            "./n0758411000000128.jpg\n",
            "./n0758411000000130.jpg\n",
            "./n0758411000000131.jpg\n",
            "./n0758411000000133.jpg\n",
            "./n0758411000000136.jpg\n",
            "./n0758411000000137.jpg\n",
            "./n0758411000000139.jpg\n",
            "./n0758411000000140.jpg\n",
            "./n0758411000000143.jpg\n",
            "./n0758411000000144.jpg\n",
            "./n0758411000000145.jpg\n",
            "./n0758411000000146.jpg\n",
            "./n0758411000000147.jpg\n",
            "./n0758411000000148.jpg\n",
            "./n0758411000000149.jpg\n",
            "./n0758411000000150.jpg\n",
            "./n0758411000000151.jpg\n",
            "./n0758411000000152.jpg\n",
            "./n0758411000000154.jpg\n",
            "./n0758411000000159.jpg\n",
            "./n0758411000000161.jpg\n",
            "./n0758411000000162.jpg\n",
            "./n0758411000000168.jpg\n",
            "./n0758411000000169.jpg\n",
            "./n0758411000000170.jpg\n",
            "./n0758411000000171.jpg\n",
            "./n0758411000000172.jpg\n",
            "./n0758411000000174.jpg\n",
            "./n0758411000000177.jpg\n",
            "./n0758411000000180.jpg\n",
            "./n0758411000000181.jpg\n",
            "./n0758411000000182.jpg\n",
            "./n0758411000000183.jpg\n",
            "./n0758411000000184.jpg\n",
            "./n0758411000000186.jpg\n",
            "./n0758411000000187.jpg\n",
            "./n0758411000000190.jpg\n",
            "./n0758411000000191.jpg\n",
            "./n0758411000000193.jpg\n",
            "./n0758411000000198.jpg\n",
            "./n0758411000000205.jpg\n",
            "./n0758411000000206.jpg\n",
            "./n0758411000000208.jpg\n",
            "./n0758411000000212.jpg\n",
            "./n0758411000000213.jpg\n",
            "./n0758411000000214.jpg\n",
            "./n0758411000000215.jpg\n",
            "./n0758411000000216.jpg\n",
            "./n0758411000000220.jpg\n",
            "./n0758411000000223.jpg\n",
            "./n0758411000000225.jpg\n",
            "./n0758411000000226.jpg\n",
            "./n0758411000000229.jpg\n",
            "./n0758411000000232.jpg\n",
            "./n0758411000000234.jpg\n",
            "./n0758411000000235.jpg\n",
            "./n0758411000000237.jpg\n",
            "./n0758411000000238.jpg\n",
            "./n0758411000000239.jpg\n",
            "./n0758411000000243.jpg\n",
            "./n0758411000000244.jpg\n",
            "./n0758411000000247.jpg\n",
            "./n0758411000000250.jpg\n",
            "./n0758411000000252.jpg\n",
            "./n0758411000000254.jpg\n",
            "./n0758411000000258.jpg\n",
            "./n0758411000000262.jpg\n",
            "./n0758411000000268.jpg\n",
            "./n0758411000000269.jpg\n",
            "./n0758411000000270.jpg\n",
            "./n0758411000000271.jpg\n",
            "./n0758411000000272.jpg\n",
            "./n0758411000000276.jpg\n",
            "./n0758411000000277.jpg\n",
            "./n0758411000000278.jpg\n",
            "./n0758411000000280.jpg\n",
            "./n0758411000000282.jpg\n",
            "./n0758411000000283.jpg\n",
            "./n0758411000000284.jpg\n",
            "./n0758411000000285.jpg\n",
            "./n0758411000000287.jpg\n",
            "./n0758411000000289.jpg\n",
            "./n0758411000000290.jpg\n",
            "./n0758411000000292.jpg\n",
            "./n0758411000000293.jpg\n",
            "./n0758411000000294.jpg\n",
            "./n0758411000000295.jpg\n",
            "./n0758411000000299.jpg\n",
            "./n0758411000000301.jpg\n",
            "./n0758411000000306.jpg\n",
            "./n0758411000000307.jpg\n",
            "./n0758411000000309.jpg\n",
            "./n0758411000000313.jpg\n",
            "./n0758411000000314.jpg\n",
            "./n0758411000000315.jpg\n",
            "./n0758411000000316.jpg\n",
            "./n0758411000000317.jpg\n",
            "./n0758411000000319.jpg\n",
            "./n0758411000000323.jpg\n",
            "./n0758411000000325.jpg\n",
            "./n0758411000000327.jpg\n",
            "./n0758411000000328.jpg\n",
            "./n0758411000000333.jpg\n",
            "./n0758411000000334.jpg\n",
            "./n0758411000000335.jpg\n",
            "./n0758411000000339.jpg\n",
            "./n0758411000000340.jpg\n",
            "./n0758411000000343.jpg\n",
            "./n0758411000000344.jpg\n",
            "./n0758411000000350.jpg\n",
            "./n0758411000000351.jpg\n",
            "./n0758411000000356.jpg\n",
            "./n0758411000000357.jpg\n",
            "./n0758411000000358.jpg\n",
            "./n0758411000000359.jpg\n",
            "./n0758411000000360.jpg\n",
            "./n0758411000000361.jpg\n",
            "./n0758411000000363.jpg\n",
            "./n0758411000000364.jpg\n",
            "./n0758411000000368.jpg\n",
            "./n0758411000000369.jpg\n",
            "./n0758411000000370.jpg\n",
            "./n0758411000000374.jpg\n",
            "./n0758411000000377.jpg\n",
            "./n0758411000000378.jpg\n",
            "./n0758411000000381.jpg\n",
            "./n0758411000000382.jpg\n",
            "./n0758411000000384.jpg\n",
            "./n0758411000000386.jpg\n",
            "./n0758411000000387.jpg\n",
            "./n0758411000000390.jpg\n",
            "./n0758411000000393.jpg\n",
            "./n0758411000000397.jpg\n",
            "./n0758411000000399.jpg\n",
            "./n0758411000000400.jpg\n",
            "./n0758411000000403.jpg\n",
            "./n0758411000000407.jpg\n",
            "./n0758411000000409.jpg\n",
            "./n0758411000000410.jpg\n",
            "./n0758411000000411.jpg\n",
            "./n0758411000000412.jpg\n",
            "./n0758411000000413.jpg\n",
            "./n0758411000000414.jpg\n",
            "./n0758411000000416.jpg\n",
            "./n0758411000000417.jpg\n",
            "./n0758411000000424.jpg\n",
            "./n0758411000000425.jpg\n",
            "./n0758411000000427.jpg\n",
            "./n0758411000000435.jpg\n",
            "./n0758411000000436.jpg\n",
            "./n0758411000000437.jpg\n",
            "./n0758411000000439.jpg\n",
            "./n0758411000000441.jpg\n",
            "./n0758411000000442.jpg\n",
            "./n0758411000000444.jpg\n",
            "./n0758411000000447.jpg\n",
            "./n0758411000000451.jpg\n",
            "./n0758411000000452.jpg\n",
            "./n0758411000000453.jpg\n",
            "./n0758411000000455.jpg\n",
            "./n0758411000000456.jpg\n",
            "./n0758411000000458.jpg\n",
            "./n0758411000000462.jpg\n",
            "./n0758411000000464.jpg\n",
            "./n0758411000000465.jpg\n",
            "./n0758411000000466.jpg\n",
            "./n0758411000000467.jpg\n",
            "./n0758411000000468.jpg\n",
            "./n0758411000000469.jpg\n",
            "./n0758411000000471.jpg\n",
            "./n0758411000000472.jpg\n",
            "./n0758411000000473.jpg\n",
            "./n0758411000000474.jpg\n",
            "./n0758411000000475.jpg\n",
            "./n0758411000000477.jpg\n",
            "./n0758411000000479.jpg\n",
            "./n0758411000000483.jpg\n",
            "./n0758411000000484.jpg\n",
            "./n0758411000000490.jpg\n",
            "./n0758411000000492.jpg\n",
            "./n0758411000000494.jpg\n",
            "./n0758411000000497.jpg\n",
            "./n0758411000000499.jpg\n",
            "./n0758411000000503.jpg\n",
            "./n0758411000000504.jpg\n",
            "./n0758411000000508.jpg\n",
            "./n0758411000000514.jpg\n",
            "./n0758411000000517.jpg\n",
            "./n0758411000000519.jpg\n",
            "./n0758411000000522.jpg\n",
            "./n0758411000000523.jpg\n",
            "./n0758411000000525.jpg\n",
            "./n0758411000000526.jpg\n",
            "./n0758411000000527.jpg\n",
            "./n0758411000000529.jpg\n",
            "./n0758411000000533.jpg\n",
            "./n0758411000000535.jpg\n",
            "./n0758411000000536.jpg\n",
            "./n0758411000000539.jpg\n",
            "./n0758411000000542.jpg\n",
            "./n0758411000000543.jpg\n",
            "./n0758411000000545.jpg\n",
            "./n0758411000000546.jpg\n",
            "./n0758411000000549.jpg\n",
            "./n0758411000000553.jpg\n",
            "./n0758411000000557.jpg\n",
            "./n0758411000000559.jpg\n",
            "./n0758411000000560.jpg\n",
            "./n0758411000000561.jpg\n",
            "./n0758411000000564.jpg\n",
            "./n0758411000000566.jpg\n",
            "./n0758411000000567.jpg\n",
            "./n0758411000000568.jpg\n",
            "./n0758411000000573.jpg\n",
            "./n0758411000000574.jpg\n",
            "./n0758411000000575.jpg\n",
            "./n0758411000000577.jpg\n",
            "./n0758411000000578.jpg\n",
            "./n0758411000000581.jpg\n",
            "./n0758411000000585.jpg\n",
            "./n0758411000000588.jpg\n",
            "./n0758411000000590.jpg\n",
            "./n0758411000000592.jpg\n",
            "./n0758411000000594.jpg\n",
            "./n0758411000000595.jpg\n",
            "./n0758411000000597.jpg\n",
            "./n0758411000000598.jpg\n",
            "./n0758411000000599.jpg\n",
            "./n0758411000000600.jpg\n",
            "./n0758411000000601.jpg\n",
            "./n0758411000000602.jpg\n",
            "./n0758411000000605.jpg\n",
            "./n0758411000000609.jpg\n",
            "./n0758411000000610.jpg\n",
            "./n0758411000000612.jpg\n",
            "./n0758411000000614.jpg\n",
            "./n0758411000000615.jpg\n",
            "./n0758411000000618.jpg\n",
            "./n0758411000000620.jpg\n",
            "./n0758411000000622.jpg\n",
            "./n0758411000000624.jpg\n",
            "./n0758411000000626.jpg\n",
            "./n0758411000000627.jpg\n",
            "./n0758411000000628.jpg\n",
            "./n0758411000000631.jpg\n",
            "./n0758411000000632.jpg\n",
            "./n0758411000000633.jpg\n",
            "./n0758411000000635.jpg\n",
            "./n0758411000000636.jpg\n",
            "./n0758411000000639.jpg\n",
            "./n0758411000000642.jpg\n",
            "./n0758411000000645.jpg\n",
            "./n0758411000000646.jpg\n",
            "./n0758411000000648.jpg\n",
            "./n0758411000000649.jpg\n",
            "./n0758411000000650.jpg\n",
            "./n0758411000000651.jpg\n",
            "./n0758411000000652.jpg\n",
            "./n0758411000000653.jpg\n",
            "./n0758411000000654.jpg\n",
            "./n0758411000000655.jpg\n",
            "./n0758411000000656.jpg\n",
            "./n0758411000000658.jpg\n",
            "./n0758411000000660.jpg\n",
            "./n0758411000000661.jpg\n",
            "./n0758411000000662.jpg\n",
            "./n0758411000000666.jpg\n",
            "./n0758411000000670.jpg\n",
            "./n0758411000000672.jpg\n",
            "./n0758411000000675.jpg\n",
            "./n0758411000000681.jpg\n",
            "./n0758411000000683.jpg\n",
            "./n0758411000000684.jpg\n",
            "./n0758411000000686.jpg\n",
            "./n0758411000000693.jpg\n",
            "./n0758411000000694.jpg\n",
            "./n0758411000000695.jpg\n",
            "./n0758411000000696.jpg\n",
            "./n0758411000000698.jpg\n",
            "./n0758411000000699.jpg\n",
            "./n0758411000000701.jpg\n",
            "./n0758411000000704.jpg\n",
            "./n0758411000000706.jpg\n",
            "./n0758411000000707.jpg\n",
            "./n0758411000000709.jpg\n",
            "./n0758411000000710.jpg\n",
            "./n0758411000000711.jpg\n",
            "./n0758411000000712.jpg\n",
            "./n0758411000000713.jpg\n",
            "./n0758411000000715.jpg\n",
            "./n0758411000000716.jpg\n",
            "./n0758411000000717.jpg\n",
            "./n0758411000000720.jpg\n",
            "./n0758411000000722.jpg\n",
            "./n0758411000000723.jpg\n",
            "./n0758411000000724.jpg\n",
            "./n0758411000000727.jpg\n",
            "./n0758411000000728.jpg\n",
            "./n0758411000000731.jpg\n",
            "./n0758411000000732.jpg\n",
            "./n0758411000000734.jpg\n",
            "./n0758411000000737.jpg\n",
            "./n0758411000000738.jpg\n",
            "./n0758411000000739.jpg\n",
            "./n0758411000000741.jpg\n",
            "./n0758411000000742.jpg\n",
            "./n0758411000000743.jpg\n",
            "./n0758411000000748.jpg\n",
            "./n0758411000000750.jpg\n",
            "./n0758411000000751.jpg\n",
            "./n0758411000000756.jpg\n",
            "./n0758411000000759.jpg\n",
            "./n0758411000000761.jpg\n",
            "./n0758411000000763.jpg\n",
            "./n0758411000000764.jpg\n",
            "./n0758411000000765.jpg\n",
            "./n0758411000000767.jpg\n",
            "./n0758411000000768.jpg\n",
            "./n0758411000000769.jpg\n",
            "./n0758411000000770.jpg\n",
            "./n0758411000000771.jpg\n",
            "./n0758411000000773.jpg\n",
            "./n0758411000000777.jpg\n",
            "./n0758411000000780.jpg\n",
            "./n0758411000000781.jpg\n",
            "./n0758411000000785.jpg\n",
            "./n0758411000000786.jpg\n",
            "./n0758411000000788.jpg\n",
            "./n0758411000000790.jpg\n",
            "./n0758411000000791.jpg\n",
            "./n0758411000000792.jpg\n",
            "./n0758411000000794.jpg\n",
            "./n0758411000000800.jpg\n",
            "./n0758411000000801.jpg\n",
            "./n0758411000000806.jpg\n",
            "./n0758411000000807.jpg\n",
            "./n0758411000000808.jpg\n",
            "./n0758411000000809.jpg\n",
            "./n0758411000000811.jpg\n",
            "./n0758411000000813.jpg\n",
            "./n0758411000000814.jpg\n",
            "./n0758411000000815.jpg\n",
            "./n0758411000000819.jpg\n",
            "./n0758411000000825.jpg\n",
            "./n0758411000000826.jpg\n",
            "./n0758411000000828.jpg\n",
            "./n0758411000000829.jpg\n",
            "./n0758411000000830.jpg\n",
            "./n0758411000000831.jpg\n",
            "./n0758411000000834.jpg\n",
            "./n0758411000000835.jpg\n",
            "./n0758411000000836.jpg\n",
            "./n0758411000000840.jpg\n",
            "./n0758411000000843.jpg\n",
            "./n0758411000000844.jpg\n",
            "./n0758411000000845.jpg\n",
            "./n0758411000000850.jpg\n",
            "./n0758411000000852.jpg\n",
            "./n0758411000000855.jpg\n",
            "./n0758411000000856.jpg\n",
            "./n0758411000000858.jpg\n",
            "./n0758411000000859.jpg\n",
            "./n0758411000000860.jpg\n",
            "./n0758411000000861.jpg\n",
            "./n0758411000000862.jpg\n",
            "./n0758411000000863.jpg\n",
            "./n0758411000000867.jpg\n",
            "./n0758411000000868.jpg\n",
            "./n0758411000000874.jpg\n",
            "./n0758411000000875.jpg\n",
            "./n0758411000000877.jpg\n",
            "./n0758411000000878.jpg\n",
            "./n0758411000000879.jpg\n",
            "./n0758411000000880.jpg\n",
            "./n0758411000000884.jpg\n",
            "./n0758411000000885.jpg\n",
            "./n0758411000000886.jpg\n",
            "./n0758411000000890.jpg\n",
            "./n0758411000000893.jpg\n",
            "./n0758411000000894.jpg\n",
            "./n0758411000000896.jpg\n",
            "./n0758411000000898.jpg\n",
            "./n0758411000000899.jpg\n",
            "./n0758411000000901.jpg\n",
            "./n0758411000000902.jpg\n",
            "./n0758411000000903.jpg\n",
            "./n0758411000000906.jpg\n",
            "./n0758411000000907.jpg\n",
            "./n0758411000000908.jpg\n",
            "./n0758411000000912.jpg\n",
            "./n0758411000000913.jpg\n",
            "./n0758411000000915.jpg\n",
            "./n0758411000000919.jpg\n",
            "./n0758411000000920.jpg\n",
            "./n0758411000000921.jpg\n",
            "./n0758411000000922.jpg\n",
            "./n0758411000000923.jpg\n",
            "./n0758411000000925.jpg\n",
            "./n0758411000000927.jpg\n",
            "./n0758411000000929.jpg\n",
            "./n0758411000000930.jpg\n",
            "./n0758411000000933.jpg\n",
            "./n0758411000000935.jpg\n",
            "./n0758411000000937.jpg\n",
            "./n0758411000000938.jpg\n",
            "./n0758411000000941.jpg\n",
            "./n0758411000000942.jpg\n",
            "./n0758411000000945.jpg\n",
            "./n0758411000000946.jpg\n",
            "./n0758411000000948.jpg\n",
            "./n0758411000000949.jpg\n",
            "./n0758411000000950.jpg\n",
            "./n0758411000000952.jpg\n",
            "./n0758411000000956.jpg\n",
            "./n0758411000000957.jpg\n",
            "./n0758411000000959.jpg\n",
            "./n0758411000000962.jpg\n",
            "./n0758411000000963.jpg\n",
            "./n0758411000000964.jpg\n",
            "./n0758411000000965.jpg\n",
            "./n0758411000000966.jpg\n",
            "./n0758411000000969.jpg\n",
            "./n0758411000000972.jpg\n",
            "./n0758411000000973.jpg\n",
            "./n0758411000000979.jpg\n",
            "./n0758411000000980.jpg\n",
            "./n0758411000000982.jpg\n",
            "./n0758411000000990.jpg\n",
            "./n0758411000000991.jpg\n",
            "./n0758411000000993.jpg\n",
            "./n0758411000000994.jpg\n",
            "./n0758411000000995.jpg\n",
            "./n0758411000000997.jpg\n",
            "./n0758411000000998.jpg\n",
            "./n0758411000000999.jpg\n",
            "./n0758411000001000.jpg\n",
            "./n0758411000001002.jpg\n",
            "./n0758411000001003.jpg\n",
            "./n0758411000001004.jpg\n",
            "./n0758411000001005.jpg\n",
            "./n0758411000001015.jpg\n",
            "./n0758411000001017.jpg\n",
            "./n0758411000001019.jpg\n",
            "./n0758411000001022.jpg\n",
            "./n0758411000001025.jpg\n",
            "./n0758411000001028.jpg\n",
            "./n0758411000001029.jpg\n",
            "./n0758411000001031.jpg\n",
            "./n0758411000001032.jpg\n",
            "./n0758411000001033.jpg\n",
            "./n0758411000001035.jpg\n",
            "./n0758411000001039.jpg\n",
            "./n0758411000001040.jpg\n",
            "./n0758411000001041.jpg\n",
            "./n0758411000001042.jpg\n",
            "./n0758411000001046.jpg\n",
            "./n0758411000001047.jpg\n",
            "./n0758411000001049.jpg\n",
            "./n0758411000001051.jpg\n",
            "./n0758411000001055.jpg\n",
            "./n0758411000001059.jpg\n",
            "./n0758411000001060.jpg\n",
            "./n0758411000001061.jpg\n",
            "./n0758411000001062.jpg\n",
            "./n0758411000001063.jpg\n",
            "./n0758411000001066.jpg\n",
            "./n0758411000001068.jpg\n",
            "./n0758411000001069.jpg\n",
            "./n0758411000001070.jpg\n",
            "./n0758411000001073.jpg\n",
            "./n0758411000001074.jpg\n",
            "./n0758411000001076.jpg\n",
            "./n0758411000001080.jpg\n",
            "./n0758411000001083.jpg\n",
            "./n0758411000001084.jpg\n",
            "./n0758411000001087.jpg\n",
            "./n0758411000001088.jpg\n",
            "./n0758411000001090.jpg\n",
            "./n0758411000001091.jpg\n",
            "./n0758411000001092.jpg\n",
            "./n0758411000001093.jpg\n",
            "./n0758411000001096.jpg\n",
            "./n0758411000001098.jpg\n",
            "./n0758411000001099.jpg\n",
            "./n0758411000001100.jpg\n",
            "./n0758411000001102.jpg\n",
            "./n0758411000001104.jpg\n",
            "./n0758411000001105.jpg\n",
            "./n0758411000001106.jpg\n",
            "./n0758411000001107.jpg\n",
            "./n0758411000001109.jpg\n",
            "./n0758411000001110.jpg\n",
            "./n0758411000001111.jpg\n",
            "./n0758411000001112.jpg\n",
            "./n0758411000001114.jpg\n",
            "./n0758411000001116.jpg\n",
            "./n0758411000001120.jpg\n",
            "./n0758411000001121.jpg\n",
            "./n0758411000001123.jpg\n",
            "./n0758411000001125.jpg\n",
            "./n0758411000001126.jpg\n",
            "./n0758411000001127.jpg\n",
            "./n0758411000001128.jpg\n",
            "./n0758411000001130.jpg\n",
            "./n0758411000001132.jpg\n",
            "./n0758411000001134.jpg\n",
            "./n0758411000001135.jpg\n",
            "./n0758411000001136.jpg\n",
            "./n0758411000001137.jpg\n",
            "./n0758411000001140.jpg\n",
            "./n0758411000001142.jpg\n",
            "./n0758411000001144.jpg\n",
            "./n0758411000001146.jpg\n",
            "./n0758411000001147.jpg\n",
            "./n0758411000001148.jpg\n",
            "./n0758411000001149.jpg\n",
            "./n0758411000001151.jpg\n",
            "./n0758411000001154.jpg\n",
            "./n0758411000001156.jpg\n",
            "./n0758411000001157.jpg\n",
            "./n0758411000001160.jpg\n",
            "./n0758411000001161.jpg\n",
            "./n0758411000001164.jpg\n",
            "./n0758411000001165.jpg\n",
            "./n0758411000001166.jpg\n",
            "./n0758411000001170.jpg\n",
            "./n0758411000001173.jpg\n",
            "./n0758411000001174.jpg\n",
            "./n0758411000001175.jpg\n",
            "./n0758411000001176.jpg\n",
            "./n0758411000001178.jpg\n",
            "./n0758411000001180.jpg\n",
            "./n0758411000001181.jpg\n",
            "./n0758411000001182.jpg\n",
            "./n0758411000001184.jpg\n",
            "./n0758411000001186.jpg\n",
            "./n0758411000001188.jpg\n",
            "./n0758411000001190.jpg\n",
            "./n0758411000001191.jpg\n",
            "./n0758411000001192.jpg\n",
            "./n0758411000001193.jpg\n",
            "./n0758411000001195.jpg\n",
            "./n0758411000001197.jpg\n",
            "./n0758411000001202.jpg\n",
            "./n0758411000001205.jpg\n",
            "./n0761348000000001.jpg\n",
            "./n0761348000000002.jpg\n",
            "./n0761348000000005.jpg\n",
            "./n0761348000000007.jpg\n",
            "./n0761348000000009.jpg\n",
            "./n0761348000000010.jpg\n",
            "./n0761348000000011.jpg\n",
            "./n0761348000000013.jpg\n",
            "./n0761348000000015.jpg\n",
            "./n0761348000000019.jpg\n",
            "./n0761348000000022.jpg\n",
            "./n0761348000000025.jpg\n",
            "./n0761348000000027.jpg\n",
            "./n0761348000000031.jpg\n",
            "./n0761348000000037.jpg\n",
            "./n0761348000000042.jpg\n",
            "./n0761348000000044.jpg\n",
            "./n0761348000000045.jpg\n",
            "./n0761348000000046.jpg\n",
            "./n0761348000000047.jpg\n",
            "./n0761348000000049.jpg\n",
            "./n0761348000000050.jpg\n",
            "./n0761348000000054.jpg\n",
            "./n0761348000000057.jpg\n",
            "./n0761348000000061.jpg\n",
            "./n0761348000000062.jpg\n",
            "./n0761348000000064.jpg\n",
            "./n0761348000000072.jpg\n",
            "./n0761348000000073.jpg\n",
            "./n0761348000000074.jpg\n",
            "./n0761348000000075.jpg\n",
            "./n0761348000000076.jpg\n",
            "./n0761348000000079.jpg\n",
            "./n0761348000000080.jpg\n",
            "./n0761348000000082.jpg\n",
            "./n0761348000000083.jpg\n",
            "./n0761348000000084.jpg\n",
            "./n0761348000000085.jpg\n",
            "./n0761348000000090.jpg\n",
            "./n0761348000000091.jpg\n",
            "./n0761348000000092.jpg\n",
            "./n0761348000000095.jpg\n",
            "./n0761348000000097.jpg\n",
            "./n0761348000000099.jpg\n",
            "./n0761348000000110.jpg\n",
            "./n0761348000000112.jpg\n",
            "./n0761348000000113.jpg\n",
            "./n0761348000000114.jpg\n",
            "./n0761348000000115.jpg\n",
            "./n0761348000000117.jpg\n",
            "./n0761348000000121.jpg\n",
            "./n0761348000000124.jpg\n",
            "./n0761348000000125.jpg\n",
            "./n0761348000000126.jpg\n",
            "./n0761348000000127.jpg\n",
            "./n0761348000000130.jpg\n",
            "./n0761348000000134.jpg\n",
            "./n0761348000000139.jpg\n",
            "./n0761348000000140.jpg\n",
            "./n0761348000000141.jpg\n",
            "./n0761348000000143.jpg\n",
            "./n0761348000000144.jpg\n",
            "./n0761348000000145.jpg\n",
            "./n0761348000000147.jpg\n",
            "./n0761348000000149.jpg\n",
            "./n0761348000000152.jpg\n",
            "./n0761348000000153.jpg\n",
            "./n0761348000000157.jpg\n",
            "./n0761348000000158.jpg\n",
            "./n0761348000000159.jpg\n",
            "./n0761348000000162.jpg\n",
            "./n0761348000000163.jpg\n",
            "./n0761348000000164.jpg\n",
            "./n0761348000000165.jpg\n",
            "./n0761348000000166.jpg\n",
            "./n0761348000000168.jpg\n",
            "./n0761348000000169.jpg\n",
            "./n0761348000000171.jpg\n",
            "./n0761348000000176.jpg\n",
            "./n0761348000000178.jpg\n",
            "./n0761348000000179.jpg\n",
            "./n0761348000000180.jpg\n",
            "./n0761348000000183.jpg\n",
            "./n0761348000000185.jpg\n",
            "./n0761348000000186.jpg\n",
            "./n0761348000000188.jpg\n",
            "./n0761348000000189.jpg\n",
            "./n0761348000000190.jpg\n",
            "./n0761348000000191.jpg\n",
            "./n0761348000000193.jpg\n",
            "./n0761348000000197.jpg\n",
            "./n0761348000000198.jpg\n",
            "./n0761348000000199.jpg\n",
            "./n0761348000000200.jpg\n",
            "./n0761348000000201.jpg\n",
            "./n0761348000000205.jpg\n",
            "./n0761348000000206.jpg\n",
            "./n0761348000000208.jpg\n",
            "./n0761348000000209.jpg\n",
            "./n0761348000000211.jpg\n",
            "./n0761348000000213.jpg\n",
            "./n0761348000000214.jpg\n",
            "./n0761348000000217.jpg\n",
            "./n0761348000000219.jpg\n",
            "./n0761348000000222.jpg\n",
            "./n0761348000000224.jpg\n",
            "./n0761348000000225.jpg\n",
            "./n0761348000000227.jpg\n",
            "./n0761348000000228.jpg\n",
            "./n0761348000000229.jpg\n",
            "./n0761348000000230.jpg\n",
            "./n0761348000000231.jpg\n",
            "./n0761348000000239.jpg\n",
            "./n0761348000000240.jpg\n",
            "./n0761348000000242.jpg\n",
            "./n0761348000000245.jpg\n",
            "./n0761348000000249.jpg\n",
            "./n0761348000000253.jpg\n",
            "./n0761348000000254.jpg\n",
            "./n0761348000000256.jpg\n",
            "./n0761348000000257.jpg\n",
            "./n0761348000000258.jpg\n",
            "./n0761348000000262.jpg\n",
            "./n0761348000000265.jpg\n",
            "./n0761348000000266.jpg\n",
            "./n0761348000000267.jpg\n",
            "./n0761348000000269.jpg\n",
            "./n0761348000000272.jpg\n",
            "./n0761348000000274.jpg\n",
            "./n0761348000000275.jpg\n",
            "./n0761348000000276.jpg\n",
            "./n0761348000000277.jpg\n",
            "./n0761348000000278.jpg\n",
            "./n0761348000000279.jpg\n",
            "./n0761348000000280.jpg\n",
            "./n0761348000000282.jpg\n",
            "./n0761348000000283.jpg\n",
            "./n0761348000000286.jpg\n",
            "./n0761348000000287.jpg\n",
            "./n0761348000000289.jpg\n",
            "./n0761348000000291.jpg\n",
            "./n0761348000000292.jpg\n",
            "./n0761348000000293.jpg\n",
            "./n0761348000000296.jpg\n",
            "./n0761348000000297.jpg\n",
            "./n0761348000000298.jpg\n",
            "./n0761348000000299.jpg\n",
            "./n0761348000000300.jpg\n",
            "./n0761348000000301.jpg\n",
            "./n0761348000000304.jpg\n",
            "./n0761348000000306.jpg\n",
            "./n0761348000000307.jpg\n",
            "./n0761348000000308.jpg\n",
            "./n0761348000000310.jpg\n",
            "./n0761348000000312.jpg\n",
            "./n0761348000000313.jpg\n",
            "./n0761348000000314.jpg\n",
            "./n0761348000000315.jpg\n",
            "./n0761348000000317.jpg\n",
            "./n0761348000000322.jpg\n",
            "./n0761348000000323.jpg\n",
            "./n0761348000000327.jpg\n",
            "./n0761348000000328.jpg\n",
            "./n0761348000000330.jpg\n",
            "./n0761348000000331.jpg\n",
            "./n0761348000000333.jpg\n",
            "./n0761348000000336.jpg\n",
            "./n0761348000000337.jpg\n",
            "./n0761348000000343.jpg\n",
            "./n0761348000000344.jpg\n",
            "./n0761348000000349.jpg\n",
            "./n0761348000000350.jpg\n",
            "./n0761348000000351.jpg\n",
            "./n0761348000000353.jpg\n",
            "./n0761348000000355.jpg\n",
            "./n0761348000000363.jpg\n",
            "./n0761348000000364.jpg\n",
            "./n0761348000000365.jpg\n",
            "./n0761348000000371.jpg\n",
            "./n0761348000000374.jpg\n",
            "./n0761348000000377.jpg\n",
            "./n0761348000000380.jpg\n",
            "./n0761348000000382.jpg\n",
            "./n0761348000000383.jpg\n",
            "./n0761348000000386.jpg\n",
            "./n0761348000000388.jpg\n",
            "./n0761348000000393.jpg\n",
            "./n0761348000000394.jpg\n",
            "./n0761348000000396.jpg\n",
            "./n0761348000000400.jpg\n",
            "./n0761348000000401.jpg\n",
            "./n0761348000000403.jpg\n",
            "./n0761348000000406.jpg\n",
            "./n0761348000000407.jpg\n",
            "./n0761348000000408.jpg\n",
            "./n0761348000000411.jpg\n",
            "./n0761348000000413.jpg\n",
            "./n0761348000000415.jpg\n",
            "./n0761348000000417.jpg\n",
            "./n0761348000000419.jpg\n",
            "./n0761348000000421.jpg\n",
            "./n0761348000000424.jpg\n",
            "./n0761348000000427.jpg\n",
            "./n0761348000000428.jpg\n",
            "./n0761348000000429.jpg\n",
            "./n0761348000000432.jpg\n",
            "./n0761348000000433.jpg\n",
            "./n0761348000000434.jpg\n",
            "./n0761348000000436.jpg\n",
            "./n0761348000000440.jpg\n",
            "./n0761348000000441.jpg\n",
            "./n0761348000000443.jpg\n",
            "./n0761348000000444.jpg\n",
            "./n0761348000000445.jpg\n",
            "./n0761348000000447.jpg\n",
            "./n0761348000000448.jpg\n",
            "./n0761348000000450.jpg\n",
            "./n0761348000000451.jpg\n",
            "./n0761348000000452.jpg\n",
            "./n0761348000000457.jpg\n",
            "./n0761348000000461.jpg\n",
            "./n0761348000000462.jpg\n",
            "./n0761348000000464.jpg\n",
            "./n0761348000000465.jpg\n",
            "./n0761348000000467.jpg\n",
            "./n0761348000000468.jpg\n",
            "./n0761348000000469.jpg\n",
            "./n0761348000000470.jpg\n",
            "./n0761348000000472.jpg\n",
            "./n0761348000000473.jpg\n",
            "./n0761348000000475.jpg\n",
            "./n0761348000000477.jpg\n",
            "./n0761348000000478.jpg\n",
            "./n0761348000000480.jpg\n",
            "./n0761348000000482.jpg\n",
            "./n0761348000000486.jpg\n",
            "./n0761348000000487.jpg\n",
            "./n0761348000000489.jpg\n",
            "./n0761348000000493.jpg\n",
            "./n0761348000000494.jpg\n",
            "./n0761348000000495.jpg\n",
            "./n0761348000000497.jpg\n",
            "./n0761348000000498.jpg\n",
            "./n0761348000000502.jpg\n",
            "./n0761348000000509.jpg\n",
            "./n0761348000000511.jpg\n",
            "./n0761348000000513.jpg\n",
            "./n0761348000000518.jpg\n",
            "./n0761348000000520.jpg\n",
            "./n0761348000000521.jpg\n",
            "./n0761348000000531.jpg\n",
            "./n0761348000000532.jpg\n",
            "./n0761348000000535.jpg\n",
            "./n0761348000000538.jpg\n",
            "./n0761348000000541.jpg\n",
            "./n0761348000000543.jpg\n",
            "./n0761348000000545.jpg\n",
            "./n0761348000000547.jpg\n",
            "./n0761348000000549.jpg\n",
            "./n0761348000000550.jpg\n",
            "./n0761348000000553.jpg\n",
            "./n0761348000000554.jpg\n",
            "./n0761348000000556.jpg\n",
            "./n0761348000000557.jpg\n",
            "./n0761348000000560.jpg\n",
            "./n0761348000000563.jpg\n",
            "./n0761348000000564.jpg\n",
            "./n0761348000000566.jpg\n",
            "./n0761348000000568.jpg\n",
            "./n0761348000000570.jpg\n",
            "./n0761348000000571.jpg\n",
            "./n0761348000000574.jpg\n",
            "./n0761348000000577.jpg\n",
            "./n0761348000000579.jpg\n",
            "./n0761348000000580.jpg\n",
            "./n0761348000000581.jpg\n",
            "./n0761348000000582.jpg\n",
            "./n0761348000000583.jpg\n",
            "./n0761348000000586.jpg\n",
            "./n0761348000000587.jpg\n",
            "./n0761348000000588.jpg\n",
            "./n0761348000000590.jpg\n",
            "./n0761348000000592.jpg\n",
            "./n0761348000000594.jpg\n",
            "./n0761348000000595.jpg\n",
            "./n0761348000000596.jpg\n",
            "./n0761348000000597.jpg\n",
            "./n0761348000000602.jpg\n",
            "./n0761348000000605.jpg\n",
            "./n0761348000000607.jpg\n",
            "./n0761348000000611.jpg\n",
            "./n0761348000000612.jpg\n",
            "./n0761348000000615.jpg\n",
            "./n0761348000000616.jpg\n",
            "./n0761348000000618.jpg\n",
            "./n0761348000000620.jpg\n",
            "./n0761348000000622.jpg\n",
            "./n0761348000000624.jpg\n",
            "./n0761348000000627.jpg\n",
            "./n0761348000000630.jpg\n",
            "./n0761348000000632.jpg\n",
            "./n0761348000000633.jpg\n",
            "./n0761348000000635.jpg\n",
            "./n0761348000000641.jpg\n",
            "./n0761348000000642.jpg\n",
            "./n0761348000000645.jpg\n",
            "./n0761348000000649.jpg\n",
            "./n0761348000000650.jpg\n",
            "./n0761348000000651.jpg\n",
            "./n0761348000000652.jpg\n",
            "./n0761348000000656.jpg\n",
            "./n0761348000000657.jpg\n",
            "./n0761348000000659.jpg\n",
            "./n0761348000000662.jpg\n",
            "./n0761348000000663.jpg\n",
            "./n0761348000000664.jpg\n",
            "./n0761348000000665.jpg\n",
            "./n0761348000000666.jpg\n",
            "./n0761348000000667.jpg\n",
            "./n0761348000000668.jpg\n",
            "./n0761348000000669.jpg\n",
            "./n0761348000000671.jpg\n",
            "./n0761348000000672.jpg\n",
            "./n0761348000000677.jpg\n",
            "./n0761348000000681.jpg\n",
            "./n0761348000000683.jpg\n",
            "./n0761348000000684.jpg\n",
            "./n0761348000000686.jpg\n",
            "./n0761348000000689.jpg\n",
            "./n0761348000000690.jpg\n",
            "./n0761348000000691.jpg\n",
            "./n0761348000000692.jpg\n",
            "./n0761348000000693.jpg\n",
            "./n0761348000000696.jpg\n",
            "./n0761348000000697.jpg\n",
            "./n0761348000000698.jpg\n",
            "./n0761348000000700.jpg\n",
            "./n0761348000000705.jpg\n",
            "./n0761348000000707.jpg\n",
            "./n0761348000000709.jpg\n",
            "./n0761348000000710.jpg\n",
            "./n0761348000000713.jpg\n",
            "./n0761348000000714.jpg\n",
            "./n0761348000000715.jpg\n",
            "./n0761348000000716.jpg\n",
            "./n0761348000000717.jpg\n",
            "./n0761348000000721.jpg\n",
            "./n0761348000000723.jpg\n",
            "./n0761348000000724.jpg\n",
            "./n0761348000000730.jpg\n",
            "./n0761348000000733.jpg\n",
            "./n0761348000000735.jpg\n",
            "./n0761348000000737.jpg\n",
            "./n0761348000000741.jpg\n",
            "./n0761348000000742.jpg\n",
            "./n0761348000000743.jpg\n",
            "./n0761348000000744.jpg\n",
            "./n0761348000000746.jpg\n",
            "./n0761348000000748.jpg\n",
            "./n0761348000000754.jpg\n",
            "./n0761348000000755.jpg\n",
            "./n0761348000000757.jpg\n",
            "./n0761348000000760.jpg\n",
            "./n0761348000000765.jpg\n",
            "./n0761348000000766.jpg\n",
            "./n0761348000000768.jpg\n",
            "./n0761348000000769.jpg\n",
            "./n0761348000000770.jpg\n",
            "./n0761348000000772.jpg\n",
            "./n0761348000000775.jpg\n",
            "./n0761348000000778.jpg\n",
            "./n0761348000000779.jpg\n",
            "./n0761348000000783.jpg\n",
            "./n0761348000000784.jpg\n",
            "./n0761348000000789.jpg\n",
            "./n0761348000000792.jpg\n",
            "./n0761348000000794.jpg\n",
            "./n0761348000000800.jpg\n",
            "./n0761348000000801.jpg\n",
            "./n0761348000000803.jpg\n",
            "./n0761348000000807.jpg\n",
            "./n0761348000000808.jpg\n",
            "./n0761348000000811.jpg\n",
            "./n0761348000000818.jpg\n",
            "./n0761348000000821.jpg\n",
            "./n0761348000000824.jpg\n",
            "./n0761348000000825.jpg\n",
            "./n0761348000000827.jpg\n",
            "./n0761348000000829.jpg\n",
            "./n0761348000000832.jpg\n",
            "./n0761348000000833.jpg\n",
            "./n0761348000000835.jpg\n",
            "./n0761348000000838.jpg\n",
            "./n0761348000000840.jpg\n",
            "./n0761348000000843.jpg\n",
            "./n0761348000000844.jpg\n",
            "./n0761348000000845.jpg\n",
            "./n0761348000000849.jpg\n",
            "./n0761348000000850.jpg\n",
            "./n0761348000000852.jpg\n",
            "./n0761348000000856.jpg\n",
            "./n0761348000000859.jpg\n",
            "./n0761348000000860.jpg\n",
            "./n0761348000000861.jpg\n",
            "./n0761348000000862.jpg\n",
            "./n0761348000000863.jpg\n",
            "./n0761348000000864.jpg\n",
            "./n0761348000000867.jpg\n",
            "./n0761348000000868.jpg\n",
            "./n0761348000000869.jpg\n",
            "./n0761348000000871.jpg\n",
            "./n0761348000000873.jpg\n",
            "./n0761348000000875.jpg\n",
            "./n0761348000000881.jpg\n",
            "./n0761348000000882.jpg\n",
            "./n0761348000000888.jpg\n",
            "./n0761348000000889.jpg\n",
            "./n0761348000000890.jpg\n",
            "./n0761348000000892.jpg\n",
            "./n0761348000000894.jpg\n",
            "./n0761348000000895.jpg\n",
            "./n0761348000000896.jpg\n",
            "./n0761348000000897.jpg\n",
            "./n0761348000000902.jpg\n",
            "./n0761348000000904.jpg\n",
            "./n0761348000000908.jpg\n",
            "./n0761348000000910.jpg\n",
            "./n0761348000000914.jpg\n",
            "./n0761348000000915.jpg\n",
            "./n0761348000000916.jpg\n",
            "./n0761348000000918.jpg\n",
            "./n0761348000000919.jpg\n",
            "./n0761348000000920.jpg\n",
            "./n0761348000000923.jpg\n",
            "./n0761348000000925.jpg\n",
            "./n0761348000000926.jpg\n",
            "./n0761348000000927.jpg\n",
            "./n0761348000000933.jpg\n",
            "./n0761348000000935.jpg\n",
            "./n0761348000000936.jpg\n",
            "./n0761348000000941.jpg\n",
            "./n0761348000000943.jpg\n",
            "./n0761348000000946.jpg\n",
            "./n0761348000000949.jpg\n",
            "./n0761348000000950.jpg\n",
            "./n0761348000000951.jpg\n",
            "./n0761348000000954.jpg\n",
            "./n0761348000000955.jpg\n",
            "./n0761348000000959.jpg\n",
            "./n0761348000000965.jpg\n",
            "./n0761348000000969.jpg\n",
            "./n0761348000000971.jpg\n",
            "./n0761348000000973.jpg\n",
            "./n0761348000000974.jpg\n",
            "./n0761348000000976.jpg\n",
            "./n0761348000000978.jpg\n",
            "./n0761348000000979.jpg\n",
            "./n0761348000000981.jpg\n",
            "./n0761348000000982.jpg\n",
            "./n0761348000000988.jpg\n",
            "./n0761348000000991.jpg\n",
            "./n0761348000000995.jpg\n",
            "./n0761348000000997.jpg\n",
            "./n0761348000000998.jpg\n",
            "./n0761348000000999.jpg\n",
            "./n0761348000001004.jpg\n",
            "./n0761348000001009.jpg\n",
            "./n0761348000001014.jpg\n",
            "./n0761348000001018.jpg\n",
            "./n0761348000001019.jpg\n",
            "./n0761348000001020.jpg\n",
            "./n0761348000001022.jpg\n",
            "./n0761348000001025.jpg\n",
            "./n0761348000001028.jpg\n",
            "./n0761348000001029.jpg\n",
            "./n0761348000001033.jpg\n",
            "./n0761348000001034.jpg\n",
            "./n0761348000001035.jpg\n",
            "./n0761348000001039.jpg\n",
            "./n0761348000001046.jpg\n",
            "./n0761348000001051.jpg\n",
            "./n0761348000001053.jpg\n",
            "./n0761348000001055.jpg\n",
            "./n0761348000001056.jpg\n",
            "./n0761348000001059.jpg\n",
            "./n0761348000001060.jpg\n",
            "./n0761348000001061.jpg\n",
            "./n0761348000001064.jpg\n",
            "./n0761348000001067.jpg\n",
            "./n0761348000001068.jpg\n",
            "./n0761348000001072.jpg\n",
            "./n0761348000001073.jpg\n",
            "./n0761348000001076.jpg\n",
            "./n0761348000001077.jpg\n",
            "./n0761348000001080.jpg\n",
            "./n0761348000001082.jpg\n",
            "./n0761348000001083.jpg\n",
            "./n0761348000001088.jpg\n",
            "./n0761348000001089.jpg\n",
            "./n0761348000001090.jpg\n",
            "./n0761348000001092.jpg\n",
            "./n0761348000001095.jpg\n",
            "./n0761348000001096.jpg\n",
            "./n0761348000001097.jpg\n",
            "./n0761348000001098.jpg\n",
            "./n0761348000001099.jpg\n",
            "./n0761348000001100.jpg\n",
            "./n0761348000001104.jpg\n",
            "./n0761348000001105.jpg\n",
            "./n0761348000001106.jpg\n",
            "./n0761348000001111.jpg\n",
            "./n0761348000001112.jpg\n",
            "./n0761348000001114.jpg\n",
            "./n0761348000001115.jpg\n",
            "./n0761348000001118.jpg\n",
            "./n0761348000001119.jpg\n",
            "./n0761348000001121.jpg\n",
            "./n0761348000001122.jpg\n",
            "./n0761348000001124.jpg\n",
            "./n0761348000001125.jpg\n",
            "./n0761348000001128.jpg\n",
            "./n0761348000001130.jpg\n",
            "./n0761348000001132.jpg\n",
            "./n0761348000001134.jpg\n",
            "./n0761348000001136.jpg\n",
            "./n0761348000001137.jpg\n",
            "./n0761348000001142.jpg\n",
            "./n0761348000001144.jpg\n",
            "./n0761348000001145.jpg\n",
            "./n0761348000001150.jpg\n",
            "./n0761348000001151.jpg\n",
            "./n0761348000001152.jpg\n",
            "./n0761348000001153.jpg\n",
            "./n0761348000001157.jpg\n",
            "./n0761348000001159.jpg\n",
            "./n0761348000001160.jpg\n",
            "./n0761348000001162.jpg\n",
            "./n0761348000001163.jpg\n",
            "./n0761348000001164.jpg\n",
            "./n0761348000001167.jpg\n",
            "./n0761348000001168.jpg\n",
            "./n0761348000001176.jpg\n",
            "./n0761348000001177.jpg\n",
            "./n0761348000001178.jpg\n",
            "./n0761348000001180.jpg\n",
            "./n0761348000001182.jpg\n",
            "./n0761348000001183.jpg\n",
            "./n0761348000001187.jpg\n",
            "./n0761348000001189.jpg\n",
            "./n0761348000001190.jpg\n",
            "./n0761348000001191.jpg\n",
            "./n0761348000001192.jpg\n",
            "./n0761348000001193.jpg\n",
            "./n0761348000001197.jpg\n",
            "./n0761348000001198.jpg\n",
            "./n0761348000001199.jpg\n",
            "./n0761348000001200.jpg\n",
            "./n0761348000001202.jpg\n",
            "./n0761348000001204.jpg\n",
            "./n0761348000001205.jpg\n",
            "./n0761348000001207.jpg\n",
            "./n0761348000001215.jpg\n",
            "./n0761348000001221.jpg\n",
            "./n0761348000001225.jpg\n",
            "./n0761348000001229.jpg\n",
            "./n0761348000001234.jpg\n",
            "./n0761348000001237.jpg\n",
            "./n0761348000001238.jpg\n",
            "./n0761348000001239.jpg\n",
            "./n0761348000001240.jpg\n",
            "./n0761348000001242.jpg\n",
            "./n0761348000001245.jpg\n",
            "./n0761348000001247.jpg\n",
            "./n0761348000001249.jpg\n",
            "./n0761348000001252.jpg\n",
            "./n0761348000001254.jpg\n",
            "./n0761348000001260.jpg\n",
            "./n0761348000001264.jpg\n",
            "./n0761348000001265.jpg\n",
            "./n0761348000001268.jpg\n",
            "./n0761348000001271.jpg\n",
            "./n0761348000001272.jpg\n",
            "./n0761348000001273.jpg\n",
            "./n0761348000001274.jpg\n",
            "./n0761348000001276.jpg\n",
            "./n0761348000001277.jpg\n",
            "./n0761348000001278.jpg\n",
            "./n0761348000001279.jpg\n",
            "./n0761348000001281.jpg\n",
            "./n0761348000001282.jpg\n",
            "./n0761348000001284.jpg\n",
            "./n0761348000001288.jpg\n",
            "./n0761348000001289.jpg\n",
            "./n0761348000001290.jpg\n",
            "./n0761348000001292.jpg\n",
            "./n0761348000001293.jpg\n",
            "./n0761348000001294.jpg\n",
            "./n0761348000001296.jpg\n",
            "./n0761348000001297.jpg\n",
            "./n0761348000001298.jpg\n",
            "./n0769753700000001.jpg\n",
            "./n0769753700000006.jpg\n",
            "./n0769753700000008.jpg\n",
            "./n0769753700000010.jpg\n",
            "./n0769753700000011.jpg\n",
            "./n0769753700000013.jpg\n",
            "./n0769753700000014.jpg\n",
            "./n0769753700000015.jpg\n",
            "./n0769753700000017.jpg\n",
            "./n0769753700000018.jpg\n",
            "./n0769753700000022.jpg\n",
            "./n0769753700000027.jpg\n",
            "./n0769753700000029.jpg\n",
            "./n0769753700000031.jpg\n",
            "./n0769753700000033.jpg\n",
            "./n0769753700000036.jpg\n",
            "./n0769753700000045.jpg\n",
            "./n0769753700000046.jpg\n",
            "./n0769753700000047.jpg\n",
            "./n0769753700000048.jpg\n",
            "./n0769753700000055.jpg\n",
            "./n0769753700000058.jpg\n",
            "./n0769753700000060.jpg\n",
            "./n0769753700000061.jpg\n",
            "./n0769753700000064.jpg\n",
            "./n0769753700000068.jpg\n",
            "./n0769753700000070.jpg\n",
            "./n0769753700000074.jpg\n",
            "./n0769753700000075.jpg\n",
            "./n0769753700000079.jpg\n",
            "./n0769753700000081.jpg\n",
            "./n0769753700000083.jpg\n",
            "./n0769753700000084.jpg\n",
            "./n0769753700000087.jpg\n",
            "./n0769753700000088.jpg\n",
            "./n0769753700000091.jpg\n",
            "./n0769753700000092.jpg\n",
            "./n0769753700000097.jpg\n",
            "./n0769753700000099.jpg\n",
            "./n0769753700000100.jpg\n",
            "./n0769753700000101.jpg\n",
            "./n0769753700000104.jpg\n",
            "./n0769753700000105.jpg\n",
            "./n0769753700000106.jpg\n",
            "./n0769753700000109.jpg\n",
            "./n0769753700000114.jpg\n",
            "./n0769753700000115.jpg\n",
            "./n0769753700000120.jpg\n",
            "./n0769753700000125.jpg\n",
            "./n0769753700000126.jpg\n",
            "./n0769753700000131.jpg\n",
            "./n0769753700000132.jpg\n",
            "./n0769753700000133.jpg\n",
            "./n0769753700000137.jpg\n",
            "./n0769753700000138.jpg\n",
            "./n0769753700000141.jpg\n",
            "./n0769753700000142.jpg\n",
            "./n0769753700000143.jpg\n",
            "./n0769753700000144.jpg\n",
            "./n0769753700000145.jpg\n",
            "./n0769753700000148.jpg\n",
            "./n0769753700000151.jpg\n",
            "./n0769753700000154.jpg\n",
            "./n0769753700000155.jpg\n",
            "./n0769753700000156.jpg\n",
            "./n0769753700000157.jpg\n",
            "./n0769753700000158.jpg\n",
            "./n0769753700000159.jpg\n",
            "./n0769753700000162.jpg\n",
            "./n0769753700000163.jpg\n",
            "./n0769753700000166.jpg\n",
            "./n0769753700000169.jpg\n",
            "./n0769753700000171.jpg\n",
            "./n0769753700000174.jpg\n",
            "./n0769753700000175.jpg\n",
            "./n0769753700000176.jpg\n",
            "./n0769753700000177.jpg\n",
            "./n0769753700000179.jpg\n",
            "./n0769753700000181.jpg\n",
            "./n0769753700000183.jpg\n",
            "./n0769753700000184.jpg\n",
            "./n0769753700000185.jpg\n",
            "./n0769753700000194.jpg\n",
            "./n0769753700000197.jpg\n",
            "./n0769753700000199.jpg\n",
            "./n0769753700000202.jpg\n",
            "./n0769753700000209.jpg\n",
            "./n0769753700000211.jpg\n",
            "./n0769753700000212.jpg\n",
            "./n0769753700000214.jpg\n",
            "./n0769753700000216.jpg\n",
            "./n0769753700000217.jpg\n",
            "./n0769753700000218.jpg\n",
            "./n0769753700000220.jpg\n",
            "./n0769753700000221.jpg\n",
            "./n0769753700000223.jpg\n",
            "./n0769753700000226.jpg\n",
            "./n0769753700000227.jpg\n",
            "./n0769753700000229.jpg\n",
            "./n0769753700000231.jpg\n",
            "./n0769753700000232.jpg\n",
            "./n0769753700000233.jpg\n",
            "./n0769753700000235.jpg\n",
            "./n0769753700000237.jpg\n",
            "./n0769753700000240.jpg\n",
            "./n0769753700000242.jpg\n",
            "./n0769753700000245.jpg\n",
            "./n0769753700000249.jpg\n",
            "./n0769753700000250.jpg\n",
            "./n0769753700000251.jpg\n",
            "./n0769753700000257.jpg\n",
            "./n0769753700000258.jpg\n",
            "./n0769753700000259.jpg\n",
            "./n0769753700000261.jpg\n",
            "./n0769753700000262.jpg\n",
            "./n0769753700000263.jpg\n",
            "./n0769753700000267.jpg\n",
            "./n0769753700000271.jpg\n",
            "./n0769753700000272.jpg\n",
            "./n0769753700000273.jpg\n",
            "./n0769753700000275.jpg\n",
            "./n0769753700000278.jpg\n",
            "./n0769753700000280.jpg\n",
            "./n0769753700000282.jpg\n",
            "./n0769753700000283.jpg\n",
            "./n0769753700000287.jpg\n",
            "./n0769753700000288.jpg\n",
            "./n0769753700000293.jpg\n",
            "./n0769753700000294.jpg\n",
            "./n0769753700000296.jpg\n",
            "./n0769753700000301.jpg\n",
            "./n0769753700000302.jpg\n",
            "./n0769753700000303.jpg\n",
            "./n0769753700000304.jpg\n",
            "./n0769753700000309.jpg\n",
            "./n0769753700000311.jpg\n",
            "./n0769753700000312.jpg\n",
            "./n0769753700000313.jpg\n",
            "./n0769753700000316.jpg\n",
            "./n0769753700000320.jpg\n",
            "./n0769753700000321.jpg\n",
            "./n0769753700000322.jpg\n",
            "./n0769753700000323.jpg\n",
            "./n0769753700000324.jpg\n",
            "./n0769753700000326.jpg\n",
            "./n0769753700000327.jpg\n",
            "./n0769753700000328.jpg\n",
            "./n0769753700000331.jpg\n",
            "./n0769753700000335.jpg\n",
            "./n0769753700000337.jpg\n",
            "./n0769753700000339.jpg\n",
            "./n0769753700000341.jpg\n",
            "./n0769753700000342.jpg\n",
            "./n0769753700000343.jpg\n",
            "./n0769753700000344.jpg\n",
            "./n0769753700000345.jpg\n",
            "./n0769753700000347.jpg\n",
            "./n0769753700000349.jpg\n",
            "./n0769753700000351.jpg\n",
            "./n0769753700000355.jpg\n",
            "./n0769753700000357.jpg\n",
            "./n0769753700000363.jpg\n",
            "./n0769753700000364.jpg\n",
            "./n0769753700000366.jpg\n",
            "./n0769753700000369.jpg\n",
            "./n0769753700000370.jpg\n",
            "./n0769753700000371.jpg\n",
            "./n0769753700000374.jpg\n",
            "./n0769753700000377.jpg\n",
            "./n0769753700000382.jpg\n",
            "./n0769753700000385.jpg\n",
            "./n0769753700000386.jpg\n",
            "./n0769753700000387.jpg\n",
            "./n0769753700000391.jpg\n",
            "./n0769753700000393.jpg\n",
            "./n0769753700000394.jpg\n",
            "./n0769753700000395.jpg\n",
            "./n0769753700000396.jpg\n",
            "./n0769753700000397.jpg\n",
            "./n0769753700000398.jpg\n",
            "./n0769753700000399.jpg\n",
            "./n0769753700000400.jpg\n",
            "./n0769753700000405.jpg\n",
            "./n0769753700000406.jpg\n",
            "./n0769753700000409.jpg\n",
            "./n0769753700000413.jpg\n",
            "./n0769753700000417.jpg\n",
            "./n0769753700000419.jpg\n",
            "./n0769753700000421.jpg\n",
            "./n0769753700000424.jpg\n",
            "./n0769753700000430.jpg\n",
            "./n0769753700000432.jpg\n",
            "./n0769753700000436.jpg\n",
            "./n0769753700000437.jpg\n",
            "./n0769753700000441.jpg\n",
            "./n0769753700000442.jpg\n",
            "./n0769753700000444.jpg\n",
            "./n0769753700000446.jpg\n",
            "./n0769753700000447.jpg\n",
            "./n0769753700000448.jpg\n",
            "./n0769753700000450.jpg\n",
            "./n0769753700000451.jpg\n",
            "./n0769753700000452.jpg\n",
            "./n0769753700000453.jpg\n",
            "./n0769753700000455.jpg\n",
            "./n0769753700000459.jpg\n",
            "./n0769753700000460.jpg\n",
            "./n0769753700000461.jpg\n",
            "./n0769753700000462.jpg\n",
            "./n0769753700000464.jpg\n",
            "./n0769753700000466.jpg\n",
            "./n0769753700000469.jpg\n",
            "./n0769753700000470.jpg\n",
            "./n0769753700000472.jpg\n",
            "./n0769753700000473.jpg\n",
            "./n0769753700000475.jpg\n",
            "./n0769753700000476.jpg\n",
            "./n0769753700000478.jpg\n",
            "./n0769753700000480.jpg\n",
            "./n0769753700000481.jpg\n",
            "./n0769753700000482.jpg\n",
            "./n0769753700000485.jpg\n",
            "./n0769753700000489.jpg\n",
            "./n0769753700000490.jpg\n",
            "./n0769753700000493.jpg\n",
            "./n0769753700000494.jpg\n",
            "./n0769753700000495.jpg\n",
            "./n0769753700000498.jpg\n",
            "./n0769753700000499.jpg\n",
            "./n0769753700000501.jpg\n",
            "./n0769753700000507.jpg\n",
            "./n0769753700000508.jpg\n",
            "./n0769753700000509.jpg\n",
            "./n0769753700000510.jpg\n",
            "./n0769753700000511.jpg\n",
            "./n0769753700000512.jpg\n",
            "./n0769753700000516.jpg\n",
            "./n0769753700000520.jpg\n",
            "./n0769753700000526.jpg\n",
            "./n0769753700000527.jpg\n",
            "./n0769753700000530.jpg\n",
            "./n0769753700000535.jpg\n",
            "./n0769753700000537.jpg\n",
            "./n0769753700000538.jpg\n",
            "./n0769753700000543.jpg\n",
            "./n0769753700000545.jpg\n",
            "./n0769753700000546.jpg\n",
            "./n0769753700000547.jpg\n",
            "./n0769753700000548.jpg\n",
            "./n0769753700000554.jpg\n",
            "./n0769753700000558.jpg\n",
            "./n0769753700000559.jpg\n",
            "./n0769753700000560.jpg\n",
            "./n0769753700000562.jpg\n",
            "./n0769753700000566.jpg\n",
            "./n0769753700000567.jpg\n",
            "./n0769753700000569.jpg\n",
            "./n0769753700000572.jpg\n",
            "./n0769753700000573.jpg\n",
            "./n0769753700000574.jpg\n",
            "./n0769753700000575.jpg\n",
            "./n0769753700000576.jpg\n",
            "./n0769753700000577.jpg\n",
            "./n0769753700000578.jpg\n",
            "./n0769753700000585.jpg\n",
            "./n0769753700000586.jpg\n",
            "./n0769753700000588.jpg\n",
            "./n0769753700000589.jpg\n",
            "./n0769753700000590.jpg\n",
            "./n0769753700000591.jpg\n",
            "./n0769753700000593.jpg\n",
            "./n0769753700000597.jpg\n",
            "./n0769753700000598.jpg\n",
            "./n0769753700000599.jpg\n",
            "./n0769753700000601.jpg\n",
            "./n0769753700000603.jpg\n",
            "./n0769753700000604.jpg\n",
            "./n0769753700000606.jpg\n",
            "./n0769753700000609.jpg\n",
            "./n0769753700000610.jpg\n",
            "./n0769753700000614.jpg\n",
            "./n0769753700000617.jpg\n",
            "./n0769753700000619.jpg\n",
            "./n0769753700000621.jpg\n",
            "./n0769753700000622.jpg\n",
            "./n0769753700000624.jpg\n",
            "./n0769753700000625.jpg\n",
            "./n0769753700000626.jpg\n",
            "./n0769753700000629.jpg\n",
            "./n0769753700000633.jpg\n",
            "./n0769753700000634.jpg\n",
            "./n0769753700000637.jpg\n",
            "./n0769753700000638.jpg\n",
            "./n0769753700000639.jpg\n",
            "./n0769753700000640.jpg\n",
            "./n0769753700000642.jpg\n",
            "./n0769753700000646.jpg\n",
            "./n0769753700000648.jpg\n",
            "./n0769753700000650.jpg\n",
            "./n0769753700000651.jpg\n",
            "./n0769753700000652.jpg\n",
            "./n0769753700000658.jpg\n",
            "./n0769753700000659.jpg\n",
            "./n0769753700000662.jpg\n",
            "./n0769753700000666.jpg\n",
            "./n0769753700000668.jpg\n",
            "./n0769753700000673.jpg\n",
            "./n0769753700000674.jpg\n",
            "./n0769753700000675.jpg\n",
            "./n0769753700000677.jpg\n",
            "./n0769753700000678.jpg\n",
            "./n0769753700000679.jpg\n",
            "./n0769753700000680.jpg\n",
            "./n0769753700000681.jpg\n",
            "./n0769753700000683.jpg\n",
            "./n0769753700000685.jpg\n",
            "./n0769753700000686.jpg\n",
            "./n0769753700000687.jpg\n",
            "./n0769753700000688.jpg\n",
            "./n0769753700000690.jpg\n",
            "./n0769753700000691.jpg\n",
            "./n0769753700000692.jpg\n",
            "./n0769753700000695.jpg\n",
            "./n0769753700000701.jpg\n",
            "./n0769753700000702.jpg\n",
            "./n0769753700000703.jpg\n",
            "./n0769753700000704.jpg\n",
            "./n0769753700000706.jpg\n",
            "./n0769753700000707.jpg\n",
            "./n0769753700000708.jpg\n",
            "./n0769753700000710.jpg\n",
            "./n0769753700000712.jpg\n",
            "./n0769753700000713.jpg\n",
            "./n0769753700000714.jpg\n",
            "./n0769753700000715.jpg\n",
            "./n0769753700000717.jpg\n",
            "./n0769753700000718.jpg\n",
            "./n0769753700000719.jpg\n",
            "./n0769753700000720.jpg\n",
            "./n0769753700000721.jpg\n",
            "./n0769753700000722.jpg\n",
            "./n0769753700000724.jpg\n",
            "./n0769753700000725.jpg\n",
            "./n0769753700000729.jpg\n",
            "./n0769753700000731.jpg\n",
            "./n0769753700000732.jpg\n",
            "./n0769753700000736.jpg\n",
            "./n0769753700000738.jpg\n",
            "./n0769753700000743.jpg\n",
            "./n0769753700000744.jpg\n",
            "./n0769753700000746.jpg\n",
            "./n0769753700000748.jpg\n",
            "./n0769753700000749.jpg\n",
            "./n0769753700000756.jpg\n",
            "./n0769753700000759.jpg\n",
            "./n0769753700000760.jpg\n",
            "./n0769753700000765.jpg\n",
            "./n0769753700000766.jpg\n",
            "./n0769753700000769.jpg\n",
            "./n0769753700000770.jpg\n",
            "./n0769753700000772.jpg\n",
            "./n0769753700000774.jpg\n",
            "./n0769753700000776.jpg\n",
            "./n0769753700000778.jpg\n",
            "./n0769753700000779.jpg\n",
            "./n0769753700000781.jpg\n",
            "./n0769753700000783.jpg\n",
            "./n0769753700000785.jpg\n",
            "./n0769753700000787.jpg\n",
            "./n0769753700000790.jpg\n",
            "./n0769753700000793.jpg\n",
            "./n0769753700000794.jpg\n",
            "./n0769753700000795.jpg\n",
            "./n0769753700000796.jpg\n",
            "./n0769753700000799.jpg\n",
            "./n0769753700000801.jpg\n",
            "./n0769753700000802.jpg\n",
            "./n0769753700000805.jpg\n",
            "./n0769753700000810.jpg\n",
            "./n0769753700000813.jpg\n",
            "./n0769753700000815.jpg\n",
            "./n0769753700000820.jpg\n",
            "./n0769753700000821.jpg\n",
            "./n0769753700000823.jpg\n",
            "./n0769753700000826.jpg\n",
            "./n0769753700000829.jpg\n",
            "./n0769753700000831.jpg\n",
            "./n0769753700000833.jpg\n",
            "./n0769753700000834.jpg\n",
            "./n0769753700000838.jpg\n",
            "./n0769753700000839.jpg\n",
            "./n0769753700000840.jpg\n",
            "./n0769753700000843.jpg\n",
            "./n0769753700000844.jpg\n",
            "./n0769753700000849.jpg\n",
            "./n0769753700000850.jpg\n",
            "./n0769753700000853.jpg\n",
            "./n0769753700000855.jpg\n",
            "./n0769753700000857.jpg\n",
            "./n0769753700000860.jpg\n",
            "./n0769753700000861.jpg\n",
            "./n0769753700000862.jpg\n",
            "./n0769753700000863.jpg\n",
            "./n0769753700000864.jpg\n",
            "./n0769753700000867.jpg\n",
            "./n0769753700000870.jpg\n",
            "./n0769753700000871.jpg\n",
            "./n0769753700000872.jpg\n",
            "./n0769753700000873.jpg\n",
            "./n0769753700000883.jpg\n",
            "./n0769753700000886.jpg\n",
            "./n0769753700000887.jpg\n",
            "./n0769753700000888.jpg\n",
            "./n0769753700000890.jpg\n",
            "./n0769753700000891.jpg\n",
            "./n0769753700000892.jpg\n",
            "./n0769753700000894.jpg\n",
            "./n0769753700000895.jpg\n",
            "./n0769753700000896.jpg\n",
            "./n0769753700000901.jpg\n",
            "./n0769753700000902.jpg\n",
            "./n0769753700000903.jpg\n",
            "./n0769753700000906.jpg\n",
            "./n0769753700000908.jpg\n",
            "./n0769753700000909.jpg\n",
            "./n0769753700000910.jpg\n",
            "./n0769753700000911.jpg\n",
            "./n0769753700000915.jpg\n",
            "./n0769753700000918.jpg\n",
            "./n0769753700000921.jpg\n",
            "./n0769753700000923.jpg\n",
            "./n0769753700000924.jpg\n",
            "./n0769753700000925.jpg\n",
            "./n0769753700000927.jpg\n",
            "./n0769753700000928.jpg\n",
            "./n0769753700000929.jpg\n",
            "./n0769753700000930.jpg\n",
            "./n0769753700000932.jpg\n",
            "./n0769753700000934.jpg\n",
            "./n0769753700000938.jpg\n",
            "./n0769753700000940.jpg\n",
            "./n0769753700000945.jpg\n",
            "./n0769753700000947.jpg\n",
            "./n0769753700000951.jpg\n",
            "./n0769753700000952.jpg\n",
            "./n0769753700000956.jpg\n",
            "./n0769753700000963.jpg\n",
            "./n0769753700000966.jpg\n",
            "./n0769753700000968.jpg\n",
            "./n0769753700000969.jpg\n",
            "./n0769753700000971.jpg\n",
            "./n0769753700000972.jpg\n",
            "./n0769753700000973.jpg\n",
            "./n0769753700000978.jpg\n",
            "./n0769753700000981.jpg\n",
            "./n0769753700000982.jpg\n",
            "./n0769753700000983.jpg\n",
            "./n0769753700000990.jpg\n",
            "./n0769753700000991.jpg\n",
            "./n0769753700000992.jpg\n",
            "./n0769753700000994.jpg\n",
            "./n0769753700000999.jpg\n",
            "./n0769753700001000.jpg\n",
            "./n0769753700001001.jpg\n",
            "./n0769753700001002.jpg\n",
            "./n0769753700001005.jpg\n",
            "./n0769753700001008.jpg\n",
            "./n0769753700001011.jpg\n",
            "./n0769753700001012.jpg\n",
            "./n0769753700001021.jpg\n",
            "./n0769753700001031.jpg\n",
            "./n0769753700001033.jpg\n",
            "./n0769753700001035.jpg\n",
            "./n0769753700001036.jpg\n",
            "./n0769753700001037.jpg\n",
            "./n0769753700001038.jpg\n",
            "./n0769753700001044.jpg\n",
            "./n0769753700001045.jpg\n",
            "./n0769753700001048.jpg\n",
            "./n0769753700001049.jpg\n",
            "./n0769753700001051.jpg\n",
            "./n0769753700001052.jpg\n",
            "./n0769753700001053.jpg\n",
            "./n0769753700001054.jpg\n",
            "./n0769753700001057.jpg\n",
            "./n0769753700001062.jpg\n",
            "./n0769753700001065.jpg\n",
            "./n0769753700001068.jpg\n",
            "./n0769753700001070.jpg\n",
            "./n0769753700001071.jpg\n",
            "./n0769753700001074.jpg\n",
            "./n0769753700001076.jpg\n",
            "./n0769753700001077.jpg\n",
            "./n0769753700001081.jpg\n",
            "./n0769753700001082.jpg\n",
            "./n0769753700001083.jpg\n",
            "./n0769753700001087.jpg\n",
            "./n0769753700001088.jpg\n",
            "./n0769753700001090.jpg\n",
            "./n0769753700001092.jpg\n",
            "./n0769753700001093.jpg\n",
            "./n0769753700001094.jpg\n",
            "./n0769753700001095.jpg\n",
            "./n0769753700001096.jpg\n",
            "./n0769753700001099.jpg\n",
            "./n0769753700001100.jpg\n",
            "./n0769753700001103.jpg\n",
            "./n0769753700001104.jpg\n",
            "./n0769753700001106.jpg\n",
            "./n0769753700001107.jpg\n",
            "./n0769753700001108.jpg\n",
            "./n0769753700001109.jpg\n",
            "./n0769753700001111.jpg\n",
            "./n0769753700001113.jpg\n",
            "./n0769753700001116.jpg\n",
            "./n0769753700001117.jpg\n",
            "./n0769753700001119.jpg\n",
            "./n0769753700001121.jpg\n",
            "./n0769753700001125.jpg\n",
            "./n0769753700001127.jpg\n",
            "./n0769753700001130.jpg\n",
            "./n0769753700001133.jpg\n",
            "./n0769753700001135.jpg\n",
            "./n0769753700001136.jpg\n",
            "./n0769753700001137.jpg\n",
            "./n0769753700001138.jpg\n",
            "./n0769753700001141.jpg\n",
            "./n0769753700001143.jpg\n",
            "./n0769753700001144.jpg\n",
            "./n0769753700001146.jpg\n",
            "./n0769753700001149.jpg\n",
            "./n0769753700001151.jpg\n",
            "./n0769753700001153.jpg\n",
            "./n0769753700001154.jpg\n",
            "./n0769753700001155.jpg\n",
            "./n0769753700001159.jpg\n",
            "./n0769753700001160.jpg\n",
            "./n0769753700001161.jpg\n",
            "./n0769753700001163.jpg\n",
            "./n0769753700001164.jpg\n",
            "./n0769753700001165.jpg\n",
            "./n0769753700001167.jpg\n",
            "./n0769753700001170.jpg\n",
            "./n0769753700001171.jpg\n",
            "./n0769753700001172.jpg\n",
            "./n0769753700001173.jpg\n",
            "./n0769753700001174.jpg\n",
            "./n0769753700001175.jpg\n",
            "./n0769753700001179.jpg\n",
            "./n0769753700001180.jpg\n",
            "./n0769753700001182.jpg\n",
            "./n0769753700001184.jpg\n",
            "./n0769753700001186.jpg\n",
            "./n0769753700001189.jpg\n",
            "./n0769753700001190.jpg\n",
            "./n0769753700001193.jpg\n",
            "./n0769753700001194.jpg\n",
            "./n0769753700001199.jpg\n",
            "./n0769753700001200.jpg\n",
            "./n0769753700001202.jpg\n",
            "./n0769753700001203.jpg\n",
            "./n0769753700001208.jpg\n",
            "./n0769753700001209.jpg\n",
            "./n0769753700001210.jpg\n",
            "./n0769753700001214.jpg\n",
            "./n0769753700001219.jpg\n",
            "./n0769753700001221.jpg\n",
            "./n0769753700001222.jpg\n",
            "./n0769753700001224.jpg\n",
            "./n0769753700001225.jpg\n",
            "./n0769753700001227.jpg\n",
            "./n0769753700001228.jpg\n",
            "./n0769753700001231.jpg\n",
            "./n0769753700001234.jpg\n",
            "./n0769753700001239.jpg\n",
            "./n0769753700001240.jpg\n",
            "./n0769753700001242.jpg\n",
            "./n0769753700001248.jpg\n",
            "./n0769753700001249.jpg\n",
            "./n0769753700001254.jpg\n",
            "./n0769753700001255.jpg\n",
            "./n0769753700001256.jpg\n",
            "./n0769753700001260.jpg\n",
            "./n0769753700001263.jpg\n",
            "./n0769753700001265.jpg\n",
            "./n0769753700001268.jpg\n",
            "./n0769753700001283.jpg\n",
            "./n0769753700001284.jpg\n",
            "./n0769753700001285.jpg\n",
            "./n0769753700001286.jpg\n",
            "./n0769753700001287.jpg\n",
            "./n0769753700001288.jpg\n",
            "./n0769753700001290.jpg\n",
            "./n0769753700001291.jpg\n",
            "./n0769753700001294.jpg\n",
            "./n0769753700001295.jpg\n",
            "./n0769753700001296.jpg\n",
            "./n0769753700001298.jpg\n",
            "./n0769753700001299.jpg\n",
            "./n0769753700001300.jpg\n",
            "./n0774760700000001.jpg\n",
            "./n0774760700000006.jpg\n",
            "./n0774760700000007.jpg\n",
            "./n0774760700000008.jpg\n",
            "./n0774760700000012.jpg\n",
            "./n0774760700000014.jpg\n",
            "./n0774760700000015.jpg\n",
            "./n0774760700000017.jpg\n",
            "./n0774760700000019.jpg\n",
            "./n0774760700000023.jpg\n",
            "./n0774760700000026.jpg\n",
            "./n0774760700000030.jpg\n",
            "./n0774760700000031.jpg\n",
            "./n0774760700000032.jpg\n",
            "./n0774760700000033.jpg\n",
            "./n0774760700000034.jpg\n",
            "./n0774760700000037.jpg\n",
            "./n0774760700000039.jpg\n",
            "./n0774760700000042.jpg\n",
            "./n0774760700000047.jpg\n",
            "./n0774760700000049.jpg\n",
            "./n0774760700000053.jpg\n",
            "./n0774760700000057.jpg\n",
            "./n0774760700000059.jpg\n",
            "./n0774760700000060.jpg\n",
            "./n0774760700000062.jpg\n",
            "./n0774760700000064.jpg\n",
            "./n0774760700000065.jpg\n",
            "./n0774760700000066.jpg\n",
            "./n0774760700000067.jpg\n",
            "./n0774760700000071.jpg\n",
            "./n0774760700000073.jpg\n",
            "./n0774760700000076.jpg\n",
            "./n0774760700000082.jpg\n",
            "./n0774760700000083.jpg\n",
            "./n0774760700000087.jpg\n",
            "./n0774760700000088.jpg\n",
            "./n0774760700000090.jpg\n",
            "./n0774760700000091.jpg\n",
            "./n0774760700000094.jpg\n",
            "./n0774760700000098.jpg\n",
            "./n0774760700000102.jpg\n",
            "./n0774760700000103.jpg\n",
            "./n0774760700000104.jpg\n",
            "./n0774760700000105.jpg\n",
            "./n0774760700000107.jpg\n",
            "./n0774760700000111.jpg\n",
            "./n0774760700000113.jpg\n",
            "./n0774760700000117.jpg\n",
            "./n0774760700000119.jpg\n",
            "./n0774760700000120.jpg\n",
            "./n0774760700000122.jpg\n",
            "./n0774760700000125.jpg\n",
            "./n0774760700000128.jpg\n",
            "./n0774760700000129.jpg\n",
            "./n0774760700000131.jpg\n",
            "./n0774760700000132.jpg\n",
            "./n0774760700000133.jpg\n",
            "./n0774760700000136.jpg\n",
            "./n0774760700000140.jpg\n",
            "./n0774760700000143.jpg\n",
            "./n0774760700000145.jpg\n",
            "./n0774760700000150.jpg\n",
            "./n0774760700000153.jpg\n",
            "./n0774760700000157.jpg\n",
            "./n0774760700000158.jpg\n",
            "./n0774760700000159.jpg\n",
            "./n0774760700000160.jpg\n",
            "./n0774760700000163.jpg\n",
            "./n0774760700000164.jpg\n",
            "./n0774760700000165.jpg\n",
            "./n0774760700000166.jpg\n",
            "./n0774760700000167.jpg\n",
            "./n0774760700000171.jpg\n",
            "./n0774760700000172.jpg\n",
            "./n0774760700000173.jpg\n",
            "./n0774760700000178.jpg\n",
            "./n0774760700000179.jpg\n",
            "./n0774760700000182.jpg\n",
            "./n0774760700000184.jpg\n",
            "./n0774760700000187.jpg\n",
            "./n0774760700000188.jpg\n",
            "./n0774760700000190.jpg\n",
            "./n0774760700000193.jpg\n",
            "./n0774760700000196.jpg\n",
            "./n0774760700000198.jpg\n",
            "./n0774760700000199.jpg\n",
            "./n0774760700000200.jpg\n",
            "./n0774760700000201.jpg\n",
            "./n0774760700000207.jpg\n",
            "./n0774760700000209.jpg\n",
            "./n0774760700000210.jpg\n",
            "./n0774760700000215.jpg\n",
            "./n0774760700000216.jpg\n",
            "./n0774760700000217.jpg\n",
            "./n0774760700000219.jpg\n",
            "./n0774760700000226.jpg\n",
            "./n0774760700000230.jpg\n",
            "./n0774760700000234.jpg\n",
            "./n0774760700000236.jpg\n",
            "./n0774760700000237.jpg\n",
            "./n0774760700000238.jpg\n",
            "./n0774760700000244.jpg\n",
            "./n0774760700000246.jpg\n",
            "./n0774760700000249.jpg\n",
            "./n0774760700000252.jpg\n",
            "./n0774760700000253.jpg\n",
            "./n0774760700000254.jpg\n",
            "./n0774760700000256.jpg\n",
            "./n0774760700000258.jpg\n",
            "./n0774760700000260.jpg\n",
            "./n0774760700000262.jpg\n",
            "./n0774760700000264.jpg\n",
            "./n0774760700000266.jpg\n",
            "./n0774760700000268.jpg\n",
            "./n0774760700000269.jpg\n",
            "./n0774760700000273.jpg\n",
            "./n0774760700000275.jpg\n",
            "./n0774760700000276.jpg\n",
            "./n0774760700000277.jpg\n",
            "./n0774760700000278.jpg\n",
            "./n0774760700000280.jpg\n",
            "./n0774760700000281.jpg\n",
            "./n0774760700000283.jpg\n",
            "./n0774760700000285.jpg\n",
            "./n0774760700000287.jpg\n",
            "./n0774760700000289.jpg\n",
            "./n0774760700000291.jpg\n",
            "./n0774760700000292.jpg\n",
            "./n0774760700000295.jpg\n",
            "./n0774760700000296.jpg\n",
            "./n0774760700000297.jpg\n",
            "./n0774760700000298.jpg\n",
            "./n0774760700000299.jpg\n",
            "./n0774760700000305.jpg\n",
            "./n0774760700000306.jpg\n",
            "./n0774760700000309.jpg\n",
            "./n0774760700000311.jpg\n",
            "./n0774760700000312.jpg\n",
            "./n0774760700000314.jpg\n",
            "./n0774760700000315.jpg\n",
            "./n0774760700000317.jpg\n",
            "./n0774760700000318.jpg\n",
            "./n0774760700000320.jpg\n",
            "./n0774760700000321.jpg\n",
            "./n0774760700000325.jpg\n",
            "./n0774760700000326.jpg\n",
            "./n0774760700000327.jpg\n",
            "./n0774760700000328.jpg\n",
            "./n0774760700000330.jpg\n",
            "./n0774760700000334.jpg\n",
            "./n0774760700000336.jpg\n",
            "./n0774760700000345.jpg\n",
            "./n0774760700000349.jpg\n",
            "./n0774760700000352.jpg\n",
            "./n0774760700000357.jpg\n",
            "./n0774760700000358.jpg\n",
            "./n0774760700000359.jpg\n",
            "./n0774760700000365.jpg\n",
            "./n0774760700000369.jpg\n",
            "./n0774760700000370.jpg\n",
            "./n0774760700000373.jpg\n",
            "./n0774760700000374.jpg\n",
            "./n0774760700000375.jpg\n",
            "./n0774760700000377.jpg\n",
            "./n0774760700000378.jpg\n",
            "./n0774760700000383.jpg\n",
            "./n0774760700000384.jpg\n",
            "./n0774760700000388.jpg\n",
            "./n0774760700000390.jpg\n",
            "./n0774760700000394.jpg\n",
            "./n0774760700000395.jpg\n",
            "./n0774760700000399.jpg\n",
            "./n0774760700000400.jpg\n",
            "./n0774760700000401.jpg\n",
            "./n0774760700000405.jpg\n",
            "./n0774760700000406.jpg\n",
            "./n0774760700000410.jpg\n",
            "./n0774760700000411.jpg\n",
            "./n0774760700000413.jpg\n",
            "./n0774760700000415.jpg\n",
            "./n0774760700000416.jpg\n",
            "./n0774760700000419.jpg\n",
            "./n0774760700000421.jpg\n",
            "./n0774760700000422.jpg\n",
            "./n0774760700000423.jpg\n",
            "./n0774760700000424.jpg\n",
            "./n0774760700000425.jpg\n",
            "./n0774760700000426.jpg\n",
            "./n0774760700000429.jpg\n",
            "./n0774760700000431.jpg\n",
            "./n0774760700000432.jpg\n",
            "./n0774760700000433.jpg\n",
            "./n0774760700000435.jpg\n",
            "./n0774760700000440.jpg\n",
            "./n0774760700000441.jpg\n",
            "./n0774760700000443.jpg\n",
            "./n0774760700000444.jpg\n",
            "./n0774760700000445.jpg\n",
            "./n0774760700000447.jpg\n",
            "./n0774760700000448.jpg\n",
            "./n0774760700000449.jpg\n",
            "./n0774760700000450.jpg\n",
            "./n0774760700000451.jpg\n",
            "./n0774760700000452.jpg\n",
            "./n0774760700000453.jpg\n",
            "./n0774760700000454.jpg\n",
            "./n0774760700000455.jpg\n",
            "./n0774760700000457.jpg\n",
            "./n0774760700000458.jpg\n",
            "./n0774760700000460.jpg\n",
            "./n0774760700000461.jpg\n",
            "./n0774760700000462.jpg\n",
            "./n0774760700000469.jpg\n",
            "./n0774760700000471.jpg\n",
            "./n0774760700000472.jpg\n",
            "./n0774760700000474.jpg\n",
            "./n0774760700000475.jpg\n",
            "./n0774760700000479.jpg\n",
            "./n0774760700000481.jpg\n",
            "./n0774760700000482.jpg\n",
            "./n0774760700000485.jpg\n",
            "./n0774760700000487.jpg\n",
            "./n0774760700000488.jpg\n",
            "./n0774760700000489.jpg\n",
            "./n0774760700000492.jpg\n",
            "./n0774760700000493.jpg\n",
            "./n0774760700000494.jpg\n",
            "./n0774760700000496.jpg\n",
            "./n0774760700000497.jpg\n",
            "./n0774760700000499.jpg\n",
            "./n0774760700000502.jpg\n",
            "./n0774760700000503.jpg\n",
            "./n0774760700000506.jpg\n",
            "./n0774760700000507.jpg\n",
            "./n0774760700000508.jpg\n",
            "./n0774760700000510.jpg\n",
            "./n0774760700000512.jpg\n",
            "./n0774760700000513.jpg\n",
            "./n0774760700000517.jpg\n",
            "./n0774760700000518.jpg\n",
            "./n0774760700000519.jpg\n",
            "./n0774760700000521.jpg\n",
            "./n0774760700000522.jpg\n",
            "./n0774760700000524.jpg\n",
            "./n0774760700000525.jpg\n",
            "./n0774760700000527.jpg\n",
            "./n0774760700000529.jpg\n",
            "./n0774760700000532.jpg\n",
            "./n0774760700000534.jpg\n",
            "./n0774760700000536.jpg\n",
            "./n0774760700000537.jpg\n",
            "./n0774760700000539.jpg\n",
            "./n0774760700000540.jpg\n",
            "./n0774760700000543.jpg\n",
            "./n0774760700000545.jpg\n",
            "./n0774760700000547.jpg\n",
            "./n0774760700000549.jpg\n",
            "./n0774760700000550.jpg\n",
            "./n0774760700000553.jpg\n",
            "./n0774760700000554.jpg\n",
            "./n0774760700000556.jpg\n",
            "./n0774760700000557.jpg\n",
            "./n0774760700000562.jpg\n",
            "./n0774760700000564.jpg\n",
            "./n0774760700000566.jpg\n",
            "./n0774760700000567.jpg\n",
            "./n0774760700000572.jpg\n",
            "./n0774760700000573.jpg\n",
            "./n0774760700000574.jpg\n",
            "./n0774760700000576.jpg\n",
            "./n0774760700000577.jpg\n",
            "./n0774760700000578.jpg\n",
            "./n0774760700000579.jpg\n",
            "./n0774760700000580.jpg\n",
            "./n0774760700000583.jpg\n",
            "./n0774760700000585.jpg\n",
            "./n0774760700000588.jpg\n",
            "./n0774760700000590.jpg\n",
            "./n0774760700000591.jpg\n",
            "./n0774760700000594.jpg\n",
            "./n0774760700000595.jpg\n",
            "./n0774760700000599.jpg\n",
            "./n0774760700000601.jpg\n",
            "./n0774760700000603.jpg\n",
            "./n0774760700000612.jpg\n",
            "./n0774760700000615.jpg\n",
            "./n0774760700000616.jpg\n",
            "./n0774760700000617.jpg\n",
            "./n0774760700000618.jpg\n",
            "./n0774760700000620.jpg\n",
            "./n0774760700000621.jpg\n",
            "./n0774760700000622.jpg\n",
            "./n0774760700000630.jpg\n",
            "./n0774760700000634.jpg\n",
            "./n0774760700000635.jpg\n",
            "./n0774760700000636.jpg\n",
            "./n0774760700000637.jpg\n",
            "./n0774760700000640.jpg\n",
            "./n0774760700000641.jpg\n",
            "./n0774760700000643.jpg\n",
            "./n0774760700000644.jpg\n",
            "./n0774760700000648.jpg\n",
            "./n0774760700000649.jpg\n",
            "./n0774760700000651.jpg\n",
            "./n0774760700000652.jpg\n",
            "./n0774760700000653.jpg\n",
            "./n0774760700000660.jpg\n",
            "./n0774760700000661.jpg\n",
            "./n0774760700000668.jpg\n",
            "./n0774760700000672.jpg\n",
            "./n0774760700000675.jpg\n",
            "./n0774760700000678.jpg\n",
            "./n0774760700000680.jpg\n",
            "./n0774760700000681.jpg\n",
            "./n0774760700000684.jpg\n",
            "./n0774760700000686.jpg\n",
            "./n0774760700000689.jpg\n",
            "./n0774760700000691.jpg\n",
            "./n0774760700000693.jpg\n",
            "./n0774760700000695.jpg\n",
            "./n0774760700000696.jpg\n",
            "./n0774760700000699.jpg\n",
            "./n0774760700000705.jpg\n",
            "./n0774760700000710.jpg\n",
            "./n0774760700000712.jpg\n",
            "./n0774760700000714.jpg\n",
            "./n0774760700000716.jpg\n",
            "./n0774760700000717.jpg\n",
            "./n0774760700000719.jpg\n",
            "./n0774760700000720.jpg\n",
            "./n0774760700000722.jpg\n",
            "./n0774760700000723.jpg\n",
            "./n0774760700000724.jpg\n",
            "./n0774760700000726.jpg\n",
            "./n0774760700000727.jpg\n",
            "./n0774760700000728.jpg\n",
            "./n0774760700000729.jpg\n",
            "./n0774760700000731.jpg\n",
            "./n0774760700000736.jpg\n",
            "./n0774760700000737.jpg\n",
            "./n0774760700000739.jpg\n",
            "./n0774760700000740.jpg\n",
            "./n0774760700000742.jpg\n",
            "./n0774760700000748.jpg\n",
            "./n0774760700000750.jpg\n",
            "./n0774760700000751.jpg\n",
            "./n0774760700000752.jpg\n",
            "./n0774760700000754.jpg\n",
            "./n0774760700000755.jpg\n",
            "./n0774760700000756.jpg\n",
            "./n0774760700000757.jpg\n",
            "./n0774760700000758.jpg\n",
            "./n0774760700000759.jpg\n",
            "./n0774760700000764.jpg\n",
            "./n0774760700000766.jpg\n",
            "./n0774760700000767.jpg\n",
            "./n0774760700000768.jpg\n",
            "./n0774760700000770.jpg\n",
            "./n0774760700000772.jpg\n",
            "./n0774760700000773.jpg\n",
            "./n0774760700000776.jpg\n",
            "./n0774760700000779.jpg\n",
            "./n0774760700000783.jpg\n",
            "./n0774760700000785.jpg\n",
            "./n0774760700000786.jpg\n",
            "./n0774760700000787.jpg\n",
            "./n0774760700000788.jpg\n",
            "./n0774760700000789.jpg\n",
            "./n0774760700000790.jpg\n",
            "./n0774760700000791.jpg\n",
            "./n0774760700000794.jpg\n",
            "./n0774760700000800.jpg\n",
            "./n0774760700000801.jpg\n",
            "./n0774760700000803.jpg\n",
            "./n0774760700000808.jpg\n",
            "./n0774760700000810.jpg\n",
            "./n0774760700000813.jpg\n",
            "./n0774760700000817.jpg\n",
            "./n0774760700000819.jpg\n",
            "./n0774760700000822.jpg\n",
            "./n0774760700000823.jpg\n",
            "./n0774760700000824.jpg\n",
            "./n0774760700000825.jpg\n",
            "./n0774760700000826.jpg\n",
            "./n0774760700000827.jpg\n",
            "./n0774760700000829.jpg\n",
            "./n0774760700000830.jpg\n",
            "./n0774760700000832.jpg\n",
            "./n0774760700000834.jpg\n",
            "./n0774760700000835.jpg\n",
            "./n0774760700000836.jpg\n",
            "./n0774760700000837.jpg\n",
            "./n0774760700000838.jpg\n",
            "./n0774760700000839.jpg\n",
            "./n0774760700000841.jpg\n",
            "./n0774760700000848.jpg\n",
            "./n0774760700000851.jpg\n",
            "./n0774760700000852.jpg\n",
            "./n0774760700000853.jpg\n",
            "./n0774760700000855.jpg\n",
            "./n0774760700000856.jpg\n",
            "./n0774760700000857.jpg\n",
            "./n0774760700000858.jpg\n",
            "./n0774760700000861.jpg\n",
            "./n0774760700000862.jpg\n",
            "./n0774760700000869.jpg\n",
            "./n0774760700000870.jpg\n",
            "./n0774760700000875.jpg\n",
            "./n0774760700000876.jpg\n",
            "./n0774760700000879.jpg\n",
            "./n0774760700000881.jpg\n",
            "./n0774760700000888.jpg\n",
            "./n0774760700000889.jpg\n",
            "./n0774760700000890.jpg\n",
            "./n0774760700000894.jpg\n",
            "./n0774760700000899.jpg\n",
            "./n0774760700000901.jpg\n",
            "./n0774760700000904.jpg\n",
            "./n0774760700000906.jpg\n",
            "./n0774760700000908.jpg\n",
            "./n0774760700000909.jpg\n",
            "./n0774760700000912.jpg\n",
            "./n0774760700000913.jpg\n",
            "./n0774760700000914.jpg\n",
            "./n0774760700000915.jpg\n",
            "./n0774760700000918.jpg\n",
            "./n0774760700000920.jpg\n",
            "./n0774760700000921.jpg\n",
            "./n0774760700000927.jpg\n",
            "./n0774760700000928.jpg\n",
            "./n0774760700000929.jpg\n",
            "./n0774760700000930.jpg\n",
            "./n0774760700000931.jpg\n",
            "./n0774760700000932.jpg\n",
            "./n0774760700000933.jpg\n",
            "./n0774760700000935.jpg\n",
            "./n0774760700000936.jpg\n",
            "./n0774760700000937.jpg\n",
            "./n0774760700000938.jpg\n",
            "./n0774760700000939.jpg\n",
            "./n0774760700000940.jpg\n",
            "./n0774760700000945.jpg\n",
            "./n0774760700000948.jpg\n",
            "./n0774760700000958.jpg\n",
            "./n0774760700000959.jpg\n",
            "./n0774760700000961.jpg\n",
            "./n0774760700000962.jpg\n",
            "./n0774760700000966.jpg\n",
            "./n0774760700000971.jpg\n",
            "./n0774760700000974.jpg\n",
            "./n0774760700000975.jpg\n",
            "./n0774760700000978.jpg\n",
            "./n0774760700000982.jpg\n",
            "./n0774760700000984.jpg\n",
            "./n0774760700000988.jpg\n",
            "./n0774760700000989.jpg\n",
            "./n0774760700000990.jpg\n",
            "./n0774760700000991.jpg\n",
            "./n0774760700000992.jpg\n",
            "./n0774760700000993.jpg\n",
            "./n0774760700000994.jpg\n",
            "./n0774760700000996.jpg\n",
            "./n0774760700001000.jpg\n",
            "./n0774760700001001.jpg\n",
            "./n0774760700001004.jpg\n",
            "./n0774760700001007.jpg\n",
            "./n0774760700001010.jpg\n",
            "./n0774760700001012.jpg\n",
            "./n0774760700001013.jpg\n",
            "./n0774760700001017.jpg\n",
            "./n0774760700001022.jpg\n",
            "./n0774760700001024.jpg\n",
            "./n0774760700001026.jpg\n",
            "./n0774760700001027.jpg\n",
            "./n0774760700001028.jpg\n",
            "./n0774760700001032.jpg\n",
            "./n0774760700001033.jpg\n",
            "./n0774760700001037.jpg\n",
            "./n0774760700001038.jpg\n",
            "./n0774760700001040.jpg\n",
            "./n0774760700001041.jpg\n",
            "./n0774760700001046.jpg\n",
            "./n0774760700001048.jpg\n",
            "./n0774760700001054.jpg\n",
            "./n0774760700001055.jpg\n",
            "./n0774760700001058.jpg\n",
            "./n0774760700001061.jpg\n",
            "./n0774760700001062.jpg\n",
            "./n0774760700001064.jpg\n",
            "./n0774760700001067.jpg\n",
            "./n0774760700001068.jpg\n",
            "./n0774760700001069.jpg\n",
            "./n0774760700001071.jpg\n",
            "./n0774760700001073.jpg\n",
            "./n0774760700001077.jpg\n",
            "./n0774760700001079.jpg\n",
            "./n0774760700001082.jpg\n",
            "./n0774760700001083.jpg\n",
            "./n0774760700001084.jpg\n",
            "./n0774760700001086.jpg\n",
            "./n0774760700001087.jpg\n",
            "./n0774760700001088.jpg\n",
            "./n0774760700001090.jpg\n",
            "./n0774760700001092.jpg\n",
            "./n0774760700001093.jpg\n",
            "./n0774760700001094.jpg\n",
            "./n0774760700001095.jpg\n",
            "./n0774760700001097.jpg\n",
            "./n0774760700001102.jpg\n",
            "./n0774760700001104.jpg\n",
            "./n0774760700001105.jpg\n",
            "./n0774760700001106.jpg\n",
            "./n0774760700001107.jpg\n",
            "./n0774760700001109.jpg\n",
            "./n0774760700001110.jpg\n",
            "./n0774760700001111.jpg\n",
            "./n0774760700001112.jpg\n",
            "./n0774760700001113.jpg\n",
            "./n0774760700001117.jpg\n",
            "./n0774760700001121.jpg\n",
            "./n0774760700001122.jpg\n",
            "./n0774760700001124.jpg\n",
            "./n0774760700001126.jpg\n",
            "./n0774760700001129.jpg\n",
            "./n0774760700001130.jpg\n",
            "./n0774760700001133.jpg\n",
            "./n0774760700001136.jpg\n",
            "./n0774760700001141.jpg\n",
            "./n0774760700001142.jpg\n",
            "./n0774760700001144.jpg\n",
            "./n0774760700001146.jpg\n",
            "./n0774760700001148.jpg\n",
            "./n0774760700001149.jpg\n",
            "./n0774760700001150.jpg\n",
            "./n0774760700001152.jpg\n",
            "./n0774760700001153.jpg\n",
            "./n0774760700001157.jpg\n",
            "./n0774760700001158.jpg\n",
            "./n0774760700001160.jpg\n",
            "./n0774760700001167.jpg\n",
            "./n0774760700001169.jpg\n",
            "./n0774760700001170.jpg\n",
            "./n0774760700001171.jpg\n",
            "./n0774760700001174.jpg\n",
            "./n0774760700001176.jpg\n",
            "./n0774760700001181.jpg\n",
            "./n0774760700001184.jpg\n",
            "./n0774760700001186.jpg\n",
            "./n0774760700001192.jpg\n",
            "./n0774760700001196.jpg\n",
            "./n0774760700001197.jpg\n",
            "./n0774760700001202.jpg\n",
            "./n0774760700001203.jpg\n",
            "./n0774760700001204.jpg\n",
            "./n0774760700001205.jpg\n",
            "./n0774760700001208.jpg\n",
            "./n0774760700001209.jpg\n",
            "./n0774760700001212.jpg\n",
            "./n0774760700001216.jpg\n",
            "./n0774760700001217.jpg\n",
            "./n0774760700001218.jpg\n",
            "./n0774760700001220.jpg\n",
            "./n0774760700001221.jpg\n",
            "./n0774760700001223.jpg\n",
            "./n0774760700001225.jpg\n",
            "./n0774760700001228.jpg\n",
            "./n0774760700001231.jpg\n",
            "./n0774760700001232.jpg\n",
            "./n0774760700001233.jpg\n",
            "./n0774760700001234.jpg\n",
            "./n0774760700001235.jpg\n",
            "./n0774760700001245.jpg\n",
            "./n0774760700001247.jpg\n",
            "./n0774760700001248.jpg\n",
            "./n0774760700001253.jpg\n",
            "./n0774760700001254.jpg\n",
            "./n0774760700001255.jpg\n",
            "./n0774760700001258.jpg\n",
            "./n0774760700001259.jpg\n",
            "./n0774760700001263.jpg\n",
            "./n0774760700001264.jpg\n",
            "./n0774760700001266.jpg\n",
            "./n0774760700001268.jpg\n",
            "./n0774760700001269.jpg\n",
            "./n0774760700001274.jpg\n",
            "./n0774760700001275.jpg\n",
            "./n0774760700001276.jpg\n",
            "./n0774760700001278.jpg\n",
            "./n0774760700001279.jpg\n",
            "./n0774760700001280.jpg\n",
            "./n0774760700001282.jpg\n",
            "./n0774760700001283.jpg\n",
            "./n0774760700001284.jpg\n",
            "./n0774760700001289.jpg\n",
            "./n0774760700001290.jpg\n",
            "./n0774760700001291.jpg\n",
            "./n0774760700001297.jpg\n",
            "./n0774760700001298.jpg\n",
            "./n0774760700001299.jpg\n",
            "./n0924646400000004.jpg\n",
            "./n0924646400000005.jpg\n",
            "./n0924646400000006.jpg\n",
            "./n0924646400000007.jpg\n",
            "./n0924646400000008.jpg\n",
            "./n0924646400000009.jpg\n",
            "./n0924646400000010.jpg\n",
            "./n0924646400000011.jpg\n",
            "./n0924646400000015.jpg\n",
            "./n0924646400000016.jpg\n",
            "./n0924646400000017.jpg\n",
            "./n0924646400000018.jpg\n",
            "./n0924646400000020.jpg\n",
            "./n0924646400000021.jpg\n",
            "./n0924646400000022.jpg\n",
            "./n0924646400000024.jpg\n",
            "./n0924646400000025.jpg\n",
            "./n0924646400000027.jpg\n",
            "./n0924646400000029.jpg\n",
            "./n0924646400000036.jpg\n",
            "./n0924646400000038.jpg\n",
            "./n0924646400000039.jpg\n",
            "./n0924646400000040.jpg\n",
            "./n0924646400000041.jpg\n",
            "./n0924646400000042.jpg\n",
            "./n0924646400000043.jpg\n",
            "./n0924646400000045.jpg\n",
            "./n0924646400000046.jpg\n",
            "./n0924646400000049.jpg\n",
            "./n0924646400000056.jpg\n",
            "./n0924646400000058.jpg\n",
            "./n0924646400000059.jpg\n",
            "./n0924646400000060.jpg\n",
            "./n0924646400000061.jpg\n",
            "./n0924646400000062.jpg\n",
            "./n0924646400000064.jpg\n",
            "./n0924646400000067.jpg\n",
            "./n0924646400000068.jpg\n",
            "./n0924646400000070.jpg\n",
            "./n0924646400000075.jpg\n",
            "./n0924646400000076.jpg\n",
            "./n0924646400000077.jpg\n",
            "./n0924646400000078.jpg\n",
            "./n0924646400000079.jpg\n",
            "./n0924646400000083.jpg\n",
            "./n0924646400000087.jpg\n",
            "./n0924646400000088.jpg\n",
            "./n0924646400000089.jpg\n",
            "./n0924646400000092.jpg\n",
            "./n0924646400000093.jpg\n",
            "./n0924646400000095.jpg\n",
            "./n0924646400000096.jpg\n",
            "./n0924646400000097.jpg\n",
            "./n0924646400000098.jpg\n",
            "./n0924646400000100.jpg\n",
            "./n0924646400000102.jpg\n",
            "./n0924646400000107.jpg\n",
            "./n0924646400000109.jpg\n",
            "./n0924646400000110.jpg\n",
            "./n0924646400000113.jpg\n",
            "./n0924646400000114.jpg\n",
            "./n0924646400000115.jpg\n",
            "./n0924646400000117.jpg\n",
            "./n0924646400000118.jpg\n",
            "./n0924646400000119.jpg\n",
            "./n0924646400000120.jpg\n",
            "./n0924646400000121.jpg\n",
            "./n0924646400000125.jpg\n",
            "./n0924646400000126.jpg\n",
            "./n0924646400000128.jpg\n",
            "./n0924646400000129.jpg\n",
            "./n0924646400000130.jpg\n",
            "./n0924646400000131.jpg\n",
            "./n0924646400000132.jpg\n",
            "./n0924646400000133.jpg\n",
            "./n0924646400000136.jpg\n",
            "./n0924646400000137.jpg\n",
            "./n0924646400000141.jpg\n",
            "./n0924646400000142.jpg\n",
            "./n0924646400000143.jpg\n",
            "./n0924646400000144.jpg\n",
            "./n0924646400000145.jpg\n",
            "./n0924646400000146.jpg\n",
            "./n0924646400000149.jpg\n",
            "./n0924646400000152.jpg\n",
            "./n0924646400000154.jpg\n",
            "./n0924646400000156.jpg\n",
            "./n0924646400000158.jpg\n",
            "./n0924646400000168.jpg\n",
            "./n0924646400000170.jpg\n",
            "./n0924646400000171.jpg\n",
            "./n0924646400000173.jpg\n",
            "./n0924646400000174.jpg\n",
            "./n0924646400000180.jpg\n",
            "./n0924646400000181.jpg\n",
            "./n0924646400000182.jpg\n",
            "./n0924646400000188.jpg\n",
            "./n0924646400000191.jpg\n",
            "./n0924646400000192.jpg\n",
            "./n0924646400000193.jpg\n",
            "./n0924646400000194.jpg\n",
            "./n0924646400000195.jpg\n",
            "./n0924646400000197.jpg\n",
            "./n0924646400000198.jpg\n",
            "./n0924646400000204.jpg\n",
            "./n0924646400000205.jpg\n",
            "./n0924646400000206.jpg\n",
            "./n0924646400000207.jpg\n",
            "./n0924646400000209.jpg\n",
            "./n0924646400000214.jpg\n",
            "./n0924646400000216.jpg\n",
            "./n0924646400000219.jpg\n",
            "./n0924646400000220.jpg\n",
            "./n0924646400000221.jpg\n",
            "./n0924646400000222.jpg\n",
            "./n0924646400000224.jpg\n",
            "./n0924646400000230.jpg\n",
            "./n0924646400000235.jpg\n",
            "./n0924646400000238.jpg\n",
            "./n0924646400000239.jpg\n",
            "./n0924646400000240.jpg\n",
            "./n0924646400000241.jpg\n",
            "./n0924646400000243.jpg\n",
            "./n0924646400000244.jpg\n",
            "./n0924646400000246.jpg\n",
            "./n0924646400000247.jpg\n",
            "./n0924646400000249.jpg\n",
            "./n0924646400000251.jpg\n",
            "./n0924646400000253.jpg\n",
            "./n0924646400000255.jpg\n",
            "./n0924646400000260.jpg\n",
            "./n0924646400000261.jpg\n",
            "./n0924646400000263.jpg\n",
            "./n0924646400000265.jpg\n",
            "./n0924646400000266.jpg\n",
            "./n0924646400000267.jpg\n",
            "./n0924646400000268.jpg\n",
            "./n0924646400000269.jpg\n",
            "./n0924646400000276.jpg\n",
            "./n0924646400000277.jpg\n",
            "./n0924646400000282.jpg\n",
            "./n0924646400000283.jpg\n",
            "./n0924646400000284.jpg\n",
            "./n0924646400000288.jpg\n",
            "./n0924646400000291.jpg\n",
            "./n0924646400000296.jpg\n",
            "./n0924646400000302.jpg\n",
            "./n0924646400000303.jpg\n",
            "./n0924646400000304.jpg\n",
            "./n0924646400000310.jpg\n",
            "./n0924646400000315.jpg\n",
            "./n0924646400000317.jpg\n",
            "./n0924646400000319.jpg\n",
            "./n0924646400000320.jpg\n",
            "./n0924646400000321.jpg\n",
            "./n0924646400000327.jpg\n",
            "./n0924646400000328.jpg\n",
            "./n0924646400000329.jpg\n",
            "./n0924646400000333.jpg\n",
            "./n0924646400000335.jpg\n",
            "./n0924646400000336.jpg\n",
            "./n0924646400000337.jpg\n",
            "./n0924646400000338.jpg\n",
            "./n0924646400000340.jpg\n",
            "./n0924646400000341.jpg\n",
            "./n0924646400000343.jpg\n",
            "./n0924646400000344.jpg\n",
            "./n0924646400000345.jpg\n",
            "./n0924646400000346.jpg\n",
            "./n0924646400000347.jpg\n",
            "./n0924646400000350.jpg\n",
            "./n0924646400000356.jpg\n",
            "./n0924646400000361.jpg\n",
            "./n0924646400000364.jpg\n",
            "./n0924646400000366.jpg\n",
            "./n0924646400000367.jpg\n",
            "./n0924646400000370.jpg\n",
            "./n0924646400000373.jpg\n",
            "./n0924646400000375.jpg\n",
            "./n0924646400000376.jpg\n",
            "./n0924646400000379.jpg\n",
            "./n0924646400000380.jpg\n",
            "./n0924646400000381.jpg\n",
            "./n0924646400000383.jpg\n",
            "./n0924646400000385.jpg\n",
            "./n0924646400000386.jpg\n",
            "./n0924646400000389.jpg\n",
            "./n0924646400000390.jpg\n",
            "./n0924646400000392.jpg\n",
            "./n0924646400000397.jpg\n",
            "./n0924646400000400.jpg\n",
            "./n0924646400000403.jpg\n",
            "./n0924646400000404.jpg\n",
            "./n0924646400000405.jpg\n",
            "./n0924646400000406.jpg\n",
            "./n0924646400000407.jpg\n",
            "./n0924646400000410.jpg\n",
            "./n0924646400000413.jpg\n",
            "./n0924646400000414.jpg\n",
            "./n0924646400000423.jpg\n",
            "./n0924646400000424.jpg\n",
            "./n0924646400000426.jpg\n",
            "./n0924646400000427.jpg\n",
            "./n0924646400000428.jpg\n",
            "./n0924646400000429.jpg\n",
            "./n0924646400000430.jpg\n",
            "./n0924646400000431.jpg\n",
            "./n0924646400000432.jpg\n",
            "./n0924646400000433.jpg\n",
            "./n0924646400000435.jpg\n",
            "./n0924646400000436.jpg\n",
            "./n0924646400000439.jpg\n",
            "./n0924646400000441.jpg\n",
            "./n0924646400000443.jpg\n",
            "./n0924646400000444.jpg\n",
            "./n0924646400000448.jpg\n",
            "./n0924646400000449.jpg\n",
            "./n0924646400000450.jpg\n",
            "./n0924646400000454.jpg\n",
            "./n0924646400000456.jpg\n",
            "./n0924646400000457.jpg\n",
            "./n0924646400000459.jpg\n",
            "./n0924646400000460.jpg\n",
            "./n0924646400000461.jpg\n",
            "./n0924646400000463.jpg\n",
            "./n0924646400000464.jpg\n",
            "./n0924646400000467.jpg\n",
            "./n0924646400000468.jpg\n",
            "./n0924646400000471.jpg\n",
            "./n0924646400000473.jpg\n",
            "./n0924646400000476.jpg\n",
            "./n0924646400000477.jpg\n",
            "./n0924646400000478.jpg\n",
            "./n0924646400000483.jpg\n",
            "./n0924646400000486.jpg\n",
            "./n0924646400000492.jpg\n",
            "./n0924646400000499.jpg\n",
            "./n0924646400000501.jpg\n",
            "./n0924646400000503.jpg\n",
            "./n0924646400000507.jpg\n",
            "./n0924646400000509.jpg\n",
            "./n0924646400000511.jpg\n",
            "./n0924646400000514.jpg\n",
            "./n0924646400000515.jpg\n",
            "./n0924646400000519.jpg\n",
            "./n0924646400000520.jpg\n",
            "./n0924646400000521.jpg\n",
            "./n0924646400000522.jpg\n",
            "./n0924646400000523.jpg\n",
            "./n0924646400000527.jpg\n",
            "./n0924646400000528.jpg\n",
            "./n0924646400000530.jpg\n",
            "./n0924646400000531.jpg\n",
            "./n0924646400000533.jpg\n",
            "./n0924646400000534.jpg\n",
            "./n0924646400000537.jpg\n",
            "./n0924646400000542.jpg\n",
            "./n0924646400000543.jpg\n",
            "./n0924646400000546.jpg\n",
            "./n0924646400000548.jpg\n",
            "./n0924646400000552.jpg\n",
            "./n0924646400000553.jpg\n",
            "./n0924646400000555.jpg\n",
            "./n0924646400000559.jpg\n",
            "./n0924646400000560.jpg\n",
            "./n0924646400000573.jpg\n",
            "./n0924646400000574.jpg\n",
            "./n0924646400000577.jpg\n",
            "./n0924646400000579.jpg\n",
            "./n0924646400000581.jpg\n",
            "./n0924646400000582.jpg\n",
            "./n0924646400000587.jpg\n",
            "./n0924646400000589.jpg\n",
            "./n0924646400000590.jpg\n",
            "./n0924646400000591.jpg\n",
            "./n0924646400000592.jpg\n",
            "./n0924646400000593.jpg\n",
            "./n0924646400000596.jpg\n",
            "./n0924646400000599.jpg\n",
            "./n0924646400000600.jpg\n",
            "./n0924646400000603.jpg\n",
            "./n0924646400000606.jpg\n",
            "./n0924646400000608.jpg\n",
            "./n0924646400000611.jpg\n",
            "./n0924646400000614.jpg\n",
            "./n0924646400000616.jpg\n",
            "./n0924646400000624.jpg\n",
            "./n0924646400000625.jpg\n",
            "./n0924646400000626.jpg\n",
            "./n0924646400000631.jpg\n",
            "./n0924646400000632.jpg\n",
            "./n0924646400000633.jpg\n",
            "./n0924646400000634.jpg\n",
            "./n0924646400000636.jpg\n",
            "./n0924646400000638.jpg\n",
            "./n0924646400000641.jpg\n",
            "./n0924646400000643.jpg\n",
            "./n0924646400000644.jpg\n",
            "./n0924646400000649.jpg\n",
            "./n0924646400000651.jpg\n",
            "./n0924646400000652.jpg\n",
            "./n0924646400000656.jpg\n",
            "./n0924646400000657.jpg\n",
            "./n0924646400000660.jpg\n",
            "./n0924646400000661.jpg\n",
            "./n0924646400000662.jpg\n",
            "./n0924646400000663.jpg\n",
            "./n0924646400000666.jpg\n",
            "./n0924646400000669.jpg\n",
            "./n0924646400000670.jpg\n",
            "./n0924646400000671.jpg\n",
            "./n0924646400000675.jpg\n",
            "./n0924646400000676.jpg\n",
            "./n0924646400000677.jpg\n",
            "./n0924646400000681.jpg\n",
            "./n0924646400000682.jpg\n",
            "./n0924646400000683.jpg\n",
            "./n0924646400000685.jpg\n",
            "./n0924646400000686.jpg\n",
            "./n0924646400000687.jpg\n",
            "./n0924646400000692.jpg\n",
            "./n0924646400000693.jpg\n",
            "./n0924646400000696.jpg\n",
            "./n0924646400000698.jpg\n",
            "./n0924646400000699.jpg\n",
            "./n0924646400000700.jpg\n",
            "./n0924646400000702.jpg\n",
            "./n0924646400000704.jpg\n",
            "./n0924646400000708.jpg\n",
            "./n0924646400000709.jpg\n",
            "./n0924646400000710.jpg\n",
            "./n0924646400000713.jpg\n",
            "./n0924646400000716.jpg\n",
            "./n0924646400000717.jpg\n",
            "./n0924646400000720.jpg\n",
            "./n0924646400000721.jpg\n",
            "./n0924646400000725.jpg\n",
            "./n0924646400000729.jpg\n",
            "./n0924646400000730.jpg\n",
            "./n0924646400000732.jpg\n",
            "./n0924646400000733.jpg\n",
            "./n0924646400000734.jpg\n",
            "./n0924646400000738.jpg\n",
            "./n0924646400000743.jpg\n",
            "./n0924646400000744.jpg\n",
            "./n0924646400000745.jpg\n",
            "./n0924646400000747.jpg\n",
            "./n0924646400000748.jpg\n",
            "./n0924646400000752.jpg\n",
            "./n0924646400000754.jpg\n",
            "./n0924646400000756.jpg\n",
            "./n0924646400000758.jpg\n",
            "./n0924646400000759.jpg\n",
            "./n0924646400000762.jpg\n",
            "./n0924646400000765.jpg\n",
            "./n0924646400000766.jpg\n",
            "./n0924646400000768.jpg\n",
            "./n0924646400000769.jpg\n",
            "./n0924646400000770.jpg\n",
            "./n0924646400000771.jpg\n",
            "./n0924646400000772.jpg\n",
            "./n0924646400000778.jpg\n",
            "./n0924646400000782.jpg\n",
            "./n0924646400000784.jpg\n",
            "./n0924646400000785.jpg\n",
            "./n0924646400000788.jpg\n",
            "./n0924646400000794.jpg\n",
            "./n0924646400000796.jpg\n",
            "./n0924646400000797.jpg\n",
            "./n0924646400000798.jpg\n",
            "./n0924646400000800.jpg\n",
            "./n0924646400000801.jpg\n",
            "./n0924646400000804.jpg\n",
            "./n0924646400000805.jpg\n",
            "./n0924646400000808.jpg\n",
            "./n0924646400000809.jpg\n",
            "./n0924646400000811.jpg\n",
            "./n0924646400000813.jpg\n",
            "./n0924646400000817.jpg\n",
            "./n0924646400000819.jpg\n",
            "./n0924646400000821.jpg\n",
            "./n0924646400000825.jpg\n",
            "./n0924646400000827.jpg\n",
            "./n0924646400000833.jpg\n",
            "./n0924646400000834.jpg\n",
            "./n0924646400000835.jpg\n",
            "./n0924646400000837.jpg\n",
            "./n0924646400000838.jpg\n",
            "./n0924646400000841.jpg\n",
            "./n0924646400000842.jpg\n",
            "./n0924646400000846.jpg\n",
            "./n0924646400000847.jpg\n",
            "./n0924646400000850.jpg\n",
            "./n0924646400000854.jpg\n",
            "./n0924646400000855.jpg\n",
            "./n0924646400000857.jpg\n",
            "./n0924646400000858.jpg\n",
            "./n0924646400000859.jpg\n",
            "./n0924646400000866.jpg\n",
            "./n0924646400000867.jpg\n",
            "./n0924646400000870.jpg\n",
            "./n0924646400000871.jpg\n",
            "./n0924646400000873.jpg\n",
            "./n0924646400000874.jpg\n",
            "./n0924646400000876.jpg\n",
            "./n0924646400000877.jpg\n",
            "./n0924646400000881.jpg\n",
            "./n0924646400000882.jpg\n",
            "./n0924646400000885.jpg\n",
            "./n0924646400000887.jpg\n",
            "./n0924646400000889.jpg\n",
            "./n0924646400000892.jpg\n",
            "./n0924646400000893.jpg\n",
            "./n0924646400000894.jpg\n",
            "./n0924646400000895.jpg\n",
            "./n0924646400000896.jpg\n",
            "./n0924646400000898.jpg\n",
            "./n0924646400000899.jpg\n",
            "./n0924646400000901.jpg\n",
            "./n0924646400000903.jpg\n",
            "./n0924646400000904.jpg\n",
            "./n0924646400000909.jpg\n",
            "./n0924646400000913.jpg\n",
            "./n0924646400000914.jpg\n",
            "./n0924646400000915.jpg\n",
            "./n0924646400000919.jpg\n",
            "./n0924646400000920.jpg\n",
            "./n0924646400000922.jpg\n",
            "./n0924646400000923.jpg\n",
            "./n0924646400000927.jpg\n",
            "./n0924646400000929.jpg\n",
            "./n0924646400000930.jpg\n",
            "./n0924646400000931.jpg\n",
            "./n0924646400000937.jpg\n",
            "./n0924646400000939.jpg\n",
            "./n0924646400000940.jpg\n",
            "./n0924646400000941.jpg\n",
            "./n0924646400000943.jpg\n",
            "./n0924646400000945.jpg\n",
            "./n0924646400000947.jpg\n",
            "./n0924646400000949.jpg\n",
            "./n0924646400000951.jpg\n",
            "./n0924646400000952.jpg\n",
            "./n0924646400000953.jpg\n",
            "./n0924646400000961.jpg\n",
            "./n0924646400000967.jpg\n",
            "./n0924646400000968.jpg\n",
            "./n0924646400000969.jpg\n",
            "./n0924646400000975.jpg\n",
            "./n0924646400000976.jpg\n",
            "./n0924646400000979.jpg\n",
            "./n0924646400000980.jpg\n",
            "./n0924646400000981.jpg\n",
            "./n0924646400000982.jpg\n",
            "./n0924646400000984.jpg\n",
            "./n0924646400000986.jpg\n",
            "./n0924646400000989.jpg\n",
            "./n0924646400000992.jpg\n",
            "./n0924646400000993.jpg\n",
            "./n0924646400000999.jpg\n",
            "./n0924646400001000.jpg\n",
            "./n0924646400001004.jpg\n",
            "./n0924646400001009.jpg\n",
            "./n0924646400001010.jpg\n",
            "./n0924646400001011.jpg\n",
            "./n0924646400001013.jpg\n",
            "./n0924646400001014.jpg\n",
            "./n0924646400001015.jpg\n",
            "./n0924646400001019.jpg\n",
            "./n0924646400001020.jpg\n",
            "./n0924646400001024.jpg\n",
            "./n0924646400001025.jpg\n",
            "./n0924646400001026.jpg\n",
            "./n0924646400001027.jpg\n",
            "./n0924646400001028.jpg\n",
            "./n0924646400001031.jpg\n",
            "./n0924646400001033.jpg\n",
            "./n0924646400001036.jpg\n",
            "./n0924646400001037.jpg\n",
            "./n0924646400001039.jpg\n",
            "./n0924646400001042.jpg\n",
            "./n0924646400001045.jpg\n",
            "./n0924646400001049.jpg\n",
            "./n0924646400001055.jpg\n",
            "./n0924646400001058.jpg\n",
            "./n0924646400001059.jpg\n",
            "./n0924646400001063.jpg\n",
            "./n0924646400001064.jpg\n",
            "./n0924646400001066.jpg\n",
            "./n0924646400001069.jpg\n",
            "./n0924646400001070.jpg\n",
            "./n0924646400001071.jpg\n",
            "./n0924646400001072.jpg\n",
            "./n0924646400001074.jpg\n",
            "./n0924646400001075.jpg\n",
            "./n0924646400001076.jpg\n",
            "./n0924646400001078.jpg\n",
            "./n0924646400001083.jpg\n",
            "./n0924646400001084.jpg\n",
            "./n0924646400001086.jpg\n",
            "./n0924646400001087.jpg\n",
            "./n0924646400001091.jpg\n",
            "./n0924646400001092.jpg\n",
            "./n0924646400001094.jpg\n",
            "./n0924646400001095.jpg\n",
            "./n0924646400001096.jpg\n",
            "./n0924646400001097.jpg\n",
            "./n0924646400001099.jpg\n",
            "./n0924646400001100.jpg\n",
            "./n0924646400001102.jpg\n",
            "./n0924646400001105.jpg\n",
            "./n0924646400001106.jpg\n",
            "./n0924646400001109.jpg\n",
            "./n0924646400001113.jpg\n",
            "./n0924646400001114.jpg\n",
            "./n0924646400001115.jpg\n",
            "./n0924646400001117.jpg\n",
            "./n0924646400001118.jpg\n",
            "./n0924646400001122.jpg\n",
            "./n0924646400001123.jpg\n",
            "./n0924646400001127.jpg\n",
            "./n0924646400001132.jpg\n",
            "./n0924646400001135.jpg\n",
            "./n0924646400001137.jpg\n",
            "./n0924646400001138.jpg\n",
            "./n0924646400001139.jpg\n",
            "./n0924646400001142.jpg\n",
            "./n0924646400001143.jpg\n",
            "./n0924646400001145.jpg\n",
            "./n0924646400001146.jpg\n",
            "./n0924646400001147.jpg\n",
            "./n0924646400001149.jpg\n",
            "./n0924646400001151.jpg\n",
            "./n0924646400001152.jpg\n",
            "./n0924646400001155.jpg\n",
            "./n0924646400001156.jpg\n",
            "./n0924646400001157.jpg\n",
            "./n0924646400001164.jpg\n",
            "./n0924646400001165.jpg\n",
            "./n0924646400001166.jpg\n",
            "./n0924646400001167.jpg\n",
            "./n0924646400001175.jpg\n",
            "./n0924646400001180.jpg\n",
            "./n0924646400001181.jpg\n",
            "./n0924646400001189.jpg\n",
            "./n0924646400001191.jpg\n",
            "./n0924646400001194.jpg\n",
            "./n0924646400001196.jpg\n",
            "./n0924646400001198.jpg\n",
            "./n0924646400001199.jpg\n",
            "./n0924646400001203.jpg\n",
            "./n0924646400001204.jpg\n",
            "./n0924646400001205.jpg\n",
            "./n0924646400001206.jpg\n",
            "./n0924646400001207.jpg\n",
            "./n0924646400001208.jpg\n",
            "./n0924646400001213.jpg\n",
            "./n0924646400001215.jpg\n",
            "./n0924646400001218.jpg\n",
            "./n0924646400001219.jpg\n",
            "./n0924646400001220.jpg\n",
            "./n0924646400001222.jpg\n",
            "./n0924646400001223.jpg\n",
            "./n0924646400001228.jpg\n",
            "./n0924646400001229.jpg\n",
            "./n0924646400001230.jpg\n",
            "./n0924646400001231.jpg\n",
            "./n0924646400001232.jpg\n",
            "./n0924646400001233.jpg\n",
            "./n0924646400001234.jpg\n",
            "./n0924646400001237.jpg\n",
            "./n0924646400001239.jpg\n",
            "./n0924646400001240.jpg\n",
            "./n0924646400001247.jpg\n",
            "./n0924646400001248.jpg\n",
            "./n0924646400001250.jpg\n",
            "./n0924646400001251.jpg\n",
            "./n0924646400001252.jpg\n",
            "./n0924646400001253.jpg\n",
            "./n0924646400001254.jpg\n",
            "./n0924646400001255.jpg\n",
            "./n0924646400001263.jpg\n",
            "./n0924646400001264.jpg\n",
            "./n0924646400001266.jpg\n",
            "./n0924646400001268.jpg\n",
            "./n0924646400001269.jpg\n",
            "./n0924646400001272.jpg\n",
            "./n0924646400001274.jpg\n",
            "./n0924646400001277.jpg\n",
            "./n0924646400001279.jpg\n",
            "./n0924646400001280.jpg\n",
            "./n0924646400001282.jpg\n",
            "./n0924646400001283.jpg\n",
            "./n0924646400001287.jpg\n",
            "./n0924646400001291.jpg\n",
            "./n0924646400001292.jpg\n",
            "./n0924646400001293.jpg\n",
            "./n0924646400001295.jpg\n",
            "./n0924646400001297.jpg\n",
            "./n0924646400001298.jpg\n",
            "./n0925647900000002.jpg\n",
            "./n0925647900000003.jpg\n",
            "./n0925647900000006.jpg\n",
            "./n0925647900000007.jpg\n",
            "./n0925647900000012.jpg\n",
            "./n0925647900000013.jpg\n",
            "./n0925647900000014.jpg\n",
            "./n0925647900000015.jpg\n",
            "./n0925647900000016.jpg\n",
            "./n0925647900000020.jpg\n",
            "./n0925647900000021.jpg\n",
            "./n0925647900000027.jpg\n",
            "./n0925647900000029.jpg\n",
            "./n0925647900000033.jpg\n",
            "./n0925647900000034.jpg\n",
            "./n0925647900000036.jpg\n",
            "./n0925647900000040.jpg\n",
            "./n0925647900000041.jpg\n",
            "./n0925647900000042.jpg\n",
            "./n0925647900000049.jpg\n",
            "./n0925647900000052.jpg\n",
            "./n0925647900000055.jpg\n",
            "./n0925647900000056.jpg\n",
            "./n0925647900000059.jpg\n",
            "./n0925647900000060.jpg\n",
            "./n0925647900000062.jpg\n",
            "./n0925647900000065.jpg\n",
            "./n0925647900000066.jpg\n",
            "./n0925647900000068.jpg\n",
            "./n0925647900000069.jpg\n",
            "./n0925647900000070.jpg\n",
            "./n0925647900000072.jpg\n",
            "./n0925647900000073.jpg\n",
            "./n0925647900000075.jpg\n",
            "./n0925647900000080.jpg\n",
            "./n0925647900000081.jpg\n",
            "./n0925647900000082.jpg\n",
            "./n0925647900000083.jpg\n",
            "./n0925647900000089.jpg\n",
            "./n0925647900000090.jpg\n",
            "./n0925647900000091.jpg\n",
            "./n0925647900000094.jpg\n",
            "./n0925647900000096.jpg\n",
            "./n0925647900000097.jpg\n",
            "./n0925647900000104.jpg\n",
            "./n0925647900000105.jpg\n",
            "./n0925647900000106.jpg\n",
            "./n0925647900000108.jpg\n",
            "./n0925647900000111.jpg\n",
            "./n0925647900000114.jpg\n",
            "./n0925647900000115.jpg\n",
            "./n0925647900000119.jpg\n",
            "./n0925647900000123.jpg\n",
            "./n0925647900000126.jpg\n",
            "./n0925647900000127.jpg\n",
            "./n0925647900000129.jpg\n",
            "./n0925647900000131.jpg\n",
            "./n0925647900000132.jpg\n",
            "./n0925647900000134.jpg\n",
            "./n0925647900000140.jpg\n",
            "./n0925647900000142.jpg\n",
            "./n0925647900000143.jpg\n",
            "./n0925647900000144.jpg\n",
            "./n0925647900000145.jpg\n",
            "./n0925647900000149.jpg\n",
            "./n0925647900000150.jpg\n",
            "./n0925647900000151.jpg\n",
            "./n0925647900000154.jpg\n",
            "./n0925647900000155.jpg\n",
            "./n0925647900000159.jpg\n",
            "./n0925647900000161.jpg\n",
            "./n0925647900000162.jpg\n",
            "./n0925647900000163.jpg\n",
            "./n0925647900000165.jpg\n",
            "./n0925647900000166.jpg\n",
            "./n0925647900000171.jpg\n",
            "./n0925647900000175.jpg\n",
            "./n0925647900000176.jpg\n",
            "./n0925647900000177.jpg\n",
            "./n0925647900000179.jpg\n",
            "./n0925647900000180.jpg\n",
            "./n0925647900000181.jpg\n",
            "./n0925647900000183.jpg\n",
            "./n0925647900000189.jpg\n",
            "./n0925647900000190.jpg\n",
            "./n0925647900000193.jpg\n",
            "./n0925647900000194.jpg\n",
            "./n0925647900000195.jpg\n",
            "./n0925647900000205.jpg\n",
            "./n0925647900000206.jpg\n",
            "./n0925647900000208.jpg\n",
            "./n0925647900000210.jpg\n",
            "./n0925647900000213.jpg\n",
            "./n0925647900000215.jpg\n",
            "./n0925647900000216.jpg\n",
            "./n0925647900000217.jpg\n",
            "./n0925647900000224.jpg\n",
            "./n0925647900000227.jpg\n",
            "./n0925647900000229.jpg\n",
            "./n0925647900000232.jpg\n",
            "./n0925647900000233.jpg\n",
            "./n0925647900000237.jpg\n",
            "./n0925647900000239.jpg\n",
            "./n0925647900000240.jpg\n",
            "./n0925647900000249.jpg\n",
            "./n0925647900000255.jpg\n",
            "./n0925647900000257.jpg\n",
            "./n0925647900000259.jpg\n",
            "./n0925647900000260.jpg\n",
            "./n0925647900000261.jpg\n",
            "./n0925647900000262.jpg\n",
            "./n0925647900000267.jpg\n",
            "./n0925647900000271.jpg\n",
            "./n0925647900000273.jpg\n",
            "./n0925647900000275.jpg\n",
            "./n0925647900000276.jpg\n",
            "./n0925647900000277.jpg\n",
            "./n0925647900000279.jpg\n",
            "./n0925647900000281.jpg\n",
            "./n0925647900000282.jpg\n",
            "./n0925647900000284.jpg\n",
            "./n0925647900000287.jpg\n",
            "./n0925647900000289.jpg\n",
            "./n0925647900000293.jpg\n",
            "./n0925647900000294.jpg\n",
            "./n0925647900000296.jpg\n",
            "./n0925647900000300.jpg\n",
            "./n0925647900000303.jpg\n",
            "./n0925647900000305.jpg\n",
            "./n0925647900000307.jpg\n",
            "./n0925647900000313.jpg\n",
            "./n0925647900000317.jpg\n",
            "./n0925647900000323.jpg\n",
            "./n0925647900000325.jpg\n",
            "./n0925647900000327.jpg\n",
            "./n0925647900000329.jpg\n",
            "./n0925647900000331.jpg\n",
            "./n0925647900000332.jpg\n",
            "./n0925647900000335.jpg\n",
            "./n0925647900000336.jpg\n",
            "./n0925647900000337.jpg\n",
            "./n0925647900000339.jpg\n",
            "./n0925647900000340.jpg\n",
            "./n0925647900000341.jpg\n",
            "./n0925647900000342.jpg\n",
            "./n0925647900000343.jpg\n",
            "./n0925647900000345.jpg\n",
            "./n0925647900000346.jpg\n",
            "./n0925647900000348.jpg\n",
            "./n0925647900000350.jpg\n",
            "./n0925647900000351.jpg\n",
            "./n0925647900000352.jpg\n",
            "./n0925647900000353.jpg\n",
            "./n0925647900000354.jpg\n",
            "./n0925647900000355.jpg\n",
            "./n0925647900000356.jpg\n",
            "./n0925647900000358.jpg\n",
            "./n0925647900000359.jpg\n",
            "./n0925647900000360.jpg\n",
            "./n0925647900000366.jpg\n",
            "./n0925647900000367.jpg\n",
            "./n0925647900000369.jpg\n",
            "./n0925647900000371.jpg\n",
            "./n0925647900000372.jpg\n",
            "./n0925647900000374.jpg\n",
            "./n0925647900000375.jpg\n",
            "./n0925647900000376.jpg\n",
            "./n0925647900000377.jpg\n",
            "./n0925647900000378.jpg\n",
            "./n0925647900000379.jpg\n",
            "./n0925647900000384.jpg\n",
            "./n0925647900000389.jpg\n",
            "./n0925647900000390.jpg\n",
            "./n0925647900000391.jpg\n",
            "./n0925647900000393.jpg\n",
            "./n0925647900000396.jpg\n",
            "./n0925647900000398.jpg\n",
            "./n0925647900000400.jpg\n",
            "./n0925647900000401.jpg\n",
            "./n0925647900000403.jpg\n",
            "./n0925647900000405.jpg\n",
            "./n0925647900000406.jpg\n",
            "./n0925647900000410.jpg\n",
            "./n0925647900000411.jpg\n",
            "./n0925647900000413.jpg\n",
            "./n0925647900000417.jpg\n",
            "./n0925647900000418.jpg\n",
            "./n0925647900000420.jpg\n",
            "./n0925647900000424.jpg\n",
            "./n0925647900000426.jpg\n",
            "./n0925647900000427.jpg\n",
            "./n0925647900000429.jpg\n",
            "./n0925647900000430.jpg\n",
            "./n0925647900000433.jpg\n",
            "./n0925647900000434.jpg\n",
            "./n0925647900000437.jpg\n",
            "./n0925647900000438.jpg\n",
            "./n0925647900000440.jpg\n",
            "./n0925647900000441.jpg\n",
            "./n0925647900000442.jpg\n",
            "./n0925647900000443.jpg\n",
            "./n0925647900000444.jpg\n",
            "./n0925647900000446.jpg\n",
            "./n0925647900000447.jpg\n",
            "./n0925647900000449.jpg\n",
            "./n0925647900000451.jpg\n",
            "./n0925647900000453.jpg\n",
            "./n0925647900000454.jpg\n",
            "./n0925647900000461.jpg\n",
            "./n0925647900000462.jpg\n",
            "./n0925647900000465.jpg\n",
            "./n0925647900000467.jpg\n",
            "./n0925647900000468.jpg\n",
            "./n0925647900000470.jpg\n",
            "./n0925647900000472.jpg\n",
            "./n0925647900000475.jpg\n",
            "./n0925647900000477.jpg\n",
            "./n0925647900000480.jpg\n",
            "./n0925647900000484.jpg\n",
            "./n0925647900000488.jpg\n",
            "./n0925647900000489.jpg\n",
            "./n0925647900000490.jpg\n",
            "./n0925647900000491.jpg\n",
            "./n0925647900000497.jpg\n",
            "./n0925647900000501.jpg\n",
            "./n0925647900000502.jpg\n",
            "./n0925647900000503.jpg\n",
            "./n0925647900000504.jpg\n",
            "./n0925647900000505.jpg\n",
            "./n0925647900000507.jpg\n",
            "./n0925647900000508.jpg\n",
            "./n0925647900000509.jpg\n",
            "./n0925647900000514.jpg\n",
            "./n0925647900000523.jpg\n",
            "./n0925647900000524.jpg\n",
            "./n0925647900000525.jpg\n",
            "./n0925647900000526.jpg\n",
            "./n0925647900000531.jpg\n",
            "./n0925647900000535.jpg\n",
            "./n0925647900000536.jpg\n",
            "./n0925647900000537.jpg\n",
            "./n0925647900000538.jpg\n",
            "./n0925647900000540.jpg\n",
            "./n0925647900000543.jpg\n",
            "./n0925647900000544.jpg\n",
            "./n0925647900000545.jpg\n",
            "./n0925647900000546.jpg\n",
            "./n0925647900000558.jpg\n",
            "./n0925647900000564.jpg\n",
            "./n0925647900000565.jpg\n",
            "./n0925647900000566.jpg\n",
            "./n0925647900000568.jpg\n",
            "./n0925647900000569.jpg\n",
            "./n0925647900000571.jpg\n",
            "./n0925647900000579.jpg\n",
            "./n0925647900000580.jpg\n",
            "./n0925647900000581.jpg\n",
            "./n0925647900000582.jpg\n",
            "./n0925647900000586.jpg\n",
            "./n0925647900000587.jpg\n",
            "./n0925647900000588.jpg\n",
            "./n0925647900000590.jpg\n",
            "./n0925647900000592.jpg\n",
            "./n0925647900000593.jpg\n",
            "./n0925647900000594.jpg\n",
            "./n0925647900000598.jpg\n",
            "./n0925647900000599.jpg\n",
            "./n0925647900000600.jpg\n",
            "./n0925647900000603.jpg\n",
            "./n0925647900000605.jpg\n",
            "./n0925647900000609.jpg\n",
            "./n0925647900000610.jpg\n",
            "./n0925647900000611.jpg\n",
            "./n0925647900000616.jpg\n",
            "./n0925647900000617.jpg\n",
            "./n0925647900000618.jpg\n",
            "./n0925647900000619.jpg\n",
            "./n0925647900000620.jpg\n",
            "./n0925647900000621.jpg\n",
            "./n0925647900000622.jpg\n",
            "./n0925647900000624.jpg\n",
            "./n0925647900000628.jpg\n",
            "./n0925647900000629.jpg\n",
            "./n0925647900000630.jpg\n",
            "./n0925647900000631.jpg\n",
            "./n0925647900000639.jpg\n",
            "./n0925647900000641.jpg\n",
            "./n0925647900000642.jpg\n",
            "./n0925647900000643.jpg\n",
            "./n0925647900000644.jpg\n",
            "./n0925647900000646.jpg\n",
            "./n0925647900000647.jpg\n",
            "./n0925647900000652.jpg\n",
            "./n0925647900000653.jpg\n",
            "./n0925647900000654.jpg\n",
            "./n0925647900000655.jpg\n",
            "./n0925647900000657.jpg\n",
            "./n0925647900000659.jpg\n",
            "./n0925647900000660.jpg\n",
            "./n0925647900000661.jpg\n",
            "./n0925647900000662.jpg\n",
            "./n0925647900000666.jpg\n",
            "./n0925647900000672.jpg\n",
            "./n0925647900000677.jpg\n",
            "./n0925647900000678.jpg\n",
            "./n0925647900000679.jpg\n",
            "./n0925647900000681.jpg\n",
            "./n0925647900000684.jpg\n",
            "./n0925647900000691.jpg\n",
            "./n0925647900000693.jpg\n",
            "./n0925647900000694.jpg\n",
            "./n0925647900000698.jpg\n",
            "./n0925647900000699.jpg\n",
            "./n0925647900000700.jpg\n",
            "./n0925647900000706.jpg\n",
            "./n0925647900000707.jpg\n",
            "./n0925647900000708.jpg\n",
            "./n0925647900000709.jpg\n",
            "./n0925647900000714.jpg\n",
            "./n0925647900000718.jpg\n",
            "./n0925647900000719.jpg\n",
            "./n0925647900000720.jpg\n",
            "./n0925647900000722.jpg\n",
            "./n0925647900000723.jpg\n",
            "./n0925647900000724.jpg\n",
            "./n0925647900000726.jpg\n",
            "./n0925647900000728.jpg\n",
            "./n0925647900000729.jpg\n",
            "./n0925647900000731.jpg\n",
            "./n0925647900000732.jpg\n",
            "./n0925647900000736.jpg\n",
            "./n0925647900000737.jpg\n",
            "./n0925647900000739.jpg\n",
            "./n0925647900000743.jpg\n",
            "./n0925647900000744.jpg\n",
            "./n0925647900000746.jpg\n",
            "./n0925647900000747.jpg\n",
            "./n0925647900000749.jpg\n",
            "./n0925647900000750.jpg\n",
            "./n0925647900000753.jpg\n",
            "./n0925647900000755.jpg\n",
            "./n0925647900000757.jpg\n",
            "./n0925647900000764.jpg\n",
            "./n0925647900000767.jpg\n",
            "./n0925647900000768.jpg\n",
            "./n0925647900000769.jpg\n",
            "./n0925647900000773.jpg\n",
            "./n0925647900000774.jpg\n",
            "./n0925647900000778.jpg\n",
            "./n0925647900000779.jpg\n",
            "./n0925647900000782.jpg\n",
            "./n0925647900000783.jpg\n",
            "./n0925647900000784.jpg\n",
            "./n0925647900000787.jpg\n",
            "./n0925647900000788.jpg\n",
            "./n0925647900000792.jpg\n",
            "./n0925647900000794.jpg\n",
            "./n0925647900000795.jpg\n",
            "./n0925647900000797.jpg\n",
            "./n0925647900000799.jpg\n",
            "./n0925647900000801.jpg\n",
            "./n0925647900000803.jpg\n",
            "./n0925647900000804.jpg\n",
            "./n0925647900000806.jpg\n",
            "./n0925647900000809.jpg\n",
            "./n0925647900000810.jpg\n",
            "./n0925647900000811.jpg\n",
            "./n0925647900000814.jpg\n",
            "./n0925647900000818.jpg\n",
            "./n0925647900000819.jpg\n",
            "./n0925647900000822.jpg\n",
            "./n0925647900000823.jpg\n",
            "./n0925647900000824.jpg\n",
            "./n0925647900000825.jpg\n",
            "./n0925647900000827.jpg\n",
            "./n0925647900000828.jpg\n",
            "./n0925647900000829.jpg\n",
            "./n0925647900000830.jpg\n",
            "./n0925647900000831.jpg\n",
            "./n0925647900000832.jpg\n",
            "./n0925647900000839.jpg\n",
            "./n0925647900000840.jpg\n",
            "./n0925647900000844.jpg\n",
            "./n0925647900000845.jpg\n",
            "./n0925647900000846.jpg\n",
            "./n0925647900000847.jpg\n",
            "./n0925647900000850.jpg\n",
            "./n0925647900000852.jpg\n",
            "./n0925647900000855.jpg\n",
            "./n0925647900000860.jpg\n",
            "./n0925647900000862.jpg\n",
            "./n0925647900000868.jpg\n",
            "./n0925647900000870.jpg\n",
            "./n0925647900000872.jpg\n",
            "./n0925647900000874.jpg\n",
            "./n0925647900000877.jpg\n",
            "./n0925647900000878.jpg\n",
            "./n0925647900000881.jpg\n",
            "./n0925647900000882.jpg\n",
            "./n0925647900000883.jpg\n",
            "./n0925647900000884.jpg\n",
            "./n0925647900000886.jpg\n",
            "./n0925647900000887.jpg\n",
            "./n0925647900000888.jpg\n",
            "./n0925647900000889.jpg\n",
            "./n0925647900000890.jpg\n",
            "./n0925647900000894.jpg\n",
            "./n0925647900000896.jpg\n",
            "./n0925647900000898.jpg\n",
            "./n0925647900000899.jpg\n",
            "./n0925647900000902.jpg\n",
            "./n0925647900000908.jpg\n",
            "./n0925647900000909.jpg\n",
            "./n0925647900000912.jpg\n",
            "./n0925647900000913.jpg\n",
            "./n0925647900000915.jpg\n",
            "./n0925647900000916.jpg\n",
            "./n0925647900000917.jpg\n",
            "./n0925647900000918.jpg\n",
            "./n0925647900000919.jpg\n",
            "./n0925647900000920.jpg\n",
            "./n0925647900000922.jpg\n",
            "./n0925647900000923.jpg\n",
            "./n0925647900000927.jpg\n",
            "./n0925647900000928.jpg\n",
            "./n0925647900000929.jpg\n",
            "./n0925647900000931.jpg\n",
            "./n0925647900000933.jpg\n",
            "./n0925647900000934.jpg\n",
            "./n0925647900000938.jpg\n",
            "./n0925647900000943.jpg\n",
            "./n0925647900000945.jpg\n",
            "./n0925647900000947.jpg\n",
            "./n0925647900000955.jpg\n",
            "./n0925647900000956.jpg\n",
            "./n0925647900000961.jpg\n",
            "./n0925647900000962.jpg\n",
            "./n0925647900000965.jpg\n",
            "./n0925647900000966.jpg\n",
            "./n0925647900000969.jpg\n",
            "./n0925647900000971.jpg\n",
            "./n0925647900000972.jpg\n",
            "./n0925647900000973.jpg\n",
            "./n0925647900000975.jpg\n",
            "./n0925647900000977.jpg\n",
            "./n0925647900000978.jpg\n",
            "./n0925647900000979.jpg\n",
            "./n0925647900000981.jpg\n",
            "./n0925647900000982.jpg\n",
            "./n0925647900000983.jpg\n",
            "./n0925647900000986.jpg\n",
            "./n0925647900000988.jpg\n",
            "./n0925647900000992.jpg\n",
            "./n0925647900000993.jpg\n",
            "./n0925647900000999.jpg\n",
            "./n0925647900001000.jpg\n",
            "./n0925647900001006.jpg\n",
            "./n0925647900001008.jpg\n",
            "./n0925647900001009.jpg\n",
            "./n0925647900001010.jpg\n",
            "./n0925647900001011.jpg\n",
            "./n0925647900001013.jpg\n",
            "./n0925647900001017.jpg\n",
            "./n0925647900001019.jpg\n",
            "./n0925647900001022.jpg\n",
            "./n0925647900001023.jpg\n",
            "./n0925647900001024.jpg\n",
            "./n0925647900001027.jpg\n",
            "./n0925647900001031.jpg\n",
            "./n0925647900001035.jpg\n",
            "./n0925647900001038.jpg\n",
            "./n0925647900001041.jpg\n",
            "./n0925647900001042.jpg\n",
            "./n0925647900001047.jpg\n",
            "./n0925647900001050.jpg\n",
            "./n0925647900001051.jpg\n",
            "./n0925647900001052.jpg\n",
            "./n0925647900001054.jpg\n",
            "./n0925647900001056.jpg\n",
            "./n0925647900001059.jpg\n",
            "./n0925647900001060.jpg\n",
            "./n0925647900001061.jpg\n",
            "./n0925647900001062.jpg\n",
            "./n0925647900001064.jpg\n",
            "./n0925647900001066.jpg\n",
            "./n0925647900001068.jpg\n",
            "./n0925647900001069.jpg\n",
            "./n0925647900001070.jpg\n",
            "./n0925647900001071.jpg\n",
            "./n0925647900001072.jpg\n",
            "./n0925647900001073.jpg\n",
            "./n0925647900001075.jpg\n",
            "./n0925647900001076.jpg\n",
            "./n0925647900001077.jpg\n",
            "./n0925647900001080.jpg\n",
            "./n0925647900001082.jpg\n",
            "./n0925647900001084.jpg\n",
            "./n0925647900001085.jpg\n",
            "./n0925647900001088.jpg\n",
            "./n0925647900001092.jpg\n",
            "./n0925647900001093.jpg\n",
            "./n0925647900001095.jpg\n",
            "./n0925647900001096.jpg\n",
            "./n0925647900001097.jpg\n",
            "./n0925647900001098.jpg\n",
            "./n0925647900001099.jpg\n",
            "./n0925647900001100.jpg\n",
            "./n0925647900001102.jpg\n",
            "./n0925647900001105.jpg\n",
            "./n0925647900001106.jpg\n",
            "./n0925647900001107.jpg\n",
            "./n0925647900001109.jpg\n",
            "./n0925647900001115.jpg\n",
            "./n0925647900001116.jpg\n",
            "./n0925647900001119.jpg\n",
            "./n0925647900001123.jpg\n",
            "./n0925647900001124.jpg\n",
            "./n0925647900001129.jpg\n",
            "./n0925647900001130.jpg\n",
            "./n0925647900001131.jpg\n",
            "./n0925647900001135.jpg\n",
            "./n0925647900001137.jpg\n",
            "./n0925647900001138.jpg\n",
            "./n0925647900001140.jpg\n",
            "./n0925647900001141.jpg\n",
            "./n0925647900001142.jpg\n",
            "./n0925647900001143.jpg\n",
            "./n0925647900001148.jpg\n",
            "./n0925647900001150.jpg\n",
            "./n0925647900001151.jpg\n",
            "./n0925647900001154.jpg\n",
            "./n0925647900001162.jpg\n",
            "./n0925647900001163.jpg\n",
            "./n0925647900001164.jpg\n",
            "./n0925647900001165.jpg\n",
            "./n0925647900001166.jpg\n",
            "./n0925647900001168.jpg\n",
            "./n0925647900001169.jpg\n",
            "./n0925647900001172.jpg\n",
            "./n0925647900001175.jpg\n",
            "./n0925647900001178.jpg\n",
            "./n0925647900001179.jpg\n",
            "./n0925647900001180.jpg\n",
            "./n0925647900001181.jpg\n",
            "./n0925647900001182.jpg\n",
            "./n0925647900001183.jpg\n",
            "./n0925647900001188.jpg\n",
            "./n0925647900001190.jpg\n",
            "./n0925647900001191.jpg\n",
            "./n0925647900001192.jpg\n",
            "./n0925647900001195.jpg\n",
            "./n0925647900001198.jpg\n",
            "./n0925647900001201.jpg\n",
            "./n0925647900001202.jpg\n",
            "./n0925647900001203.jpg\n",
            "./n0925647900001205.jpg\n",
            "./n0925647900001212.jpg\n",
            "./n0925647900001213.jpg\n",
            "./n0925647900001215.jpg\n",
            "./n0925647900001218.jpg\n",
            "./n0925647900001219.jpg\n",
            "./n0925647900001220.jpg\n",
            "./n0925647900001224.jpg\n",
            "./n0925647900001225.jpg\n",
            "./n0925647900001226.jpg\n",
            "./n0925647900001228.jpg\n",
            "./n0925647900001229.jpg\n",
            "./n0925647900001231.jpg\n",
            "./n0925647900001232.jpg\n",
            "./n0925647900001234.jpg\n",
            "./n0925647900001236.jpg\n",
            "./n0925647900001237.jpg\n",
            "./n0925647900001239.jpg\n",
            "./n0925647900001243.jpg\n",
            "./n0925647900001244.jpg\n",
            "./n0925647900001245.jpg\n",
            "./n0925647900001246.jpg\n",
            "./n0925647900001250.jpg\n",
            "./n0925647900001254.jpg\n",
            "./n0925647900001256.jpg\n",
            "./n0925647900001258.jpg\n",
            "./n0925647900001259.jpg\n",
            "./n0925647900001261.jpg\n",
            "./n0925647900001262.jpg\n",
            "./n0925647900001263.jpg\n",
            "./n0925647900001265.jpg\n",
            "./n0925647900001267.jpg\n",
            "./n0925647900001269.jpg\n",
            "./n0925647900001270.jpg\n",
            "./n0925647900001271.jpg\n",
            "./n0925647900001272.jpg\n",
            "./n0925647900001273.jpg\n",
            "./n0925647900001274.jpg\n",
            "./n0925647900001275.jpg\n",
            "./n0925647900001279.jpg\n",
            "./n0925647900001283.jpg\n",
            "./n0925647900001284.jpg\n",
            "./n0925647900001285.jpg\n",
            "./n0925647900001286.jpg\n",
            "./n0925647900001291.jpg\n",
            "./n1305456000000001.jpg\n",
            "./n1305456000000004.jpg\n",
            "./n1305456000000005.jpg\n",
            "./n1305456000000008.jpg\n",
            "./n1305456000000009.jpg\n",
            "./n1305456000000014.jpg\n",
            "./n1305456000000016.jpg\n",
            "./n1305456000000017.jpg\n",
            "./n1305456000000018.jpg\n",
            "./n1305456000000021.jpg\n",
            "./n1305456000000025.jpg\n",
            "./n1305456000000026.jpg\n",
            "./n1305456000000027.jpg\n",
            "./n1305456000000030.jpg\n",
            "./n1305456000000032.jpg\n",
            "./n1305456000000033.jpg\n",
            "./n1305456000000034.jpg\n",
            "./n1305456000000035.jpg\n",
            "./n1305456000000037.jpg\n",
            "./n1305456000000038.jpg\n",
            "./n1305456000000039.jpg\n",
            "./n1305456000000040.jpg\n",
            "./n1305456000000041.jpg\n",
            "./n1305456000000042.jpg\n",
            "./n1305456000000043.jpg\n",
            "./n1305456000000045.jpg\n",
            "./n1305456000000046.jpg\n",
            "./n1305456000000047.jpg\n",
            "./n1305456000000048.jpg\n",
            "./n1305456000000052.jpg\n",
            "./n1305456000000057.jpg\n",
            "./n1305456000000058.jpg\n",
            "./n1305456000000060.jpg\n",
            "./n1305456000000061.jpg\n",
            "./n1305456000000062.jpg\n",
            "./n1305456000000063.jpg\n",
            "./n1305456000000066.jpg\n",
            "./n1305456000000067.jpg\n",
            "./n1305456000000068.jpg\n",
            "./n1305456000000073.jpg\n",
            "./n1305456000000074.jpg\n",
            "./n1305456000000076.jpg\n",
            "./n1305456000000077.jpg\n",
            "./n1305456000000078.jpg\n",
            "./n1305456000000081.jpg\n",
            "./n1305456000000083.jpg\n",
            "./n1305456000000085.jpg\n",
            "./n1305456000000086.jpg\n",
            "./n1305456000000087.jpg\n",
            "./n1305456000000088.jpg\n",
            "./n1305456000000093.jpg\n",
            "./n1305456000000094.jpg\n",
            "./n1305456000000097.jpg\n",
            "./n1305456000000098.jpg\n",
            "./n1305456000000099.jpg\n",
            "./n1305456000000100.jpg\n",
            "./n1305456000000101.jpg\n",
            "./n1305456000000102.jpg\n",
            "./n1305456000000103.jpg\n",
            "./n1305456000000104.jpg\n",
            "./n1305456000000108.jpg\n",
            "./n1305456000000112.jpg\n",
            "./n1305456000000114.jpg\n",
            "./n1305456000000116.jpg\n",
            "./n1305456000000119.jpg\n",
            "./n1305456000000122.jpg\n",
            "./n1305456000000126.jpg\n",
            "./n1305456000000130.jpg\n",
            "./n1305456000000132.jpg\n",
            "./n1305456000000138.jpg\n",
            "./n1305456000000142.jpg\n",
            "./n1305456000000145.jpg\n",
            "./n1305456000000148.jpg\n",
            "./n1305456000000149.jpg\n",
            "./n1305456000000150.jpg\n",
            "./n1305456000000153.jpg\n",
            "./n1305456000000155.jpg\n",
            "./n1305456000000156.jpg\n",
            "./n1305456000000159.jpg\n",
            "./n1305456000000160.jpg\n",
            "./n1305456000000163.jpg\n",
            "./n1305456000000164.jpg\n",
            "./n1305456000000165.jpg\n",
            "./n1305456000000166.jpg\n",
            "./n1305456000000167.jpg\n",
            "./n1305456000000169.jpg\n",
            "./n1305456000000170.jpg\n",
            "./n1305456000000171.jpg\n",
            "./n1305456000000172.jpg\n",
            "./n1305456000000176.jpg\n",
            "./n1305456000000183.jpg\n",
            "./n1305456000000186.jpg\n",
            "./n1305456000000187.jpg\n",
            "./n1305456000000188.jpg\n",
            "./n1305456000000190.jpg\n",
            "./n1305456000000197.jpg\n",
            "./n1305456000000198.jpg\n",
            "./n1305456000000199.jpg\n",
            "./n1305456000000200.jpg\n",
            "./n1305456000000202.jpg\n",
            "./n1305456000000203.jpg\n",
            "./n1305456000000206.jpg\n",
            "./n1305456000000207.jpg\n",
            "./n1305456000000209.jpg\n",
            "./n1305456000000211.jpg\n",
            "./n1305456000000213.jpg\n",
            "./n1305456000000214.jpg\n",
            "./n1305456000000216.jpg\n",
            "./n1305456000000218.jpg\n",
            "./n1305456000000219.jpg\n",
            "./n1305456000000222.jpg\n",
            "./n1305456000000226.jpg\n",
            "./n1305456000000227.jpg\n",
            "./n1305456000000230.jpg\n",
            "./n1305456000000231.jpg\n",
            "./n1305456000000237.jpg\n",
            "./n1305456000000244.jpg\n",
            "./n1305456000000252.jpg\n",
            "./n1305456000000254.jpg\n",
            "./n1305456000000255.jpg\n",
            "./n1305456000000258.jpg\n",
            "./n1305456000000263.jpg\n",
            "./n1305456000000268.jpg\n",
            "./n1305456000000269.jpg\n",
            "./n1305456000000274.jpg\n",
            "./n1305456000000276.jpg\n",
            "./n1305456000000277.jpg\n",
            "./n1305456000000279.jpg\n",
            "./n1305456000000280.jpg\n",
            "./n1305456000000282.jpg\n",
            "./n1305456000000286.jpg\n",
            "./n1305456000000287.jpg\n",
            "./n1305456000000289.jpg\n",
            "./n1305456000000290.jpg\n",
            "./n1305456000000291.jpg\n",
            "./n1305456000000294.jpg\n",
            "./n1305456000000295.jpg\n",
            "./n1305456000000298.jpg\n",
            "./n1305456000000301.jpg\n",
            "./n1305456000000303.jpg\n",
            "./n1305456000000305.jpg\n",
            "./n1305456000000306.jpg\n",
            "./n1305456000000307.jpg\n",
            "./n1305456000000309.jpg\n",
            "./n1305456000000311.jpg\n",
            "./n1305456000000312.jpg\n",
            "./n1305456000000313.jpg\n",
            "./n1305456000000315.jpg\n",
            "./n1305456000000316.jpg\n",
            "./n1305456000000318.jpg\n",
            "./n1305456000000319.jpg\n",
            "./n1305456000000320.jpg\n",
            "./n1305456000000321.jpg\n",
            "./n1305456000000322.jpg\n",
            "./n1305456000000324.jpg\n",
            "./n1305456000000332.jpg\n",
            "./n1305456000000338.jpg\n",
            "./n1305456000000341.jpg\n",
            "./n1305456000000342.jpg\n",
            "./n1305456000000346.jpg\n",
            "./n1305456000000347.jpg\n",
            "./n1305456000000349.jpg\n",
            "./n1305456000000350.jpg\n",
            "./n1305456000000351.jpg\n",
            "./n1305456000000352.jpg\n",
            "./n1305456000000355.jpg\n",
            "./n1305456000000357.jpg\n",
            "./n1305456000000360.jpg\n",
            "./n1305456000000362.jpg\n",
            "./n1305456000000367.jpg\n",
            "./n1305456000000370.jpg\n",
            "./n1305456000000372.jpg\n",
            "./n1305456000000374.jpg\n",
            "./n1305456000000379.jpg\n",
            "./n1305456000000380.jpg\n",
            "./n1305456000000383.jpg\n",
            "./n1305456000000385.jpg\n",
            "./n1305456000000386.jpg\n",
            "./n1305456000000389.jpg\n",
            "./n1305456000000390.jpg\n",
            "./n1305456000000391.jpg\n",
            "./n1305456000000394.jpg\n",
            "./n1305456000000395.jpg\n",
            "./n1305456000000397.jpg\n",
            "./n1305456000000404.jpg\n",
            "./n1305456000000406.jpg\n",
            "./n1305456000000408.jpg\n",
            "./n1305456000000413.jpg\n",
            "./n1305456000000415.jpg\n",
            "./n1305456000000418.jpg\n",
            "./n1305456000000419.jpg\n",
            "./n1305456000000420.jpg\n",
            "./n1305456000000423.jpg\n",
            "./n1305456000000425.jpg\n",
            "./n1305456000000426.jpg\n",
            "./n1305456000000428.jpg\n",
            "./n1305456000000429.jpg\n",
            "./n1305456000000430.jpg\n",
            "./n1305456000000432.jpg\n",
            "./n1305456000000436.jpg\n",
            "./n1305456000000438.jpg\n",
            "./n1305456000000439.jpg\n",
            "./n1305456000000444.jpg\n",
            "./n1305456000000445.jpg\n",
            "./n1305456000000450.jpg\n",
            "./n1305456000000451.jpg\n",
            "./n1305456000000452.jpg\n",
            "./n1305456000000453.jpg\n",
            "./n1305456000000454.jpg\n",
            "./n1305456000000458.jpg\n",
            "./n1305456000000459.jpg\n",
            "./n1305456000000460.jpg\n",
            "./n1305456000000461.jpg\n",
            "./n1305456000000462.jpg\n",
            "./n1305456000000470.jpg\n",
            "./n1305456000000471.jpg\n",
            "./n1305456000000475.jpg\n",
            "./n1305456000000476.jpg\n",
            "./n1305456000000479.jpg\n",
            "./n1305456000000480.jpg\n",
            "./n1305456000000482.jpg\n",
            "./n1305456000000483.jpg\n",
            "./n1305456000000487.jpg\n",
            "./n1305456000000491.jpg\n",
            "./n1305456000000492.jpg\n",
            "./n1305456000000493.jpg\n",
            "./n1305456000000496.jpg\n",
            "./n1305456000000498.jpg\n",
            "./n1305456000000499.jpg\n",
            "./n1305456000000500.jpg\n",
            "./n1305456000000502.jpg\n",
            "./n1305456000000503.jpg\n",
            "./n1305456000000504.jpg\n",
            "./n1305456000000505.jpg\n",
            "./n1305456000000506.jpg\n",
            "./n1305456000000510.jpg\n",
            "./n1305456000000511.jpg\n",
            "./n1305456000000513.jpg\n",
            "./n1305456000000515.jpg\n",
            "./n1305456000000517.jpg\n",
            "./n1305456000000518.jpg\n",
            "./n1305456000000519.jpg\n",
            "./n1305456000000520.jpg\n",
            "./n1305456000000522.jpg\n",
            "./n1305456000000523.jpg\n",
            "./n1305456000000525.jpg\n",
            "./n1305456000000529.jpg\n",
            "./n1305456000000530.jpg\n",
            "./n1305456000000531.jpg\n",
            "./n1305456000000533.jpg\n",
            "./n1305456000000534.jpg\n",
            "./n1305456000000538.jpg\n",
            "./n1305456000000539.jpg\n",
            "./n1305456000000543.jpg\n",
            "./n1305456000000544.jpg\n",
            "./n1305456000000545.jpg\n",
            "./n1305456000000550.jpg\n",
            "./n1305456000000551.jpg\n",
            "./n1305456000000554.jpg\n",
            "./n1305456000000556.jpg\n",
            "./n1305456000000558.jpg\n",
            "./n1305456000000559.jpg\n",
            "./n1305456000000561.jpg\n",
            "./n1305456000000564.jpg\n",
            "./n1305456000000565.jpg\n",
            "./n1305456000000567.jpg\n",
            "./n1305456000000570.jpg\n",
            "./n1305456000000571.jpg\n",
            "./n1305456000000572.jpg\n",
            "./n1305456000000575.jpg\n",
            "./n1305456000000576.jpg\n",
            "./n1305456000000577.jpg\n",
            "./n1305456000000579.jpg\n",
            "./n1305456000000582.jpg\n",
            "./n1305456000000586.jpg\n",
            "./n1305456000000588.jpg\n",
            "./n1305456000000590.jpg\n",
            "./n1305456000000592.jpg\n",
            "./n1305456000000593.jpg\n",
            "./n1305456000000594.jpg\n",
            "./n1305456000000595.jpg\n",
            "./n1305456000000596.jpg\n",
            "./n1305456000000599.jpg\n",
            "./n1305456000000600.jpg\n",
            "./n1305456000000605.jpg\n",
            "./n1305456000000608.jpg\n",
            "./n1305456000000609.jpg\n",
            "./n1305456000000610.jpg\n",
            "./n1305456000000611.jpg\n",
            "./n1305456000000612.jpg\n",
            "./n1305456000000614.jpg\n",
            "./n1305456000000616.jpg\n",
            "./n1305456000000619.jpg\n",
            "./n1305456000000625.jpg\n",
            "./n1305456000000627.jpg\n",
            "./n1305456000000628.jpg\n",
            "./n1305456000000632.jpg\n",
            "./n1305456000000633.jpg\n",
            "./n1305456000000637.jpg\n",
            "./n1305456000000639.jpg\n",
            "./n1305456000000641.jpg\n",
            "./n1305456000000642.jpg\n",
            "./n1305456000000645.jpg\n",
            "./n1305456000000647.jpg\n",
            "./n1305456000000648.jpg\n",
            "./n1305456000000650.jpg\n",
            "./n1305456000000651.jpg\n",
            "./n1305456000000652.jpg\n",
            "./n1305456000000653.jpg\n",
            "./n1305456000000654.jpg\n",
            "./n1305456000000657.jpg\n",
            "./n1305456000000658.jpg\n",
            "./n1305456000000659.jpg\n",
            "./n1305456000000660.jpg\n",
            "./n1305456000000661.jpg\n",
            "./n1305456000000662.jpg\n",
            "./n1305456000000663.jpg\n",
            "./n1305456000000665.jpg\n",
            "./n1305456000000667.jpg\n",
            "./n1305456000000670.jpg\n",
            "./n1305456000000672.jpg\n",
            "./n1305456000000674.jpg\n",
            "./n1305456000000675.jpg\n",
            "./n1305456000000678.jpg\n",
            "./n1305456000000680.jpg\n",
            "./n1305456000000681.jpg\n",
            "./n1305456000000686.jpg\n",
            "./n1305456000000690.jpg\n",
            "./n1305456000000693.jpg\n",
            "./n1305456000000695.jpg\n",
            "./n1305456000000696.jpg\n",
            "./n1305456000000701.jpg\n",
            "./n1305456000000703.jpg\n",
            "./n1305456000000710.jpg\n",
            "./n1305456000000713.jpg\n",
            "./n1305456000000714.jpg\n",
            "./n1305456000000716.jpg\n",
            "./n1305456000000718.jpg\n",
            "./n1305456000000721.jpg\n",
            "./n1305456000000726.jpg\n",
            "./n1305456000000727.jpg\n",
            "./n1305456000000730.jpg\n",
            "./n1305456000000733.jpg\n",
            "./n1305456000000736.jpg\n",
            "./n1305456000000737.jpg\n",
            "./n1305456000000741.jpg\n",
            "./n1305456000000744.jpg\n",
            "./n1305456000000745.jpg\n",
            "./n1305456000000747.jpg\n",
            "./n1305456000000751.jpg\n",
            "./n1305456000000754.jpg\n",
            "./n1305456000000755.jpg\n",
            "./n1305456000000756.jpg\n",
            "./n1305456000000757.jpg\n",
            "./n1305456000000758.jpg\n",
            "./n1305456000000763.jpg\n",
            "./n1305456000000764.jpg\n",
            "./n1305456000000765.jpg\n",
            "./n1305456000000766.jpg\n",
            "./n1305456000000769.jpg\n",
            "./n1305456000000771.jpg\n",
            "./n1305456000000773.jpg\n",
            "./n1305456000000774.jpg\n",
            "./n1305456000000777.jpg\n",
            "./n1305456000000778.jpg\n",
            "./n1305456000000784.jpg\n",
            "./n1305456000000787.jpg\n",
            "./n1305456000000788.jpg\n",
            "./n1305456000000789.jpg\n",
            "./n1305456000000790.jpg\n",
            "./n1305456000000791.jpg\n",
            "./n1305456000000792.jpg\n",
            "./n1305456000000794.jpg\n",
            "./n1305456000000795.jpg\n",
            "./n1305456000000797.jpg\n",
            "./n1305456000000800.jpg\n",
            "./n1305456000000801.jpg\n",
            "./n1305456000000802.jpg\n",
            "./n1305456000000804.jpg\n",
            "./n1305456000000805.jpg\n",
            "./n1305456000000806.jpg\n",
            "./n1305456000000807.jpg\n",
            "./n1305456000000809.jpg\n",
            "./n1305456000000811.jpg\n",
            "./n1305456000000812.jpg\n",
            "./n1305456000000813.jpg\n",
            "./n1305456000000814.jpg\n",
            "./n1305456000000817.jpg\n",
            "./n1305456000000820.jpg\n",
            "./n1305456000000823.jpg\n",
            "./n1305456000000824.jpg\n",
            "./n1305456000000826.jpg\n",
            "./n1305456000000827.jpg\n",
            "./n1305456000000833.jpg\n",
            "./n1305456000000834.jpg\n",
            "./n1305456000000835.jpg\n",
            "./n1305456000000838.jpg\n",
            "./n1305456000000844.jpg\n",
            "./n1305456000000847.jpg\n",
            "./n1305456000000848.jpg\n",
            "./n1305456000000849.jpg\n",
            "./n1305456000000850.jpg\n",
            "./n1305456000000852.jpg\n",
            "./n1305456000000854.jpg\n",
            "./n1305456000000855.jpg\n",
            "./n1305456000000856.jpg\n",
            "./n1305456000000858.jpg\n",
            "./n1305456000000860.jpg\n",
            "./n1305456000000861.jpg\n",
            "./n1305456000000863.jpg\n",
            "./n1305456000000866.jpg\n",
            "./n1305456000000867.jpg\n",
            "./n1305456000000868.jpg\n",
            "./n1305456000000874.jpg\n",
            "./n1305456000000875.jpg\n",
            "./n1305456000000880.jpg\n",
            "./n1305456000000883.jpg\n",
            "./n1305456000000885.jpg\n",
            "./n1305456000000886.jpg\n",
            "./n1305456000000889.jpg\n",
            "./n1305456000000895.jpg\n",
            "./n1305456000000897.jpg\n",
            "./n1305456000000898.jpg\n",
            "./n1305456000000899.jpg\n",
            "./n1305456000000901.jpg\n",
            "./n1305456000000903.jpg\n",
            "./n1305456000000905.jpg\n",
            "./n1305456000000906.jpg\n",
            "./n1305456000000908.jpg\n",
            "./n1305456000000909.jpg\n",
            "./n1305456000000912.jpg\n",
            "./n1305456000000914.jpg\n",
            "./n1305456000000916.jpg\n",
            "./n1305456000000917.jpg\n",
            "./n1305456000000918.jpg\n",
            "./n1305456000000919.jpg\n",
            "./n1305456000000920.jpg\n",
            "./n1305456000000921.jpg\n",
            "./n1305456000000926.jpg\n",
            "./n1305456000000928.jpg\n",
            "./n1305456000000930.jpg\n",
            "./n1305456000000931.jpg\n",
            "./n1305456000000937.jpg\n",
            "./n1305456000000938.jpg\n",
            "./n1305456000000941.jpg\n",
            "./n1305456000000942.jpg\n",
            "./n1305456000000943.jpg\n",
            "./n1305456000000944.jpg\n",
            "./n1305456000000945.jpg\n",
            "./n1305456000000947.jpg\n",
            "./n1305456000000950.jpg\n",
            "./n1305456000000953.jpg\n",
            "./n1305456000000956.jpg\n",
            "./n1305456000000962.jpg\n",
            "./n1305456000000964.jpg\n",
            "./n1305456000000965.jpg\n",
            "./n1305456000000972.jpg\n",
            "./n1305456000000973.jpg\n",
            "./n1305456000000976.jpg\n",
            "./n1305456000000977.jpg\n",
            "./n1305456000000982.jpg\n",
            "./n1305456000000983.jpg\n",
            "./n1305456000000984.jpg\n",
            "./n1305456000000985.jpg\n",
            "./n1305456000000986.jpg\n",
            "./n1305456000000987.jpg\n",
            "./n1305456000000988.jpg\n",
            "./n1305456000000990.jpg\n",
            "./n1305456000000993.jpg\n",
            "./n1305456000000997.jpg\n",
            "./n1305456000000999.jpg\n",
            "./n1305456000001005.jpg\n",
            "./n1305456000001006.jpg\n",
            "./n1305456000001007.jpg\n",
            "./n1305456000001011.jpg\n",
            "./n1305456000001012.jpg\n",
            "./n1305456000001013.jpg\n",
            "./n1305456000001016.jpg\n",
            "./n1305456000001018.jpg\n",
            "./n1305456000001020.jpg\n",
            "./n1305456000001021.jpg\n",
            "./n1305456000001025.jpg\n",
            "./n1305456000001026.jpg\n",
            "./n1305456000001027.jpg\n",
            "./n1305456000001036.jpg\n",
            "./n1305456000001038.jpg\n",
            "./n1305456000001039.jpg\n",
            "./n1305456000001043.jpg\n",
            "./n1305456000001045.jpg\n",
            "./n1305456000001047.jpg\n",
            "./n1305456000001048.jpg\n",
            "./n1305456000001049.jpg\n",
            "./n1305456000001050.jpg\n",
            "./n1305456000001051.jpg\n",
            "./n1305456000001053.jpg\n",
            "./n1305456000001055.jpg\n",
            "./n1305456000001057.jpg\n",
            "./n1305456000001058.jpg\n",
            "./n1305456000001059.jpg\n",
            "./n1305456000001061.jpg\n",
            "./n1305456000001062.jpg\n",
            "./n1305456000001064.jpg\n",
            "./n1305456000001066.jpg\n",
            "./n1305456000001067.jpg\n",
            "./n1305456000001068.jpg\n",
            "./n1305456000001070.jpg\n",
            "./n1305456000001072.jpg\n",
            "./n1305456000001079.jpg\n",
            "./n1305456000001080.jpg\n",
            "./n1305456000001081.jpg\n",
            "./n1305456000001084.jpg\n",
            "./n1305456000001086.jpg\n",
            "./n1305456000001087.jpg\n",
            "./n1305456000001088.jpg\n",
            "./n1305456000001092.jpg\n",
            "./n1305456000001094.jpg\n",
            "./n1305456000001104.jpg\n",
            "./n1305456000001105.jpg\n",
            "./n1305456000001107.jpg\n",
            "./n1305456000001108.jpg\n",
            "./n1305456000001112.jpg\n",
            "./n1305456000001115.jpg\n",
            "./n1305456000001116.jpg\n",
            "./n1305456000001117.jpg\n",
            "./n1305456000001118.jpg\n",
            "./n1305456000001119.jpg\n",
            "./n1305456000001121.jpg\n",
            "./n1305456000001122.jpg\n",
            "./n1305456000001125.jpg\n",
            "./n1305456000001126.jpg\n",
            "./n1305456000001136.jpg\n",
            "./n1305456000001138.jpg\n",
            "./n1305456000001139.jpg\n",
            "./n1305456000001141.jpg\n",
            "./n1305456000001142.jpg\n",
            "./n1305456000001143.jpg\n",
            "./n1305456000001144.jpg\n",
            "./n1305456000001145.jpg\n",
            "./n1305456000001149.jpg\n",
            "./n1305456000001152.jpg\n",
            "./n1305456000001156.jpg\n",
            "./n1305456000001159.jpg\n",
            "./n1305456000001160.jpg\n",
            "./n1305456000001163.jpg\n",
            "./n1305456000001164.jpg\n",
            "./n1305456000001167.jpg\n",
            "./n1305456000001172.jpg\n",
            "./n1305456000001177.jpg\n",
            "./n1305456000001178.jpg\n",
            "./n1305456000001180.jpg\n",
            "./n1305456000001181.jpg\n",
            "./n1305456000001182.jpg\n",
            "./n1305456000001187.jpg\n",
            "./n1305456000001188.jpg\n",
            "./n1305456000001191.jpg\n",
            "./n1305456000001194.jpg\n",
            "./n1305456000001197.jpg\n",
            "./n1305456000001198.jpg\n",
            "./n1305456000001199.jpg\n",
            "./n1305456000001201.jpg\n",
            "./n1305456000001206.jpg\n",
            "./n1305456000001209.jpg\n",
            "./n1305456000001210.jpg\n",
            "./n1305456000001212.jpg\n",
            "./n1305456000001214.jpg\n",
            "./n1305456000001216.jpg\n",
            "./n1305456000001220.jpg\n",
            "./n1305456000001222.jpg\n",
            "./n1305456000001223.jpg\n",
            "./n1305456000001225.jpg\n",
            "./n1305456000001226.jpg\n",
            "./n1305456000001227.jpg\n",
            "./n1305456000001228.jpg\n",
            "./n1305456000001229.jpg\n",
            "./n1305456000001230.jpg\n",
            "./n1305456000001231.jpg\n",
            "./n1305456000001232.jpg\n",
            "./n1305456000001235.jpg\n",
            "./n1305456000001239.jpg\n",
            "./n1305456000001240.jpg\n",
            "./n1305456000001247.jpg\n",
            "./n1305456000001248.jpg\n",
            "./n1305456000001249.jpg\n",
            "./n1305456000001252.jpg\n",
            "./n1305456000001255.jpg\n",
            "./n1305456000001259.jpg\n",
            "./n1305456000001261.jpg\n",
            "./n1305456000001262.jpg\n",
            "./n1305456000001263.jpg\n",
            "./n1305456000001266.jpg\n",
            "./n1305456000001270.jpg\n",
            "./n1305456000001271.jpg\n",
            "./n1305456000001273.jpg\n",
            "./n1305456000001277.jpg\n",
            "./n1305456000001279.jpg\n",
            "./n1305456000001280.jpg\n",
            "./n1305456000001282.jpg\n",
            "./n1305456000001288.jpg\n",
            "./n1305456000001292.jpg\n",
            "./n1305456000001293.jpg\n",
            "./n1313361300000001.jpg\n",
            "./n1313361300000002.jpg\n",
            "./n1313361300000004.jpg\n",
            "./n1313361300000007.jpg\n",
            "./n1313361300000008.jpg\n",
            "./n1313361300000009.jpg\n",
            "./n1313361300000012.jpg\n",
            "./n1313361300000014.jpg\n",
            "./n1313361300000015.jpg\n",
            "./n1313361300000017.jpg\n",
            "./n1313361300000018.jpg\n",
            "./n1313361300000021.jpg\n",
            "./n1313361300000022.jpg\n",
            "./n1313361300000024.jpg\n",
            "./n1313361300000028.jpg\n",
            "./n1313361300000033.jpg\n",
            "./n1313361300000035.jpg\n",
            "./n1313361300000040.jpg\n",
            "./n1313361300000041.jpg\n",
            "./n1313361300000042.jpg\n",
            "./n1313361300000044.jpg\n",
            "./n1313361300000045.jpg\n",
            "./n1313361300000046.jpg\n",
            "./n1313361300000047.jpg\n",
            "./n1313361300000048.jpg\n",
            "./n1313361300000049.jpg\n",
            "./n1313361300000050.jpg\n",
            "./n1313361300000051.jpg\n",
            "./n1313361300000052.jpg\n",
            "./n1313361300000054.jpg\n",
            "./n1313361300000055.jpg\n",
            "./n1313361300000056.jpg\n",
            "./n1313361300000059.jpg\n",
            "./n1313361300000062.jpg\n",
            "./n1313361300000063.jpg\n",
            "./n1313361300000064.jpg\n",
            "./n1313361300000065.jpg\n",
            "./n1313361300000066.jpg\n",
            "./n1313361300000067.jpg\n",
            "./n1313361300000070.jpg\n",
            "./n1313361300000076.jpg\n",
            "./n1313361300000077.jpg\n",
            "./n1313361300000078.jpg\n",
            "./n1313361300000079.jpg\n",
            "./n1313361300000083.jpg\n",
            "./n1313361300000085.jpg\n",
            "./n1313361300000086.jpg\n",
            "./n1313361300000087.jpg\n",
            "./n1313361300000088.jpg\n",
            "./n1313361300000090.jpg\n",
            "./n1313361300000091.jpg\n",
            "./n1313361300000094.jpg\n",
            "./n1313361300000095.jpg\n",
            "./n1313361300000096.jpg\n",
            "./n1313361300000097.jpg\n",
            "./n1313361300000098.jpg\n",
            "./n1313361300000101.jpg\n",
            "./n1313361300000102.jpg\n",
            "./n1313361300000104.jpg\n",
            "./n1313361300000106.jpg\n",
            "./n1313361300000110.jpg\n",
            "./n1313361300000112.jpg\n",
            "./n1313361300000113.jpg\n",
            "./n1313361300000116.jpg\n",
            "./n1313361300000117.jpg\n",
            "./n1313361300000123.jpg\n",
            "./n1313361300000124.jpg\n",
            "./n1313361300000128.jpg\n",
            "./n1313361300000130.jpg\n",
            "./n1313361300000131.jpg\n",
            "./n1313361300000137.jpg\n",
            "./n1313361300000139.jpg\n",
            "./n1313361300000143.jpg\n",
            "./n1313361300000144.jpg\n",
            "./n1313361300000147.jpg\n",
            "./n1313361300000148.jpg\n",
            "./n1313361300000150.jpg\n",
            "./n1313361300000152.jpg\n",
            "./n1313361300000153.jpg\n",
            "./n1313361300000154.jpg\n",
            "./n1313361300000155.jpg\n",
            "./n1313361300000157.jpg\n",
            "./n1313361300000158.jpg\n",
            "./n1313361300000159.jpg\n",
            "./n1313361300000160.jpg\n",
            "./n1313361300000161.jpg\n",
            "./n1313361300000162.jpg\n",
            "./n1313361300000165.jpg\n",
            "./n1313361300000166.jpg\n",
            "./n1313361300000168.jpg\n",
            "./n1313361300000170.jpg\n",
            "./n1313361300000171.jpg\n",
            "./n1313361300000175.jpg\n",
            "./n1313361300000177.jpg\n",
            "./n1313361300000181.jpg\n",
            "./n1313361300000183.jpg\n",
            "./n1313361300000189.jpg\n",
            "./n1313361300000192.jpg\n",
            "./n1313361300000193.jpg\n",
            "./n1313361300000196.jpg\n",
            "./n1313361300000200.jpg\n",
            "./n1313361300000201.jpg\n",
            "./n1313361300000202.jpg\n",
            "./n1313361300000206.jpg\n",
            "./n1313361300000209.jpg\n",
            "./n1313361300000213.jpg\n",
            "./n1313361300000214.jpg\n",
            "./n1313361300000215.jpg\n",
            "./n1313361300000216.jpg\n",
            "./n1313361300000218.jpg\n",
            "./n1313361300000219.jpg\n",
            "./n1313361300000220.jpg\n",
            "./n1313361300000224.jpg\n",
            "./n1313361300000227.jpg\n",
            "./n1313361300000228.jpg\n",
            "./n1313361300000231.jpg\n",
            "./n1313361300000232.jpg\n",
            "./n1313361300000234.jpg\n",
            "./n1313361300000235.jpg\n",
            "./n1313361300000237.jpg\n",
            "./n1313361300000240.jpg\n",
            "./n1313361300000244.jpg\n",
            "./n1313361300000247.jpg\n",
            "./n1313361300000249.jpg\n",
            "./n1313361300000251.jpg\n",
            "./n1313361300000256.jpg\n",
            "./n1313361300000257.jpg\n",
            "./n1313361300000258.jpg\n",
            "./n1313361300000259.jpg\n",
            "./n1313361300000263.jpg\n",
            "./n1313361300000264.jpg\n",
            "./n1313361300000265.jpg\n",
            "./n1313361300000270.jpg\n",
            "./n1313361300000271.jpg\n",
            "./n1313361300000272.jpg\n",
            "./n1313361300000274.jpg\n",
            "./n1313361300000276.jpg\n",
            "./n1313361300000278.jpg\n",
            "./n1313361300000279.jpg\n",
            "./n1313361300000281.jpg\n",
            "./n1313361300000282.jpg\n",
            "./n1313361300000283.jpg\n",
            "./n1313361300000284.jpg\n",
            "./n1313361300000287.jpg\n",
            "./n1313361300000288.jpg\n",
            "./n1313361300000293.jpg\n",
            "./n1313361300000294.jpg\n",
            "./n1313361300000295.jpg\n",
            "./n1313361300000298.jpg\n",
            "./n1313361300000299.jpg\n",
            "./n1313361300000301.jpg\n",
            "./n1313361300000302.jpg\n",
            "./n1313361300000303.jpg\n",
            "./n1313361300000310.jpg\n",
            "./n1313361300000311.jpg\n",
            "./n1313361300000313.jpg\n",
            "./n1313361300000314.jpg\n",
            "./n1313361300000315.jpg\n",
            "./n1313361300000317.jpg\n",
            "./n1313361300000318.jpg\n",
            "./n1313361300000320.jpg\n",
            "./n1313361300000321.jpg\n",
            "./n1313361300000322.jpg\n",
            "./n1313361300000324.jpg\n",
            "./n1313361300000326.jpg\n",
            "./n1313361300000327.jpg\n",
            "./n1313361300000328.jpg\n",
            "./n1313361300000329.jpg\n",
            "./n1313361300000330.jpg\n",
            "./n1313361300000332.jpg\n",
            "./n1313361300000333.jpg\n",
            "./n1313361300000334.jpg\n",
            "./n1313361300000335.jpg\n",
            "./n1313361300000337.jpg\n",
            "./n1313361300000339.jpg\n",
            "./n1313361300000340.jpg\n",
            "./n1313361300000344.jpg\n",
            "./n1313361300000347.jpg\n",
            "./n1313361300000357.jpg\n",
            "./n1313361300000358.jpg\n",
            "./n1313361300000359.jpg\n",
            "./n1313361300000362.jpg\n",
            "./n1313361300000365.jpg\n",
            "./n1313361300000368.jpg\n",
            "./n1313361300000370.jpg\n",
            "./n1313361300000371.jpg\n",
            "./n1313361300000375.jpg\n",
            "./n1313361300000376.jpg\n",
            "./n1313361300000379.jpg\n",
            "./n1313361300000382.jpg\n",
            "./n1313361300000384.jpg\n",
            "./n1313361300000385.jpg\n",
            "./n1313361300000386.jpg\n",
            "./n1313361300000389.jpg\n",
            "./n1313361300000391.jpg\n",
            "./n1313361300000397.jpg\n",
            "./n1313361300000399.jpg\n",
            "./n1313361300000400.jpg\n",
            "./n1313361300000404.jpg\n",
            "./n1313361300000407.jpg\n",
            "./n1313361300000409.jpg\n",
            "./n1313361300000410.jpg\n",
            "./n1313361300000411.jpg\n",
            "./n1313361300000416.jpg\n",
            "./n1313361300000418.jpg\n",
            "./n1313361300000419.jpg\n",
            "./n1313361300000420.jpg\n",
            "./n1313361300000421.jpg\n",
            "./n1313361300000427.jpg\n",
            "./n1313361300000428.jpg\n",
            "./n1313361300000429.jpg\n",
            "./n1313361300000430.jpg\n",
            "./n1313361300000431.jpg\n",
            "./n1313361300000432.jpg\n",
            "./n1313361300000435.jpg\n",
            "./n1313361300000438.jpg\n",
            "./n1313361300000440.jpg\n",
            "./n1313361300000443.jpg\n",
            "./n1313361300000444.jpg\n",
            "./n1313361300000447.jpg\n",
            "./n1313361300000449.jpg\n",
            "./n1313361300000450.jpg\n",
            "./n1313361300000451.jpg\n",
            "./n1313361300000452.jpg\n",
            "./n1313361300000458.jpg\n",
            "./n1313361300000459.jpg\n",
            "./n1313361300000467.jpg\n",
            "./n1313361300000470.jpg\n",
            "./n1313361300000471.jpg\n",
            "./n1313361300000473.jpg\n",
            "./n1313361300000474.jpg\n",
            "./n1313361300000477.jpg\n",
            "./n1313361300000481.jpg\n",
            "./n1313361300000482.jpg\n",
            "./n1313361300000484.jpg\n",
            "./n1313361300000487.jpg\n",
            "./n1313361300000490.jpg\n",
            "./n1313361300000491.jpg\n",
            "./n1313361300000492.jpg\n",
            "./n1313361300000494.jpg\n",
            "./n1313361300000497.jpg\n",
            "./n1313361300000498.jpg\n",
            "./n1313361300000499.jpg\n",
            "./n1313361300000500.jpg\n",
            "./n1313361300000502.jpg\n",
            "./n1313361300000507.jpg\n",
            "./n1313361300000511.jpg\n",
            "./n1313361300000517.jpg\n",
            "./n1313361300000518.jpg\n",
            "./n1313361300000521.jpg\n",
            "./n1313361300000522.jpg\n",
            "./n1313361300000523.jpg\n",
            "./n1313361300000528.jpg\n",
            "./n1313361300000532.jpg\n",
            "./n1313361300000533.jpg\n",
            "./n1313361300000534.jpg\n",
            "./n1313361300000537.jpg\n",
            "./n1313361300000540.jpg\n",
            "./n1313361300000541.jpg\n",
            "./n1313361300000542.jpg\n",
            "./n1313361300000543.jpg\n",
            "./n1313361300000544.jpg\n",
            "./n1313361300000545.jpg\n",
            "./n1313361300000546.jpg\n",
            "./n1313361300000547.jpg\n",
            "./n1313361300000549.jpg\n",
            "./n1313361300000550.jpg\n",
            "./n1313361300000551.jpg\n",
            "./n1313361300000556.jpg\n",
            "./n1313361300000557.jpg\n",
            "./n1313361300000561.jpg\n",
            "./n1313361300000562.jpg\n",
            "./n1313361300000563.jpg\n",
            "./n1313361300000565.jpg\n",
            "./n1313361300000569.jpg\n",
            "./n1313361300000570.jpg\n",
            "./n1313361300000571.jpg\n",
            "./n1313361300000572.jpg\n",
            "./n1313361300000574.jpg\n",
            "./n1313361300000575.jpg\n",
            "./n1313361300000576.jpg\n",
            "./n1313361300000580.jpg\n",
            "./n1313361300000584.jpg\n",
            "./n1313361300000587.jpg\n",
            "./n1313361300000588.jpg\n",
            "./n1313361300000591.jpg\n",
            "./n1313361300000592.jpg\n",
            "./n1313361300000596.jpg\n",
            "./n1313361300000598.jpg\n",
            "./n1313361300000599.jpg\n",
            "./n1313361300000605.jpg\n",
            "./n1313361300000606.jpg\n",
            "./n1313361300000608.jpg\n",
            "./n1313361300000610.jpg\n",
            "./n1313361300000611.jpg\n",
            "./n1313361300000614.jpg\n",
            "./n1313361300000617.jpg\n",
            "./n1313361300000618.jpg\n",
            "./n1313361300000622.jpg\n",
            "./n1313361300000623.jpg\n",
            "./n1313361300000624.jpg\n",
            "./n1313361300000625.jpg\n",
            "./n1313361300000629.jpg\n",
            "./n1313361300000630.jpg\n",
            "./n1313361300000634.jpg\n",
            "./n1313361300000635.jpg\n",
            "./n1313361300000639.jpg\n",
            "./n1313361300000642.jpg\n",
            "./n1313361300000650.jpg\n",
            "./n1313361300000651.jpg\n",
            "./n1313361300000652.jpg\n",
            "./n1313361300000654.jpg\n",
            "./n1313361300000655.jpg\n",
            "./n1313361300000657.jpg\n",
            "./n1313361300000658.jpg\n",
            "./n1313361300000659.jpg\n",
            "./n1313361300000661.jpg\n",
            "./n1313361300000663.jpg\n",
            "./n1313361300000664.jpg\n",
            "./n1313361300000667.jpg\n",
            "./n1313361300000668.jpg\n",
            "./n1313361300000669.jpg\n",
            "./n1313361300000672.jpg\n",
            "./n1313361300000673.jpg\n",
            "./n1313361300000675.jpg\n",
            "./n1313361300000679.jpg\n",
            "./n1313361300000680.jpg\n",
            "./n1313361300000681.jpg\n",
            "./n1313361300000682.jpg\n",
            "./n1313361300000684.jpg\n",
            "./n1313361300000685.jpg\n",
            "./n1313361300000687.jpg\n",
            "./n1313361300000689.jpg\n",
            "./n1313361300000691.jpg\n",
            "./n1313361300000693.jpg\n",
            "./n1313361300000697.jpg\n",
            "./n1313361300000699.jpg\n",
            "./n1313361300000701.jpg\n",
            "./n1313361300000702.jpg\n",
            "./n1313361300000704.jpg\n",
            "./n1313361300000705.jpg\n",
            "./n1313361300000708.jpg\n",
            "./n1313361300000713.jpg\n",
            "./n1313361300000714.jpg\n",
            "./n1313361300000715.jpg\n",
            "./n1313361300000717.jpg\n",
            "./n1313361300000720.jpg\n",
            "./n1313361300000721.jpg\n",
            "./n1313361300000724.jpg\n",
            "./n1313361300000726.jpg\n",
            "./n1313361300000727.jpg\n",
            "./n1313361300000728.jpg\n",
            "./n1313361300000730.jpg\n",
            "./n1313361300000731.jpg\n",
            "./n1313361300000732.jpg\n",
            "./n1313361300000733.jpg\n",
            "./n1313361300000734.jpg\n",
            "./n1313361300000735.jpg\n",
            "./n1313361300000736.jpg\n",
            "./n1313361300000738.jpg\n",
            "./n1313361300000739.jpg\n",
            "./n1313361300000743.jpg\n",
            "./n1313361300000747.jpg\n",
            "./n1313361300000749.jpg\n",
            "./n1313361300000751.jpg\n",
            "./n1313361300000752.jpg\n",
            "./n1313361300000755.jpg\n",
            "./n1313361300000758.jpg\n",
            "./n1313361300000764.jpg\n",
            "./n1313361300000768.jpg\n",
            "./n1313361300000769.jpg\n",
            "./n1313361300000771.jpg\n",
            "./n1313361300000775.jpg\n",
            "./n1313361300000778.jpg\n",
            "./n1313361300000779.jpg\n",
            "./n1313361300000780.jpg\n",
            "./n1313361300000793.jpg\n",
            "./n1313361300000795.jpg\n",
            "./n1313361300000796.jpg\n",
            "./n1313361300000797.jpg\n",
            "./n1313361300000801.jpg\n",
            "./n1313361300000802.jpg\n",
            "./n1313361300000805.jpg\n",
            "./n1313361300000809.jpg\n",
            "./n1313361300000815.jpg\n",
            "./n1313361300000818.jpg\n",
            "./n1313361300000820.jpg\n",
            "./n1313361300000825.jpg\n",
            "./n1313361300000830.jpg\n",
            "./n1313361300000831.jpg\n",
            "./n1313361300000834.jpg\n",
            "./n1313361300000835.jpg\n",
            "./n1313361300000836.jpg\n",
            "./n1313361300000838.jpg\n",
            "./n1313361300000841.jpg\n",
            "./n1313361300000842.jpg\n",
            "./n1313361300000843.jpg\n",
            "./n1313361300000846.jpg\n",
            "./n1313361300000847.jpg\n",
            "./n1313361300000848.jpg\n",
            "./n1313361300000849.jpg\n",
            "./n1313361300000851.jpg\n",
            "./n1313361300000852.jpg\n",
            "./n1313361300000855.jpg\n",
            "./n1313361300000857.jpg\n",
            "./n1313361300000862.jpg\n",
            "./n1313361300000864.jpg\n",
            "./n1313361300000865.jpg\n",
            "./n1313361300000866.jpg\n",
            "./n1313361300000867.jpg\n",
            "./n1313361300000871.jpg\n",
            "./n1313361300000872.jpg\n",
            "./n1313361300000873.jpg\n",
            "./n1313361300000880.jpg\n",
            "./n1313361300000882.jpg\n",
            "./n1313361300000884.jpg\n",
            "./n1313361300000885.jpg\n",
            "./n1313361300000886.jpg\n",
            "./n1313361300000890.jpg\n",
            "./n1313361300000892.jpg\n",
            "./n1313361300000894.jpg\n",
            "./n1313361300000898.jpg\n",
            "./n1313361300000899.jpg\n",
            "./n1313361300000901.jpg\n",
            "./n1313361300000902.jpg\n",
            "./n1313361300000911.jpg\n",
            "./n1313361300000912.jpg\n",
            "./n1313361300000913.jpg\n",
            "./n1313361300000915.jpg\n",
            "./n1313361300000918.jpg\n",
            "./n1313361300000921.jpg\n",
            "./n1313361300000922.jpg\n",
            "./n1313361300000923.jpg\n",
            "./n1313361300000929.jpg\n",
            "./n1313361300000930.jpg\n",
            "./n1313361300000932.jpg\n",
            "./n1313361300000934.jpg\n",
            "./n1313361300000938.jpg\n",
            "./n1313361300000939.jpg\n",
            "./n1313361300000940.jpg\n",
            "./n1313361300000947.jpg\n",
            "./n1313361300000949.jpg\n",
            "./n1313361300000950.jpg\n",
            "./n1313361300000951.jpg\n",
            "./n1313361300000952.jpg\n",
            "./n1313361300000953.jpg\n",
            "./n1313361300000956.jpg\n",
            "./n1313361300000961.jpg\n",
            "./n1313361300000962.jpg\n",
            "./n1313361300000963.jpg\n",
            "./n1313361300000964.jpg\n",
            "./n1313361300000965.jpg\n",
            "./n1313361300000966.jpg\n",
            "./n1313361300000974.jpg\n",
            "./n1313361300000979.jpg\n",
            "./n1313361300000980.jpg\n",
            "./n1313361300000982.jpg\n",
            "./n1313361300000984.jpg\n",
            "./n1313361300000985.jpg\n",
            "./n1313361300000989.jpg\n",
            "./n1313361300000991.jpg\n",
            "./n1313361300000992.jpg\n",
            "./n1313361300000993.jpg\n",
            "./n1313361300000995.jpg\n",
            "./n1313361300001001.jpg\n",
            "./n1313361300001002.jpg\n",
            "./n1313361300001003.jpg\n",
            "./n1313361300001006.jpg\n",
            "./n1313361300001010.jpg\n",
            "./n1313361300001013.jpg\n",
            "./n1313361300001014.jpg\n",
            "./n1313361300001017.jpg\n",
            "./n1313361300001019.jpg\n",
            "./n1313361300001022.jpg\n",
            "./n1313361300001023.jpg\n",
            "./n1313361300001027.jpg\n",
            "./n1313361300001029.jpg\n",
            "./n1313361300001030.jpg\n",
            "./n1313361300001033.jpg\n",
            "./n1313361300001034.jpg\n",
            "./n1313361300001035.jpg\n",
            "./n1313361300001039.jpg\n",
            "./n1313361300001041.jpg\n",
            "./n1313361300001043.jpg\n",
            "./n1313361300001047.jpg\n",
            "./n1313361300001048.jpg\n",
            "./n1313361300001051.jpg\n",
            "./n1313361300001053.jpg\n",
            "./n1313361300001054.jpg\n",
            "./n1313361300001060.jpg\n",
            "./n1313361300001063.jpg\n",
            "./n1313361300001064.jpg\n",
            "./n1313361300001066.jpg\n",
            "./n1313361300001067.jpg\n",
            "./n1313361300001068.jpg\n",
            "./n1313361300001073.jpg\n",
            "./n1313361300001078.jpg\n",
            "./n1313361300001079.jpg\n",
            "./n1313361300001081.jpg\n",
            "./n1313361300001082.jpg\n",
            "./n1313361300001085.jpg\n",
            "./n1313361300001088.jpg\n",
            "./n1313361300001089.jpg\n",
            "./n1313361300001090.jpg\n",
            "./n1313361300001094.jpg\n",
            "./n1313361300001095.jpg\n",
            "./n1313361300001098.jpg\n",
            "./n1313361300001100.jpg\n",
            "./n1313361300001101.jpg\n",
            "./n1313361300001102.jpg\n",
            "./n1313361300001110.jpg\n",
            "./n1313361300001111.jpg\n",
            "./n1313361300001112.jpg\n",
            "./n1313361300001113.jpg\n",
            "./n1313361300001117.jpg\n",
            "./n1313361300001118.jpg\n",
            "./n1313361300001119.jpg\n",
            "./n1313361300001120.jpg\n",
            "./n1313361300001122.jpg\n",
            "./n1313361300001125.jpg\n",
            "./n1313361300001126.jpg\n",
            "./n1313361300001127.jpg\n",
            "./n1313361300001130.jpg\n",
            "./n1313361300001131.jpg\n",
            "./n1313361300001134.jpg\n",
            "./n1313361300001136.jpg\n",
            "./n1313361300001137.jpg\n",
            "./n1313361300001144.jpg\n",
            "./n1313361300001145.jpg\n",
            "./n1313361300001146.jpg\n",
            "./n1313361300001147.jpg\n",
            "./n1313361300001148.jpg\n",
            "./n1313361300001149.jpg\n",
            "./n1313361300001150.jpg\n",
            "./n1313361300001153.jpg\n",
            "./n1313361300001154.jpg\n",
            "./n1313361300001157.jpg\n",
            "./n1313361300001158.jpg\n",
            "./n1313361300001160.jpg\n",
            "./n1313361300001161.jpg\n",
            "./n1313361300001162.jpg\n",
            "./n1313361300001165.jpg\n",
            "./n1313361300001167.jpg\n",
            "./n1313361300001168.jpg\n",
            "./n1313361300001170.jpg\n",
            "./n1313361300001171.jpg\n",
            "./n1313361300001172.jpg\n",
            "./n1313361300001173.jpg\n",
            "./n1313361300001179.jpg\n",
            "./n1313361300001181.jpg\n",
            "./n1313361300001182.jpg\n",
            "./n1313361300001187.jpg\n",
            "./n1313361300001188.jpg\n",
            "./n1313361300001190.jpg\n",
            "./n1313361300001191.jpg\n",
            "./n1313361300001192.jpg\n",
            "./n1313361300001194.jpg\n",
            "./n1313361300001196.jpg\n",
            "./n1313361300001198.jpg\n",
            "./n1313361300001200.jpg\n",
            "./n1313361300001201.jpg\n",
            "./n1313361300001203.jpg\n",
            "./n1313361300001206.jpg\n",
            "./n1313361300001207.jpg\n",
            "./n1313361300001208.jpg\n",
            "./n1313361300001212.jpg\n",
            "./n1313361300001214.jpg\n",
            "./n1313361300001221.jpg\n",
            "./n1313361300001222.jpg\n",
            "./n1313361300001224.jpg\n",
            "./n1313361300001227.jpg\n",
            "./n1313361300001228.jpg\n",
            "./n1313361300001231.jpg\n",
            "./n1313361300001232.jpg\n",
            "./n1313361300001239.jpg\n",
            "./n1313361300001240.jpg\n",
            "./n1313361300001241.jpg\n",
            "./n1313361300001242.jpg\n",
            "./n1313361300001243.jpg\n",
            "./n1313361300001249.jpg\n",
            "./n1313361300001251.jpg\n",
            "./n1313361300001252.jpg\n",
            "./n1313361300001262.jpg\n",
            "./n1313361300001265.jpg\n",
            "./n1313361300001267.jpg\n",
            "./n1313361300001269.jpg\n",
            "./n1313361300001270.jpg\n",
            "./n1313361300001271.jpg\n",
            "./n1313361300001272.jpg\n",
            "./n1313361300001275.jpg\n",
            "./n1313361300001276.jpg\n",
            "./n1313361300001279.jpg\n",
            "./n1313361300001282.jpg\n",
            "./n1313361300001285.jpg\n",
            "./n1313361300001286.jpg\n",
            "./n1313361300001288.jpg\n",
            "./n1313361300001290.jpg\n",
            "./n1313361300001296.jpg\n",
            "./n1313361300001297.jpg\n",
            "./n1313361300001299.jpg\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images/\n",
        "!mv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images.tar /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images/\n",
        "!tar -xvf /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images/images.tar -C /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b8CqNHW6fnKN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "99ec4fecb0c94419ae509eb00fad2299",
            "d70c039d2d784bbfaf83dbfb1b74ddb8",
            "f2d70077865d4663b09554b7c5d10e2f",
            "f97fa8112be4450ea8c8f16d799d31ba",
            "f230aa795f9e4352a25adb655e13ffee",
            "b56c614297ec402fb134e3cae567f104",
            "20743def1ecb4fa7adb91097dc81fe95",
            "7011fa5ca6ee4393a4a21d96041f59ae",
            "2469637a18254489ad46e1dfb40ada7a",
            "e239262bed874a7b8dd9983d44b58c7e",
            "c71ed2c49bdc4fef94596072b985db40",
            "0eeafc134ec84401a853739dc9c702cf",
            "042f855804474183929114e46a46a6ad",
            "b434f06988ad446996819fa0e1a1df4a",
            "4ab09274d427411ba970caf24c328ca0",
            "1bef065127834e2a868b68b9385af84b",
            "78c3ba99e4b944f38c8c45603c447e3d",
            "9f2a4aa9cf634589971052659ffee2a3",
            "8b5ae37cabb940cdbcd687869e7df81a",
            "b24a022fd7064b1dae0dd71cd54e12a9",
            "498f7feb070047ebad81c9d0774fe668",
            "c5b23f77ec384d3387689525e2004f39",
            "0baef5ee89cc4d8ebc956b475b11d54d",
            "541a47f485e440aa9d20944e267edbf1",
            "f5864cbe97404b15be441e55b573471a",
            "8d19c147b6c44113b6f968bd79278a4f",
            "ca67930c379b41c39270722302b436ee",
            "06839d3b5c8a4f7fb52a419e6cdb47cb",
            "be994381dae4483c97d55280e30e2e79",
            "1ccef753661742a887096f56905e253f",
            "32d9ce4ca8d04de497781b11fd7fd01f",
            "989610b072fc4d55a60fcc2b0946b08b",
            "3b2f7b7c7d3e4c539c1d87577a27061d"
          ]
        },
        "outputId": "d0d20fb3-61b8-4551-9083-35371c1c2eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV files...\n",
            "\n",
            "Processing train split...\n",
            "Found 64 classes in train\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating class directories:   0%|          | 0/64 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99ec4fecb0c94419ae509eb00fad2299"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing val split...\n",
            "Found 16 classes in val\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating class directories:   0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eeafc134ec84401a853739dc9c702cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing test split...\n",
            "Found 20 classes in test\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating class directories:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0baef5ee89cc4d8ebc956b475b11d54d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Organization complete!\n",
            "\n",
            "train split statistics:\n",
            "Number of classes: 64\n",
            "Total images: 38400\n",
            "\n",
            "val split statistics:\n",
            "Number of classes: 16\n",
            "Total images: 9600\n",
            "\n",
            "test split statistics:\n",
            "Number of classes: 20\n",
            "Total images: 12000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Base path where your files are located\n",
        "base_path = \"/content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/\"\n",
        "\n",
        "# Output path for organized dataset\n",
        "output_path = os.path.join(base_path, 'organized')\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Read CSV files\n",
        "print(\"Reading CSV files...\")\n",
        "train_df = pd.read_csv(os.path.join(base_path, 'train.csv'))\n",
        "val_df = pd.read_csv(os.path.join(base_path, 'val.csv'))\n",
        "test_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\n",
        "\n",
        "# Process each split\n",
        "for split_name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f\"\\nProcessing {split_name} split...\")\n",
        "\n",
        "    # Get unique classes\n",
        "    classes = df['label'].unique()\n",
        "    print(f\"Found {len(classes)} classes in {split_name}\")\n",
        "\n",
        "    # Create directories for each class\n",
        "    for class_name in tqdm(classes, desc=\"Creating class directories\"):\n",
        "        class_dir = os.path.join(output_path, split_name, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        # Get all images for this class\n",
        "        class_images = df[df['label'] == class_name]['filename'].tolist()\n",
        "\n",
        "        # Move images to appropriate directory\n",
        "        for img_name in class_images:\n",
        "            src_path = os.path.join(base_path, 'images', img_name)\n",
        "            dst_path = os.path.join(class_dir, img_name)\n",
        "\n",
        "            if os.path.exists(src_path):\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "            else:\n",
        "                print(f\"Warning: {src_path} not found\")\n",
        "\n",
        "print(\"\\nOrganization complete!\")\n",
        "\n",
        "# Print some statistics\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    split_path = os.path.join(output_path, split_name)\n",
        "    if os.path.exists(split_path):\n",
        "        num_classes = len(os.listdir(split_path))\n",
        "        total_images = sum(len(os.listdir(os.path.join(split_path, class_name)))\n",
        "                         for class_name in os.listdir(split_path))\n",
        "        print(f\"\\n{split_name} split statistics:\")\n",
        "        print(f\"Number of classes: {num_classes}\")\n",
        "        print(f\"Total images: {total_images}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "41d3wfP-hfBL"
      },
      "outputs": [],
      "source": [
        "!mv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/organized/test /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/test\n",
        "!mv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/organized/val /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/val\n",
        "!mv /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/organized/train /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rdnQY7mygLQL"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/organized /content/HowToTrainYourMAMLPytorch/datasets/mini_imagenet_full_size/images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqgfzU5ElCLi"
      },
      "source": [
        "# MAML Classifier Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oanV_B5TD5Zj"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class GradientDescentLearningRule(nn.Module):\n",
        "    \"\"\"Simple (stochastic) gradient descent learning rule.\n",
        "    For a scalar error function `E(p[0], p_[1] ... )` of some set of\n",
        "    potentially multidimensional parameters this attempts to find a local\n",
        "    minimum of the loss function by applying updates to each parameter of the\n",
        "    form\n",
        "        p[i] := p[i] - learning_rate * dE/dp[i]\n",
        "    With `learning_rate` a positive scaling parameter.\n",
        "    The error function used in successive applications of these updates may be\n",
        "    a stochastic estimator of the true error function (e.g. when the error with\n",
        "    respect to only a subset of data-points is calculated) in which case this\n",
        "    will correspond to a stochastic gradient descent learning rule.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, learning_rate=1e-3):\n",
        "        \"\"\"Creates a new learning rule object.\n",
        "        Args:\n",
        "            learning_rate: A postive scalar to scale gradient updates to the\n",
        "                parameters by. This needs to be carefully set - if too large\n",
        "                the learning dynamic will be unstable and may diverge, while\n",
        "                if set too small learning will proceed very slowly.\n",
        "        \"\"\"\n",
        "        super(GradientDescentLearningRule, self).__init__()\n",
        "        assert learning_rate > 0., 'learning_rate should be positive.'\n",
        "        self.learning_rate = torch.ones(1) * learning_rate\n",
        "        self.learning_rate.to(device)\n",
        "\n",
        "    def update_params(self, names_weights_dict, names_grads_wrt_params_dict, num_step, tau=0.9):\n",
        "        \"\"\"Applies a single gradient descent update to all parameters.\n",
        "        All parameter updates are performed using in-place operations and so\n",
        "        nothing is returned.\n",
        "        Args:\n",
        "            grads_wrt_params: A list of gradients of the scalar loss function\n",
        "                with respect to each of the parameters passed to `initialise`\n",
        "                previously, with this list expected to be in the same order.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            key: names_weights_dict[key]\n",
        "            - self.learning_rate * names_grads_wrt_params_dict[key]\n",
        "            for key in names_weights_dict.keys()\n",
        "        }\n",
        "\n",
        "\n",
        "class LSLRGradientDescentLearningRule(nn.Module):\n",
        "    \"\"\"Simple (stochastic) gradient descent learning rule.\n",
        "    For a scalar error function `E(p[0], p_[1] ... )` of some set of\n",
        "    potentially multidimensional parameters this attempts to find a local\n",
        "    minimum of the loss function by applying updates to each parameter of the\n",
        "    form\n",
        "        p[i] := p[i] - learning_rate * dE/dp[i]\n",
        "    With `learning_rate` a positive scaling parameter.\n",
        "    The error function used in successive applications of these updates may be\n",
        "    a stochastic estimator of the true error function (e.g. when the error with\n",
        "    respect to only a subset of data-points is calculated) in which case this\n",
        "    will correspond to a stochastic gradient descent learning rule.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, total_num_inner_loop_steps, use_learnable_learning_rates, init_learning_rate=1e-3):\n",
        "        \"\"\"Creates a new learning rule object.\n",
        "        Args:\n",
        "            init_learning_rate: A postive scalar to scale gradient updates to the\n",
        "                parameters by. This needs to be carefully set - if too large\n",
        "                the learning dynamic will be unstable and may diverge, while\n",
        "                if set too small learning will proceed very slowly.\n",
        "        \"\"\"\n",
        "        super(LSLRGradientDescentLearningRule, self).__init__()\n",
        "        print(init_learning_rate)\n",
        "        assert init_learning_rate > 0., 'learning_rate should be positive.'\n",
        "\n",
        "        self.init_learning_rate = torch.ones(1) * init_learning_rate\n",
        "        self.init_learning_rate.to(device)\n",
        "        self.total_num_inner_loop_steps = total_num_inner_loop_steps\n",
        "        self.use_learnable_learning_rates = use_learnable_learning_rates\n",
        "\n",
        "    def initialise(self, names_weights_dict):\n",
        "        self.names_learning_rates_dict = nn.ParameterDict()\n",
        "        for idx, (key, param) in enumerate(names_weights_dict.items()):\n",
        "            self.names_learning_rates_dict[key.replace(\".\", \"-\")] = nn.Parameter(\n",
        "                data=torch.ones(self.total_num_inner_loop_steps + 1) * self.init_learning_rate,\n",
        "                requires_grad=self.use_learnable_learning_rates)\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        # for key, param in self.names_learning_rates_dict.items():\n",
        "        #     param.fill_(self.init_learning_rate)\n",
        "        pass\n",
        "\n",
        "    def update_params(self, names_weights_dict, names_grads_wrt_params_dict, num_step, tau=0.1):\n",
        "        \"\"\"Applies a single gradient descent update to all parameters.\n",
        "        All parameter updates are performed using in-place operations and so\n",
        "        nothing is returned.\n",
        "        Args:\n",
        "            grads_wrt_params: A list of gradients of the scalar loss function\n",
        "                with respect to each of the parameters passed to `initialise`\n",
        "                previously, with this list expected to be in the same order.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            key: names_weights_dict[key]\n",
        "            - self.names_learning_rates_dict[key.replace(\".\", \"-\")][num_step]\n",
        "            * names_grads_wrt_params_dict[key]\n",
        "            for key in names_grads_wrt_params_dict.keys()\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9t22ksVdiybq"
      },
      "outputs": [],
      "source": [
        "class MAMLFewShotClassifier(nn.Module):\n",
        "    def __init__(self, im_shape, device, args):\n",
        "        \"\"\"\n",
        "        Initializes a MAML few shot learning system\n",
        "        :param im_shape: The images input size, in batch, c, h, w shape\n",
        "        :param device: The device to use to use the model on.\n",
        "        :param args: A namedtuple of arguments specifying various hyperparameters.\n",
        "        \"\"\"\n",
        "        super(MAMLFewShotClassifier, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.batch_size = args.batch_size\n",
        "        self.use_cuda = args.use_cuda\n",
        "        self.im_shape = im_shape\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.rng = set_torch_seed(seed=args.seed)\n",
        "        self.classifier = VGGReLUNormNetwork(im_shape=self.im_shape, num_output_classes=self.args.\n",
        "                                             num_classes_per_set,\n",
        "                                             args=args, device=device, meta_classifier=True).to(device=self.device)\n",
        "        self.task_learning_rate = args.task_learning_rate\n",
        "\n",
        "        self.inner_loop_optimizer = LSLRGradientDescentLearningRule(device=device,\n",
        "                                                                    init_learning_rate=self.task_learning_rate,\n",
        "                                                                    total_num_inner_loop_steps=self.args.number_of_training_steps_per_iter,\n",
        "                                                                    use_learnable_learning_rates=self.args.learnable_per_layer_per_step_inner_loop_learning_rate)\n",
        "        self.inner_loop_optimizer.initialise(\n",
        "            names_weights_dict=self.get_inner_loop_parameter_dict(params=self.classifier.named_parameters()))\n",
        "\n",
        "        print(\"Inner Loop parameters\")\n",
        "        for key, value in self.inner_loop_optimizer.named_parameters():\n",
        "            print(key, value.shape)\n",
        "\n",
        "        self._noise_size = 0.1\n",
        "        self._importance_noise_size = 0\n",
        "\n",
        "        self.use_cuda = args.use_cuda\n",
        "        self.device = device\n",
        "        self.args = args\n",
        "        self.to(device)\n",
        "        print(\"Outer Loop parameters\")\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                print(name, param.shape, param.device, param.requires_grad)\n",
        "\n",
        "\n",
        "        self.optimizer = optim.Adam(self.trainable_parameters(), lr=args.meta_learning_rate, amsgrad=False)\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=self.optimizer, T_max=self.args.total_epochs,\n",
        "                                                              eta_min=self.args.min_learning_rate)\n",
        "\n",
        "        self.device = torch.device('cpu')\n",
        "        if torch.cuda.is_available():\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                self.to(torch.cuda.current_device())\n",
        "                self.classifier = nn.DataParallel(module=self.classifier)\n",
        "            else:\n",
        "                self.to(torch.cuda.current_device())\n",
        "\n",
        "            self.device = torch.cuda.current_device()\n",
        "\n",
        "    def get_per_step_loss_importance_vector(self):\n",
        "        \"\"\"\n",
        "        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n",
        "        loss towards the optimization loss.\n",
        "        :return: A tensor to be used to compute the weighted average of the loss, useful for\n",
        "        the MSL (Multi Step Loss) mechanism.\n",
        "        \"\"\"\n",
        "        loss_weights = np.ones(shape=(self.args.number_of_training_steps_per_iter)) * (\n",
        "                1.0 / self.args.number_of_training_steps_per_iter)\n",
        "        decay_rate = 1.0 / self.args.number_of_training_steps_per_iter / self.args.multi_step_loss_num_epochs\n",
        "        min_value_for_non_final_losses = 0.03 / self.args.number_of_training_steps_per_iter\n",
        "        for i in range(len(loss_weights) - 1):\n",
        "            curr_value = np.maximum(loss_weights[i] - (self.current_epoch * decay_rate), min_value_for_non_final_losses)\n",
        "            loss_weights[i] = curr_value\n",
        "\n",
        "        curr_value = np.minimum(\n",
        "            loss_weights[-1] + (self.current_epoch * (self.args.number_of_training_steps_per_iter - 1) * decay_rate),\n",
        "            1.0 - ((self.args.number_of_training_steps_per_iter - 1) * min_value_for_non_final_losses))\n",
        "        loss_weights[-1] = curr_value\n",
        "\n",
        "        # loss_weights += torch.randn_like(torch.Tensor(loss_weights)) * self._importance_noise_size\n",
        "        loss_weights = torch.Tensor(loss_weights).to(device=self.device)\n",
        "\n",
        "        return loss_weights\n",
        "\n",
        "    def get_inner_loop_parameter_dict(self, params):\n",
        "        \"\"\"\n",
        "        Returns a dictionary with the parameters to use for inner loop updates.\n",
        "        :param params: A dictionary of the network's parameters.\n",
        "        :return: A dictionary of the parameters to use for the inner loop optimization process.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            name: param.to(device=self.device)\n",
        "            for name, param in params\n",
        "            if param.requires_grad\n",
        "            and (\n",
        "                not self.args.enable_inner_loop_optimizable_bn_params\n",
        "                and \"norm_layer\" not in name\n",
        "                or self.args.enable_inner_loop_optimizable_bn_params\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def apply_inner_loop_update(self, loss, names_weights_copy, use_second_order, current_step_idx):\n",
        "        \"\"\"\n",
        "        Applies an inner loop update given current step's loss, the weights to update, a flag indicating whether to use\n",
        "        second order derivatives and the current step's index.\n",
        "        :param loss: Current step's loss with respect to the support set.\n",
        "        :param names_weights_copy: A dictionary with names to parameters to update.\n",
        "        :param use_second_order: A boolean flag of whether to use second order derivatives.\n",
        "        :param current_step_idx: Current step's index.\n",
        "        :return: A dictionary with the updated weights (name, param)\n",
        "        \"\"\"\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        if num_gpus > 1:\n",
        "            self.classifier.module.zero_grad(params=names_weights_copy)\n",
        "        else:\n",
        "            self.classifier.zero_grad(params=names_weights_copy)\n",
        "\n",
        "        grads = torch.autograd.grad(loss, names_weights_copy.values(),\n",
        "                                    create_graph=use_second_order, allow_unused=True)\n",
        "        names_grads_copy = dict(zip(names_weights_copy.keys(), grads))\n",
        "\n",
        "        names_weights_copy = {key: value[0] for key, value in names_weights_copy.items()}\n",
        "\n",
        "        for key, grad in names_grads_copy.items():\n",
        "            if grad is None:\n",
        "                print('Grads not found for inner loop parameter', key)\n",
        "            names_grads_copy[key] = names_grads_copy[key].sum(dim=0)\n",
        "\n",
        "\n",
        "        names_weights_copy = self.inner_loop_optimizer.update_params(names_weights_dict=names_weights_copy,\n",
        "                                                                     names_grads_wrt_params_dict=names_grads_copy,\n",
        "                                                                     num_step=current_step_idx)\n",
        "\n",
        "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "        names_weights_copy = {\n",
        "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
        "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
        "            name, value in names_weights_copy.items()}\n",
        "\n",
        "\n",
        "        return names_weights_copy\n",
        "\n",
        "    def get_across_task_loss_metrics(self, total_losses, total_accuracies):\n",
        "        losses = {'loss': torch.mean(torch.stack(total_losses))}\n",
        "\n",
        "        losses['accuracy'] = np.mean(total_accuracies)\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def forward(self, data_batch, epoch, use_second_order, use_multi_step_loss_optimization, num_steps, training_phase):\n",
        "        \"\"\"\n",
        "        Runs a forward outer loop pass on the batch of tasks using the MAML/++ framework.\n",
        "        :param data_batch: A data batch containing the support and target sets.\n",
        "        :param epoch: Current epoch's index\n",
        "        :param use_second_order: A boolean saying whether to use second order derivatives.\n",
        "        :param use_multi_step_loss_optimization: Whether to optimize on the outer loop using just the last step's\n",
        "        target loss (True) or whether to use multi step loss which improves the stability of the system (False)\n",
        "        :param num_steps: Number of inner loop steps.\n",
        "        :param training_phase: Whether this is a training phase (True) or an evaluation phase (False)\n",
        "        :return: A dictionary with the collected losses of the current outer forward propagation.\n",
        "        \"\"\"\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set = data_batch\n",
        "\n",
        "        [b, ncs, spc] = y_support_set.shape\n",
        "\n",
        "        self.num_classes_per_set = ncs\n",
        "\n",
        "        total_losses = []\n",
        "        total_accuracies = []\n",
        "        per_task_target_preds = [[] for i in range(len(x_target_set))]\n",
        "        self.classifier.zero_grad()\n",
        "        task_accuracies = []\n",
        "        for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
        "                              y_support_set,\n",
        "                              x_target_set,\n",
        "                              y_target_set)):\n",
        "            task_losses = []\n",
        "            per_step_loss_importance_vectors = self.get_per_step_loss_importance_vector()\n",
        "            names_weights_copy = self.get_inner_loop_parameter_dict(self.classifier.named_parameters())\n",
        "\n",
        "            num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "\n",
        "            names_weights_copy = {\n",
        "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
        "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
        "                name, value in names_weights_copy.items()}\n",
        "\n",
        "            n, s, c, h, w = x_target_set_task.shape\n",
        "\n",
        "            x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
        "            y_support_set_task = y_support_set_task.view(-1)\n",
        "            x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
        "            y_target_set_task = y_target_set_task.view(-1)\n",
        "\n",
        "            for num_step in range(num_steps):\n",
        "\n",
        "                support_loss, support_preds = self.net_forward(\n",
        "                    x=x_support_set_task,\n",
        "                    y=y_support_set_task,\n",
        "                    weights=names_weights_copy,\n",
        "                    backup_running_statistics=num_step == 0,\n",
        "                    training=True,\n",
        "                    num_step=num_step,\n",
        "                )\n",
        "\n",
        "\n",
        "                names_weights_copy = self.apply_inner_loop_update(loss=support_loss,\n",
        "                                                                  names_weights_copy=names_weights_copy,\n",
        "                                                                  use_second_order=use_second_order,\n",
        "                                                                  current_step_idx=num_step)\n",
        "\n",
        "                if use_multi_step_loss_optimization and training_phase and epoch < self.args.multi_step_loss_num_epochs:\n",
        "                    target_loss, target_preds = self.net_forward(x=x_target_set_task,\n",
        "                                                                 y=y_target_set_task, weights=names_weights_copy,\n",
        "                                                                 backup_running_statistics=False, training=True,\n",
        "                                                                 num_step=num_step)\n",
        "\n",
        "                    task_losses.append(per_step_loss_importance_vectors[num_step] * target_loss)\n",
        "                elif num_step == (self.args.number_of_training_steps_per_iter - 1):\n",
        "                    target_loss, target_preds = self.net_forward(x=x_target_set_task,\n",
        "                                                                 y=y_target_set_task, weights=names_weights_copy,\n",
        "                                                                 backup_running_statistics=False, training=True,\n",
        "                                                                 num_step=num_step)\n",
        "                    task_losses.append(target_loss)\n",
        "\n",
        "            per_task_target_preds[task_id] = target_preds.detach().cpu().numpy()\n",
        "            _, predicted = torch.max(target_preds.data, 1)\n",
        "\n",
        "            accuracy = predicted.float().eq(y_target_set_task.data.float()).cpu().float()\n",
        "            task_losses = torch.sum(torch.stack(task_losses))\n",
        "            total_losses.append(task_losses)\n",
        "            total_accuracies.extend(accuracy)\n",
        "\n",
        "            if not training_phase:\n",
        "                self.classifier.restore_backup_stats()\n",
        "\n",
        "        losses = self.get_across_task_loss_metrics(total_losses=total_losses,\n",
        "                                                   total_accuracies=total_accuracies)\n",
        "\n",
        "        for idx, item in enumerate(per_step_loss_importance_vectors):\n",
        "            losses['loss_importance_vector_{}'.format(idx)] = item.detach().cpu().numpy()\n",
        "\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def net_forward(self, x, y, weights, backup_running_statistics, training, num_step):\n",
        "        \"\"\"\n",
        "        A base model forward pass on some data points x. Using the parameters in the weights dictionary. Also requires\n",
        "        boolean flags indicating whether to reset the running statistics at the end of the run (if at evaluation phase).\n",
        "        A flag indicating whether this is the training session and an int indicating the current step's number in the\n",
        "        inner loop.\n",
        "        :param x: A data batch of shape b, c, h, w\n",
        "        :param y: A data targets batch of shape b, n_classes\n",
        "        :param weights: A dictionary containing the weights to pass to the network.\n",
        "        :param backup_running_statistics: A flag indicating whether to reset the batch norm running statistics to their\n",
        "         previous values after the run (only for evaluation)\n",
        "        :param training: A flag indicating whether the current process phase is a training or evaluation.\n",
        "        :param num_step: An integer indicating the number of the step in the inner loop.\n",
        "        :return: the crossentropy losses with respect to the given y, the predictions of the base model.\n",
        "        \"\"\"\n",
        "        preds = self.classifier.forward(x=x, params=weights,\n",
        "                                        training=training,\n",
        "                                        backup_running_statistics=backup_running_statistics, num_step=num_step)\n",
        "\n",
        "        loss = F.cross_entropy(input=preds, target=y)\n",
        "\n",
        "        return loss, preds\n",
        "\n",
        "    def trainable_parameters(self):\n",
        "        \"\"\"\n",
        "        Returns an iterator over the trainable parameters of the model.\n",
        "        \"\"\"\n",
        "        for param in self.parameters():\n",
        "            if param.requires_grad:\n",
        "                noise = torch.randn(param.shape, device=self.device) * self._noise_size  # TODO EXPERIMENT WITH THIS\n",
        "                print(f\"the grinch added {noise} amount of noise :)\")\n",
        "                param.data.add_(noise)\n",
        "                yield param\n",
        "\n",
        "    def train_forward_prop(self, data_batch, epoch):\n",
        "        \"\"\"\n",
        "        Runs an outer loop forward prop using the meta-model and base-model.\n",
        "        :param data_batch: A data batch containing the support set and the target set input, output pairs.\n",
        "        :param epoch: The index of the currrent epoch.\n",
        "        :return: A dictionary of losses for the current step.\n",
        "        \"\"\"\n",
        "        losses, per_task_target_preds = self.forward(data_batch=data_batch, epoch=epoch,\n",
        "                                                     use_second_order=self.args.second_order and\n",
        "                                                                      epoch > self.args.first_order_to_second_order_epoch,\n",
        "                                                     use_multi_step_loss_optimization=self.args.use_multi_step_loss_optimization,\n",
        "                                                     num_steps=self.args.number_of_training_steps_per_iter,\n",
        "                                                     training_phase=True)\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def evaluation_forward_prop(self, data_batch, epoch):\n",
        "        \"\"\"\n",
        "        Runs an outer loop evaluation forward prop using the meta-model and base-model.\n",
        "        :param data_batch: A data batch containing the support set and the target set input, output pairs.\n",
        "        :param epoch: The index of the currrent epoch.\n",
        "        :return: A dictionary of losses for the current step.\n",
        "        \"\"\"\n",
        "        losses, per_task_target_preds = self.forward(data_batch=data_batch, epoch=epoch, use_second_order=False,\n",
        "                                                     use_multi_step_loss_optimization=True,\n",
        "                                                     num_steps=self.args.number_of_evaluation_steps_per_iter,\n",
        "                                                     training_phase=False)\n",
        "\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def meta_update(self, loss):\n",
        "        \"\"\"\n",
        "        Applies an outer loop update on the meta-parameters of the model.\n",
        "        :param loss: The current crossentropy loss.\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if 'imagenet' in self.args.dataset_name:\n",
        "            for name, param in self.classifier.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    param.grad.data.clamp_(-10, 10)  # not sure if this is necessary, more experiments are needed\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def run_train_iter(self, data_batch, epoch):\n",
        "        \"\"\"\n",
        "        Runs an outer loop update step on the meta-model's parameters.\n",
        "        :param data_batch: input data batch containing the support set and target set input, output pairs\n",
        "        :param epoch: the index of the current epoch\n",
        "        :return: The losses of the ran iteration.\n",
        "        \"\"\"\n",
        "        epoch = int(epoch)\n",
        "        if epoch > 1:\n",
        "          self.scheduler.step(epoch=epoch)\n",
        "        if self.current_epoch != epoch:\n",
        "            self.current_epoch = epoch\n",
        "\n",
        "        if not self.training:\n",
        "            self.train()\n",
        "\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set = data_batch\n",
        "\n",
        "        x_support_set = torch.Tensor(x_support_set).float().to(device=self.device)\n",
        "        x_target_set = torch.Tensor(x_target_set).float().to(device=self.device)\n",
        "        y_support_set = torch.Tensor(y_support_set).long().to(device=self.device)\n",
        "        y_target_set = torch.Tensor(y_target_set).long().to(device=self.device)\n",
        "\n",
        "        data_batch = (x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        losses, per_task_target_preds = self.train_forward_prop(data_batch=data_batch, epoch=epoch)\n",
        "\n",
        "        self.meta_update(loss=losses['loss'])\n",
        "        losses['learning_rate'] = self.scheduler.get_last_lr()[0]\n",
        "        self.optimizer.zero_grad()\n",
        "        self.zero_grad()\n",
        "\n",
        "        self._noise_size *= 0.9999\n",
        "        # print(f\"noise size: {self._noise_size}\")\n",
        "\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def run_validation_iter(self, data_batch):\n",
        "        \"\"\"\n",
        "        Runs an outer loop evaluation step on the meta-model's parameters.\n",
        "        :param data_batch: input data batch containing the support set and target set input, output pairs\n",
        "        :param epoch: the index of the current epoch\n",
        "        :return: The losses of the ran iteration.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.training:\n",
        "            self.eval()\n",
        "\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set = data_batch\n",
        "\n",
        "        x_support_set = torch.Tensor(x_support_set).float().to(device=self.device)\n",
        "        x_target_set = torch.Tensor(x_target_set).float().to(device=self.device)\n",
        "        y_support_set = torch.Tensor(y_support_set).long().to(device=self.device)\n",
        "        y_target_set = torch.Tensor(y_target_set).long().to(device=self.device)\n",
        "\n",
        "        data_batch = (x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        losses, per_task_target_preds = self.evaluation_forward_prop(data_batch=data_batch, epoch=self.current_epoch)\n",
        "\n",
        "        # losses['loss'].backward() # uncomment if you get the weird memory error\n",
        "        # self.zero_grad()\n",
        "        # self.optimizer.zero_grad()\n",
        "\n",
        "        return losses, per_task_target_preds\n",
        "\n",
        "    def save_model(self, model_save_dir, state):\n",
        "        \"\"\"\n",
        "        Save the network parameter state and experiment state dictionary.\n",
        "        :param model_save_dir: The directory to store the state at.\n",
        "        :param state: The state containing the experiment state and the network. It's in the form of a dictionary\n",
        "        object.\n",
        "        \"\"\"\n",
        "        state['network'] = self.state_dict()\n",
        "        state['optimizer'] = self.optimizer.state_dict()\n",
        "        torch.save(state, f=model_save_dir)\n",
        "\n",
        "    def load_model(self, model_save_dir, model_name, model_idx):\n",
        "        \"\"\"\n",
        "        Load checkpoint and return the state dictionary containing the network state params and experiment state.\n",
        "        :param model_save_dir: The directory from which to load the files.\n",
        "        :param model_name: The model_name to be loaded from the direcotry.\n",
        "        :param model_idx: The index of the model (i.e. epoch number or 'latest' for the latest saved model of the current\n",
        "        experiment)\n",
        "        :return: A dictionary containing the experiment state and the saved model parameters.\n",
        "        \"\"\"\n",
        "        filepath = os.path.join(model_save_dir, \"{}_{}\".format(model_name, model_idx))\n",
        "        state = torch.load(filepath)\n",
        "        state_dict_loaded = state['network']\n",
        "        self.optimizer.load_state_dict(state['optimizer'])\n",
        "        self.load_state_dict(state_dict=state_dict_loaded)\n",
        "        return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTJUQiySnDQi"
      },
      "source": [
        "# Layer Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w0JKoFtCnWZZ"
      },
      "outputs": [],
      "source": [
        "import numbers\n",
        "from copy import copy\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_ZTbS3zjnOD5"
      },
      "outputs": [],
      "source": [
        "def extract_top_level_dict(current_dict):\n",
        "    \"\"\"\n",
        "    Builds a graph dictionary from the passed depth_keys, value pair. Useful for dynamically passing external params\n",
        "    :param depth_keys: A list of strings making up the name of a variable. Used to make a graph for that params tree.\n",
        "    :param value: Param value\n",
        "    :param key_exists: If none then assume new dict, else load existing dict and add new key->value pairs to it.\n",
        "    :return: A dictionary graph of the params already added to the graph.\n",
        "    \"\"\"\n",
        "    output_dict = {}\n",
        "    for key in current_dict.keys():\n",
        "        name = key.replace(\"layer_dict.\", \"\")\n",
        "        name = name.replace(\"layer_dict.\", \"\")\n",
        "        name = name.replace(\"block_dict.\", \"\")\n",
        "        name = name.replace(\"module-\", \"\")\n",
        "        top_level = name.split(\".\")[0]\n",
        "        sub_level = \".\".join(name.split(\".\")[1:])\n",
        "\n",
        "        if top_level in output_dict:\n",
        "            new_item = {key: value for key, value in output_dict[top_level].items()}\n",
        "            new_item[sub_level] = current_dict[key]\n",
        "            output_dict[top_level] = new_item\n",
        "\n",
        "        elif sub_level == \"\":\n",
        "            output_dict[top_level] = current_dict[key]\n",
        "        else:\n",
        "            output_dict[top_level] = {sub_level: current_dict[key]}\n",
        "    #print(current_dict.keys(), output_dict.keys())\n",
        "    return output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y1N7ERJmnZpx"
      },
      "outputs": [],
      "source": [
        "class MetaConv2dLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, use_bias, groups=1, dilation_rate=1):\n",
        "        \"\"\"\n",
        "        A MetaConv2D layer. Applies the same functionality of a standard Conv2D layer with the added functionality of\n",
        "        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n",
        "        weights instead of the internal ones stored in the conv layer. Useful for inner loop optimization in the meta\n",
        "        learning setting.\n",
        "        :param in_channels: Number of input channels\n",
        "        :param out_channels: Number of output channels\n",
        "        :param kernel_size: Convolutional kernel size\n",
        "        :param stride: Convolutional stride\n",
        "        :param padding: Convolution padding\n",
        "        :param use_bias: Boolean indicating whether to use a bias or not.\n",
        "        \"\"\"\n",
        "        super(MetaConv2dLayer, self).__init__()\n",
        "        num_filters = out_channels\n",
        "        self.stride = int(stride)\n",
        "        self.padding = int(padding)\n",
        "        self.dilation_rate = int(dilation_rate)\n",
        "        self.use_bias = use_bias\n",
        "        self.groups = int(groups)\n",
        "        self.weight = nn.Parameter(torch.empty(num_filters, in_channels, kernel_size, kernel_size))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(num_filters))\n",
        "\n",
        "    def forward(self, x, params=None):\n",
        "        \"\"\"\n",
        "        Applies a conv2D forward pass. If params are not None will use the passed params as the conv weights and biases\n",
        "        :param x: Input image batch.\n",
        "        :param params: If none, then conv layer will use the stored self.weights and self.bias, if they are not none\n",
        "        then the conv layer will use the passed params as its parameters.\n",
        "        :return: The output of a convolutional function.\n",
        "        \"\"\"\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "            if self.use_bias:\n",
        "                (weight, bias) = params[\"weight\"], params[\"bias\"]\n",
        "            else:\n",
        "                (weight) = params[\"weight\"]\n",
        "                bias = None\n",
        "        elif self.use_bias:\n",
        "            weight, bias = self.weight, self.bias\n",
        "        else:\n",
        "            weight = self.weight\n",
        "            bias = None\n",
        "\n",
        "        return F.conv2d(\n",
        "            input=x,\n",
        "            weight=weight,\n",
        "            bias=bias,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding,\n",
        "            dilation=self.dilation_rate,\n",
        "            groups=self.groups,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EvrE9o8Vnf6q"
      },
      "outputs": [],
      "source": [
        "class MetaLinearLayer(nn.Module):\n",
        "    def __init__(self, input_shape, num_filters, use_bias):\n",
        "        \"\"\"\n",
        "        A MetaLinear layer. Applies the same functionality of a standard linearlayer with the added functionality of\n",
        "        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n",
        "        weights instead of the internal ones stored in the linear layer. Useful for inner loop optimization in the meta\n",
        "        learning setting.\n",
        "        :param input_shape: The shape of the input data, in the form (b, f)\n",
        "        :param num_filters: Number of output filters\n",
        "        :param use_bias: Whether to use biases or not.\n",
        "        \"\"\"\n",
        "        super(MetaLinearLayer, self).__init__()\n",
        "        b, c = input_shape\n",
        "\n",
        "        self.use_bias = use_bias\n",
        "        self.weights = nn.Parameter(torch.ones(num_filters, c))\n",
        "        # nn.init.xavier_uniform_(self.weights) TODO CHANGE BACK TEST GENE (it works kinda)\n",
        "        if self.use_bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(num_filters))\n",
        "\n",
        "    def forward(self, x, params=None):\n",
        "        \"\"\"\n",
        "        Forward propagates by applying a linear function (Wx + b). If params are none then internal params are used.\n",
        "        Otherwise passed params will be used to execute the function.\n",
        "        :param x: Input data batch, in the form (b, f)\n",
        "        :param params: A dictionary containing 'weights' and 'bias'. If params are none then internal params are used.\n",
        "        Otherwise the external are used.\n",
        "        :return: The result of the linear function.\n",
        "        \"\"\"\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "            if self.use_bias:\n",
        "                (weight, bias) = params[\"weights\"], params[\"bias\"]\n",
        "            else:\n",
        "                (weight) = params[\"weights\"]\n",
        "                bias = None\n",
        "        elif self.use_bias:\n",
        "            weight, bias = self.weights, self.bias\n",
        "        else:\n",
        "            weight = self.weights\n",
        "            bias = None\n",
        "        return F.linear(input=x, weight=weight, bias=bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b4UV1OiDnkU7"
      },
      "outputs": [],
      "source": [
        "class MetaBatchNormLayer(nn.Module):\n",
        "    def __init__(self, num_features, device, args, eps=1e-5, momentum=0.1, affine=True,\n",
        "                 track_running_stats=True, meta_batch_norm=True, no_learnable_params=False,\n",
        "                 use_per_step_bn_statistics=False):\n",
        "        \"\"\"\n",
        "        A MetaBatchNorm layer. Applies the same functionality of a standard BatchNorm layer with the added functionality of\n",
        "        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n",
        "        weights instead of the internal ones stored in the conv layer. Useful for inner loop optimization in the meta\n",
        "        learning setting. Also has the additional functionality of being able to store per step running stats and per step beta and gamma.\n",
        "        :param num_features:\n",
        "        :param device:\n",
        "        :param args:\n",
        "        :param eps:\n",
        "        :param momentum:\n",
        "        :param affine:\n",
        "        :param track_running_stats:\n",
        "        :param meta_batch_norm:\n",
        "        :param no_learnable_params:\n",
        "        :param use_per_step_bn_statistics:\n",
        "        \"\"\"\n",
        "        super(MetaBatchNormLayer, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "\n",
        "        self.affine = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.meta_batch_norm = meta_batch_norm\n",
        "        self.num_features = num_features\n",
        "        self.device = device\n",
        "        self.use_per_step_bn_statistics = use_per_step_bn_statistics\n",
        "        self.args = args\n",
        "        self.learnable_gamma = self.args.learnable_bn_gamma\n",
        "        self.learnable_beta = self.args.learnable_bn_beta\n",
        "\n",
        "        if use_per_step_bn_statistics:\n",
        "            self.running_mean = nn.Parameter(torch.zeros(args.number_of_training_steps_per_iter, num_features),\n",
        "                                             requires_grad=False)\n",
        "            self.running_var = nn.Parameter(torch.ones(args.number_of_training_steps_per_iter, num_features),\n",
        "                                            requires_grad=False)\n",
        "            self.bias = nn.Parameter(torch.zeros(args.number_of_training_steps_per_iter, num_features),\n",
        "                                     requires_grad=self.learnable_beta)\n",
        "            self.weight = nn.Parameter(torch.ones(args.number_of_training_steps_per_iter, num_features),\n",
        "                                       requires_grad=self.learnable_gamma)\n",
        "        else:\n",
        "            self.running_mean = nn.Parameter(torch.zeros(num_features), requires_grad=False)\n",
        "            self.running_var = nn.Parameter(torch.zeros(num_features), requires_grad=False)\n",
        "            self.bias = nn.Parameter(torch.zeros(num_features),\n",
        "                                     requires_grad=self.learnable_beta)\n",
        "            self.weight = nn.Parameter(torch.ones(num_features),\n",
        "                                       requires_grad=self.learnable_gamma)\n",
        "\n",
        "        if self.args.enable_inner_loop_optimizable_bn_params:\n",
        "            self.bias = nn.Parameter(torch.zeros(num_features),\n",
        "                                     requires_grad=self.learnable_beta)\n",
        "            self.weight = nn.Parameter(torch.ones(num_features),\n",
        "                                       requires_grad=self.learnable_gamma)\n",
        "\n",
        "        self.backup_running_mean = torch.zeros(self.running_mean.shape)\n",
        "        self.backup_running_var = torch.ones(self.running_var.shape)\n",
        "\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def forward(self, input, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "        Forward propagates by applying a bach norm function. If params are none then internal params are used.\n",
        "        Otherwise passed params will be used to execute the function.\n",
        "        :param input: input data batch, size either can be any.\n",
        "        :param num_step: The current inner loop step being taken. This is used when we are learning per step params and\n",
        "         collecting per step batch statistics. It indexes the correct object to use for the current time-step\n",
        "        :param params: A dictionary containing 'weight' and 'bias'.\n",
        "        :param training: Whether this is currently the training or evaluation phase.\n",
        "        :param backup_running_statistics: Whether to backup the running statistics. This is used\n",
        "        at evaluation time, when after the pass is complete we want to throw away the collected validation stats.\n",
        "        :return: The result of the batch norm operation.\n",
        "        \"\"\"\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "            (weight, bias) = params[\"weight\"], params[\"bias\"]\n",
        "            #print(num_step, params['weight'])\n",
        "        else:\n",
        "            #print(num_step, \"no params\")\n",
        "            weight, bias = self.weight, self.bias\n",
        "\n",
        "        if self.use_per_step_bn_statistics:\n",
        "            running_mean = self.running_mean[num_step]\n",
        "            running_var = self.running_var[num_step]\n",
        "            if (\n",
        "                params is None\n",
        "                and not self.args.enable_inner_loop_optimizable_bn_params\n",
        "            ):\n",
        "                bias = self.bias[num_step]\n",
        "                weight = self.weight[num_step]\n",
        "        else:\n",
        "            running_mean = None\n",
        "            running_var = None\n",
        "\n",
        "\n",
        "        if backup_running_statistics and self.use_per_step_bn_statistics:\n",
        "            self.backup_running_mean.data = copy(self.running_mean.data)\n",
        "            self.backup_running_var.data = copy(self.running_var.data)\n",
        "\n",
        "        momentum = self.momentum\n",
        "\n",
        "        return F.batch_norm(input, running_mean, running_var, weight, bias,\n",
        "                              training=True, momentum=momentum, eps=self.eps)\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        \"\"\"\n",
        "        Resets batch statistics to their backup values which are collected after each forward pass.\n",
        "        \"\"\"\n",
        "        if self.use_per_step_bn_statistics:\n",
        "            self.running_mean = nn.Parameter(self.backup_running_mean.to(device=self.device), requires_grad=False)\n",
        "            self.running_var = nn.Parameter(self.backup_running_var.to(device=self.device), requires_grad=False)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n",
        "               'track_running_stats={track_running_stats}'.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6nEWVSSLnna2"
      },
      "outputs": [],
      "source": [
        "class MetaLayerNormLayer(nn.Module):\n",
        "    def __init__(self, input_feature_shape, eps=1e-5, elementwise_affine=True):\n",
        "        \"\"\"\n",
        "        A MetaLayerNorm layer. A layer that applies the same functionality as a layer norm layer with the added\n",
        "        capability of being able to receive params at inference time to use instead of the internal ones. As well as\n",
        "        being able to use its own internal weights.\n",
        "        :param input_feature_shape: The input shape without the batch dimension, e.g. c, h, w\n",
        "        :param eps: Epsilon to use for protection against overflows\n",
        "        :param elementwise_affine: Whether to learn a multiplicative interaction parameter 'w' in addition to\n",
        "        the biases.\n",
        "        \"\"\"\n",
        "        super(MetaLayerNormLayer, self).__init__()\n",
        "        if isinstance(input_feature_shape, numbers.Integral):\n",
        "            input_feature_shape = (input_feature_shape,)\n",
        "        self.normalized_shape = torch.Size(input_feature_shape)\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(*input_feature_shape), requires_grad=False)\n",
        "            self.bias = nn.Parameter(torch.Tensor(*input_feature_shape))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Reset parameters to their initialization values.\n",
        "        \"\"\"\n",
        "        if self.elementwise_affine:\n",
        "            self.weight.data.fill_(1)\n",
        "            self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "            Forward propagates by applying a layer norm function. If params are none then internal params are used.\n",
        "            Otherwise passed params will be used to execute the function.\n",
        "            :param input: input data batch, size either can be any.\n",
        "            :param num_step: The current inner loop step being taken. This is used when we are learning per step params and\n",
        "             collecting per step batch statistics. It indexes the correct object to use for the current time-step\n",
        "            :param params: A dictionary containing 'weight' and 'bias'.\n",
        "            :param training: Whether this is currently the training or evaluation phase.\n",
        "            :param backup_running_statistics: Whether to backup the running statistics. This is used\n",
        "            at evaluation time, when after the pass is complete we want to throw away the collected validation stats.\n",
        "            :return: The result of the batch norm operation.\n",
        "        \"\"\"\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "            bias = params[\"bias\"]\n",
        "        else:\n",
        "            bias = self.bias\n",
        "            #print('no inner loop params', self)\n",
        "\n",
        "        return F.layer_norm(\n",
        "            input, self.normalized_shape, self.weight, bias, self.eps)\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        pass\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{normalized_shape}, eps={eps}, ' \\\n",
        "               'elementwise_affine={elementwise_affine}'.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "j1Jj9kodnrOV"
      },
      "outputs": [],
      "source": [
        "class MetaConvNormLayerReLU(nn.Module):\n",
        "    def __init__(self, input_shape, num_filters, kernel_size, stride, padding, use_bias, args, normalization=True,\n",
        "                 meta_layer=True, no_bn_learnable_params=False, device=None):\n",
        "        \"\"\"\n",
        "           Initializes a BatchNorm->Conv->ReLU layer which applies those operation in that order.\n",
        "           :param args: A named tuple containing the system's hyperparameters.\n",
        "           :param device: The device to run the layer on.\n",
        "           :param normalization: The type of normalization to use 'batch_norm' or 'layer_norm'\n",
        "           :param meta_layer: Whether this layer will require meta-layer capabilities such as meta-batch norm,\n",
        "           meta-conv etc.\n",
        "           :param input_shape: The image input shape in the form (b, c, h, w)\n",
        "           :param num_filters: number of filters for convolutional layer\n",
        "           :param kernel_size: the kernel size of the convolutional layer\n",
        "           :param stride: the stride of the convolutional layer\n",
        "           :param padding: the bias of the convolutional layer\n",
        "           :param use_bias: whether the convolutional layer utilizes a bias\n",
        "        \"\"\"\n",
        "        super(MetaConvNormLayerReLU, self).__init__()\n",
        "        self.normalization = normalization\n",
        "        self.use_per_step_bn_statistics = args.per_step_bn_statistics\n",
        "        self.input_shape = input_shape\n",
        "        self.args = args\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "        self.meta_layer = meta_layer\n",
        "        self.no_bn_learnable_params = no_bn_learnable_params\n",
        "        self.device = device\n",
        "        self.layer_dict = nn.ModuleDict()\n",
        "        self.build_block()\n",
        "\n",
        "    def build_block(self):\n",
        "\n",
        "        x = torch.zeros(self.input_shape)\n",
        "\n",
        "        out = x\n",
        "\n",
        "        self.conv = MetaConv2dLayer(in_channels=out.shape[1], out_channels=self.num_filters,\n",
        "                                    kernel_size=self.kernel_size,\n",
        "                                    stride=self.stride, padding=self.padding, use_bias=self.use_bias)\n",
        "\n",
        "\n",
        "\n",
        "        out = self.conv(out)\n",
        "\n",
        "        if self.normalization:\n",
        "            if self.args.norm_layer == \"batch_norm\":\n",
        "                self.norm_layer = MetaBatchNormLayer(out.shape[1], track_running_stats=True,\n",
        "                                                     meta_batch_norm=self.meta_layer,\n",
        "                                                     no_learnable_params=self.no_bn_learnable_params,\n",
        "                                                     device=self.device,\n",
        "                                                     use_per_step_bn_statistics=self.use_per_step_bn_statistics,\n",
        "                                                     args=self.args)\n",
        "            elif self.args.norm_layer == \"layer_norm\":\n",
        "                self.norm_layer = MetaLayerNormLayer(input_feature_shape=out.shape[1:])\n",
        "\n",
        "            out = self.norm_layer(out, num_step=0)\n",
        "\n",
        "        out = F.leaky_relu(out)\n",
        "\n",
        "        print(out.shape)\n",
        "\n",
        "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "            Forward propagates by applying the function. If params are none then internal params are used.\n",
        "            Otherwise passed params will be used to execute the function.\n",
        "            :param input: input data batch, size either can be any.\n",
        "            :param num_step: The current inner loop step being taken. This is used when we are learning per step params and\n",
        "             collecting per step batch statistics. It indexes the correct object to use for the current time-step\n",
        "            :param params: A dictionary containing 'weight' and 'bias'.\n",
        "            :param training: Whether this is currently the training or evaluation phase.\n",
        "            :param backup_running_statistics: Whether to backup the running statistics. This is used\n",
        "            at evaluation time, when after the pass is complete we want to throw away the collected validation stats.\n",
        "            :return: The result of the batch norm operation.\n",
        "        \"\"\"\n",
        "        batch_norm_params = None\n",
        "        conv_params = None\n",
        "        activation_function_pre_params = None\n",
        "\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "\n",
        "            if self.normalization:\n",
        "                if 'norm_layer' in params:\n",
        "                    batch_norm_params = params['norm_layer']\n",
        "\n",
        "                if 'activation_function_pre' in params:\n",
        "                    activation_function_pre_params = params['activation_function_pre']\n",
        "\n",
        "            conv_params = params['conv']\n",
        "\n",
        "        out = x\n",
        "\n",
        "\n",
        "        out = self.conv(out, params=conv_params)\n",
        "\n",
        "        if self.normalization:\n",
        "            out = self.norm_layer.forward(out, num_step=num_step,\n",
        "                                          params=batch_norm_params, training=training,\n",
        "                                          backup_running_statistics=backup_running_statistics)\n",
        "\n",
        "        out = F.leaky_relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        \"\"\"\n",
        "        Restore stored statistics from the backup, replacing the current ones.\n",
        "        \"\"\"\n",
        "        if self.normalization:\n",
        "            self.norm_layer.restore_backup_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cl6609dBntk4"
      },
      "outputs": [],
      "source": [
        "class MetaNormLayerConvReLU(nn.Module):\n",
        "    def __init__(self, input_shape, num_filters, kernel_size, stride, padding, use_bias, args, normalization=True,\n",
        "                 meta_layer=True, no_bn_learnable_params=False, device=None):\n",
        "        \"\"\"\n",
        "           Initializes a BatchNorm->Conv->ReLU layer which applies those operation in that order.\n",
        "           :param args: A named tuple containing the system's hyperparameters.\n",
        "           :param device: The device to run the layer on.\n",
        "           :param normalization: The type of normalization to use 'batch_norm' or 'layer_norm'\n",
        "           :param meta_layer: Whether this layer will require meta-layer capabilities such as meta-batch norm,\n",
        "           meta-conv etc.\n",
        "           :param input_shape: The image input shape in the form (b, c, h, w)\n",
        "           :param num_filters: number of filters for convolutional layer\n",
        "           :param kernel_size: the kernel size of the convolutional layer\n",
        "           :param stride: the stride of the convolutional layer\n",
        "           :param padding: the bias of the convolutional layer\n",
        "           :param use_bias: whether the convolutional layer utilizes a bias\n",
        "        \"\"\"\n",
        "        super(MetaNormLayerConvReLU, self).__init__()\n",
        "        self.normalization = normalization\n",
        "        self.use_per_step_bn_statistics = args.per_step_bn_statistics\n",
        "        self.input_shape = input_shape\n",
        "        self.args = args\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "        self.meta_layer = meta_layer\n",
        "        self.no_bn_learnable_params = no_bn_learnable_params\n",
        "        self.device = device\n",
        "        self.layer_dict = nn.ModuleDict()\n",
        "        self.build_block()\n",
        "\n",
        "    def build_block(self):\n",
        "\n",
        "        x = torch.zeros(self.input_shape)\n",
        "\n",
        "        out = x\n",
        "        if self.normalization:\n",
        "            if self.args.norm_layer == \"batch_norm\":\n",
        "                self.norm_layer = MetaBatchNormLayer(self.input_shape[1], track_running_stats=True,\n",
        "                                                     meta_batch_norm=self.meta_layer,\n",
        "                                                     no_learnable_params=self.no_bn_learnable_params,\n",
        "                                                     device=self.device,\n",
        "                                                     use_per_step_bn_statistics=self.use_per_step_bn_statistics,\n",
        "                                                     args=self.args)\n",
        "            elif self.args.norm_layer == \"layer_norm\":\n",
        "                self.norm_layer = MetaLayerNormLayer(input_feature_shape=out.shape[1:])\n",
        "\n",
        "            out = self.norm_layer.forward(out, num_step=0)\n",
        "        self.conv = MetaConv2dLayer(in_channels=out.shape[1], out_channels=self.num_filters,\n",
        "                                    kernel_size=self.kernel_size,\n",
        "                                    stride=self.stride, padding=self.padding, use_bias=self.use_bias)\n",
        "\n",
        "\n",
        "        self.layer_dict['activation_function_pre'] = nn.LeakyReLU()\n",
        "\n",
        "\n",
        "        out = self.layer_dict['activation_function_pre'].forward(self.conv.forward(out))\n",
        "        print(out.shape)\n",
        "\n",
        "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "            Forward propagates by applying the function. If params are none then internal params are used.\n",
        "            Otherwise passed params will be used to execute the function.\n",
        "            :param input: input data batch, size either can be any.\n",
        "            :param num_step: The current inner loop step being taken. This is used when we are learning per step params and\n",
        "             collecting per step batch statistics. It indexes the correct object to use for the current time-step\n",
        "            :param params: A dictionary containing 'weight' and 'bias'.\n",
        "            :param training: Whether this is currently the training or evaluation phase.\n",
        "            :param backup_running_statistics: Whether to backup the running statistics. This is used\n",
        "            at evaluation time, when after the pass is complete we want to throw away the collected validation stats.\n",
        "            :return: The result of the batch norm operation.\n",
        "        \"\"\"\n",
        "        batch_norm_params = None\n",
        "\n",
        "        if params is not None:\n",
        "            params = extract_top_level_dict(current_dict=params)\n",
        "\n",
        "            if self.normalization and 'norm_layer' in params:\n",
        "                batch_norm_params = params['norm_layer']\n",
        "\n",
        "            conv_params = params['conv']\n",
        "        else:\n",
        "            conv_params = None\n",
        "            #print('no inner loop params', self)\n",
        "\n",
        "        out = x\n",
        "\n",
        "        if self.normalization:\n",
        "            out = self.norm_layer.forward(out, num_step=num_step,\n",
        "                                          params=batch_norm_params, training=training,\n",
        "                                          backup_running_statistics=backup_running_statistics)\n",
        "\n",
        "        out = self.conv.forward(out, params=conv_params)\n",
        "        out = self.layer_dict['activation_function_pre'].forward(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        \"\"\"\n",
        "        Restore stored statistics from the backup, replacing the current ones.\n",
        "        \"\"\"\n",
        "        if self.normalization:\n",
        "            self.norm_layer.restore_backup_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LiAJAkvInHY-"
      },
      "outputs": [],
      "source": [
        "class VGGReLUNormNetwork(nn.Module):\n",
        "    def __init__(self, im_shape, num_output_classes, args, device, meta_classifier=True):\n",
        "        \"\"\"\n",
        "        Builds a multilayer convolutional network. It also provides functionality for passing external parameters to be\n",
        "        used at inference time. Enables inner loop optimization readily.\n",
        "        :param im_shape: The input image batch shape.\n",
        "        :param num_output_classes: The number of output classes of the network.\n",
        "        :param args: A named tuple containing the system's hyperparameters.\n",
        "        :param device: The device to run this on.\n",
        "        :param meta_classifier: A flag indicating whether the system's meta-learning (inner-loop) functionalities should\n",
        "        be enabled.\n",
        "        \"\"\"\n",
        "        super(VGGReLUNormNetwork, self).__init__()\n",
        "        b, c, self.h, self.w = im_shape\n",
        "        self.device = device\n",
        "        self.total_layers = 0\n",
        "        self.args = args\n",
        "        self.upscale_shapes = []\n",
        "        self.cnn_filters = args.cnn_num_filters\n",
        "        self.input_shape = list(im_shape)\n",
        "        self.num_stages = args.num_stages\n",
        "        self.num_output_classes = num_output_classes\n",
        "\n",
        "        if args.max_pooling:\n",
        "            print(\"Using max pooling\")\n",
        "            self.conv_stride = 1\n",
        "        else:\n",
        "            print(\"Using strided convolutions\")\n",
        "            self.conv_stride = 2\n",
        "        self.meta_classifier = meta_classifier\n",
        "\n",
        "        self.build_network()\n",
        "        print(\"meta network params\")\n",
        "        for name, param in self.named_parameters():\n",
        "            print(name, param.shape)\n",
        "\n",
        "    def build_network(self):\n",
        "        \"\"\"\n",
        "        Builds the network before inference is required by creating some dummy inputs with the same input as the\n",
        "        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n",
        "        sets output shapes for each layer.\n",
        "        \"\"\"\n",
        "        x = torch.zeros(self.input_shape)\n",
        "        out = x\n",
        "        self.layer_dict = nn.ModuleDict()\n",
        "        self.upscale_shapes.append(x.shape)\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            self.layer_dict['conv{}'.format(i)] = MetaConvNormLayerReLU(input_shape=out.shape,\n",
        "                                                                        num_filters=self.cnn_filters,\n",
        "                                                                        kernel_size=3, stride=self.conv_stride,\n",
        "                                                                        padding=self.args.conv_padding,\n",
        "                                                                        use_bias=True, args=self.args,\n",
        "                                                                        normalization=True,\n",
        "                                                                        meta_layer=self.meta_classifier,\n",
        "                                                                        no_bn_learnable_params=False,\n",
        "                                                                        device=self.device)\n",
        "            out = self.layer_dict['conv{}'.format(i)](out, training=True, num_step=0)\n",
        "\n",
        "            if self.args.max_pooling:\n",
        "                out = F.max_pool2d(input=out, kernel_size=(2, 2), stride=2, padding=0)\n",
        "\n",
        "\n",
        "        if not self.args.max_pooling:\n",
        "            out = F.avg_pool2d(out, out.shape[2])\n",
        "\n",
        "        self.encoder_features_shape = list(out.shape)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "\n",
        "        self.layer_dict['linear'] = MetaLinearLayer(input_shape=(out.shape[0], np.prod(out.shape[1:])),\n",
        "                                                    num_filters=self.num_output_classes, use_bias=True)\n",
        "\n",
        "        out = self.layer_dict['linear'](out)\n",
        "        print(\"VGGNetwork build\", out.shape)\n",
        "\n",
        "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
        "        \"\"\"\n",
        "        Forward propages through the network. If any params are passed then they are used instead of stored params.\n",
        "        :param x: Input image batch.\n",
        "        :param num_step: The current inner loop step number\n",
        "        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n",
        "         same as the layer names then they will be used instead.\n",
        "        :param training: Whether this is training (True) or eval time.\n",
        "        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n",
        "        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n",
        "        :return: Logits of shape b, num_output_classes.\n",
        "        \"\"\"\n",
        "        param_dict = {}\n",
        "\n",
        "        if params is not None:\n",
        "            params = {key: value[0] for key, value in params.items()}\n",
        "            param_dict = extract_top_level_dict(current_dict=params)\n",
        "\n",
        "        # print('top network', param_dict.keys())\n",
        "        for name, param in self.layer_dict.named_parameters():\n",
        "            path_bits = name.split(\".\")\n",
        "            layer_name = path_bits[0]\n",
        "            if layer_name not in param_dict:\n",
        "                param_dict[layer_name] = None\n",
        "\n",
        "        out = x\n",
        "\n",
        "        for i in range(self.num_stages):\n",
        "            out = self.layer_dict['conv{}'.format(i)](out, params=param_dict['conv{}'.format(i)], training=training,\n",
        "                                                      backup_running_statistics=backup_running_statistics,\n",
        "                                                      num_step=num_step)\n",
        "            if self.args.max_pooling:\n",
        "                out = F.max_pool2d(input=out, kernel_size=(2, 2), stride=2, padding=0)\n",
        "\n",
        "        if not self.args.max_pooling:\n",
        "            out = F.avg_pool2d(out, out.shape[2])\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.layer_dict['linear'](out, param_dict['linear'])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def zero_grad(self, params=None):\n",
        "        if params is None:\n",
        "            for param in self.parameters():\n",
        "                if (\n",
        "                    param.requires_grad == True\n",
        "                    and param.grad is not None\n",
        "                    and torch.sum(param.grad) > 0\n",
        "                ):\n",
        "                    print(param.grad)\n",
        "                    param.grad.zero_()\n",
        "        else:\n",
        "            for name, param in params.items():\n",
        "                if (\n",
        "                    param.requires_grad == True\n",
        "                    and param.grad is not None\n",
        "                    and torch.sum(param.grad) > 0\n",
        "                ):\n",
        "                    print(param.grad)\n",
        "                    param.grad.zero_()\n",
        "                    params[name].grad = None\n",
        "\n",
        "    def restore_backup_stats(self):\n",
        "        \"\"\"\n",
        "        Reset stored batch statistics from the stored backup.\n",
        "        \"\"\"\n",
        "        for i in range(self.num_stages):\n",
        "            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-uTHDDT0YGl"
      },
      "source": [
        "TODO:\n",
        "\n",
        "add more functionalities specific to MAML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoajsklRk8Os"
      },
      "source": [
        "# Experiment Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FmscGJS8cCUv"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "17o11EhPJ4NL"
      },
      "outputs": [],
      "source": [
        "def build_experiment_folder(experiment_name):\n",
        "    experiment_path = os.path.abspath(experiment_name)\n",
        "    saved_models_filepath = \"{}/{}\".format(experiment_path, \"saved_models\")\n",
        "    logs_filepath = \"{}/{}\".format(experiment_path, \"logs\")\n",
        "    samples_filepath = \"{}/{}\".format(experiment_path, \"visual_outputs\")\n",
        "\n",
        "    if not os.path.exists(experiment_path):\n",
        "        os.makedirs(experiment_path)\n",
        "    if not os.path.exists(logs_filepath):\n",
        "        os.makedirs(logs_filepath)\n",
        "    if not os.path.exists(samples_filepath):\n",
        "        os.makedirs(samples_filepath)\n",
        "    if not os.path.exists(saved_models_filepath):\n",
        "        os.makedirs(saved_models_filepath)\n",
        "\n",
        "    outputs = (saved_models_filepath, logs_filepath, samples_filepath)\n",
        "    outputs = (os.path.abspath(item) for item in outputs)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3BMXNVZ6b9qk"
      },
      "outputs": [],
      "source": [
        "def save_statistics(experiment_name, line_to_add, filename=\"summary_statistics.csv\", create=False):\n",
        "    summary_filename = \"{}/{}\".format(experiment_name, filename)\n",
        "    if create:\n",
        "        with open(summary_filename, 'w') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(line_to_add)\n",
        "    else:\n",
        "        with open(summary_filename, 'a') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(line_to_add)\n",
        "\n",
        "    return summary_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qIFIaiMXcIOc"
      },
      "outputs": [],
      "source": [
        "def save_to_json(filename, dict_to_store):\n",
        "    with open(os.path.abspath(filename), 'w') as f:\n",
        "        json.dump(dict_to_store, fp=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nV0wkiZBk59X"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "# from utils.storage import build_experiment_folder, save_statistics, save_to_json\n",
        "import time\n",
        "import torch\n",
        "\n",
        "\n",
        "class ExperimentBuilder(object):\n",
        "    def __init__(self, args, data, model, device):\n",
        "        \"\"\"\n",
        "        Initializes an experiment builder using a named tuple (args), a data provider (data), a meta learning system\n",
        "        (model) and a device (e.g. gpu/cpu/n)\n",
        "        :param args: A namedtuple containing all experiment hyperparameters\n",
        "        :param data: A data provider of instance MetaLearningSystemDataLoader\n",
        "        :param model: A meta learning system instance\n",
        "        :param device: Device/s to use for the experiment\n",
        "        \"\"\"\n",
        "        self.args, self.device = args, device\n",
        "\n",
        "        self.model = model\n",
        "        self.saved_models_filepath, self.logs_filepath, self.samples_filepath = build_experiment_folder(\n",
        "            experiment_name=self.args.experiment_name)\n",
        "\n",
        "        self.total_losses = {}\n",
        "        self.state = {'best_val_acc': 0.0, 'best_val_iter': 0, 'current_iter': 0}\n",
        "        self.start_epoch = 0\n",
        "        self.max_models_to_save = self.args.max_models_to_save\n",
        "        self.create_summary_csv = False\n",
        "\n",
        "        if self.args.continue_from_epoch == 'from_scratch':\n",
        "            self.create_summary_csv = True\n",
        "\n",
        "        elif self.args.continue_from_epoch == 'latest':\n",
        "            checkpoint = os.path.join(self.saved_models_filepath, \"train_model_latest\")\n",
        "            print(\"attempting to find existing checkpoint\", )\n",
        "            if os.path.exists(checkpoint):\n",
        "                self.state = \\\n",
        "                    self.model.load_model(model_save_dir=self.saved_models_filepath, model_name=\"train_model\",\n",
        "                                          model_idx='latest')\n",
        "                self.start_epoch = int(self.state['current_iter'] / self.args.total_iter_per_epoch)\n",
        "\n",
        "            else:\n",
        "                self.args.continue_from_epoch = 'from_scratch'\n",
        "                self.create_summary_csv = True\n",
        "        elif int(self.args.continue_from_epoch) >= 0:\n",
        "            self.state = \\\n",
        "                self.model.load_model(model_save_dir=self.saved_models_filepath, model_name=\"train_model\",\n",
        "                                      model_idx=self.args.continue_from_epoch)\n",
        "            self.start_epoch = int(self.state['current_iter'] / self.args.total_iter_per_epoch)\n",
        "\n",
        "        self.data = data(args=args, current_iter=self.state['current_iter'])\n",
        "\n",
        "        print(\"train_seed {}, val_seed: {}, at start time\".format(self.data.dataset.seed[\"train\"],\n",
        "                                                                  self.data.dataset.seed[\"val\"]))\n",
        "        self.total_epochs_before_pause = self.args.total_epochs_before_pause\n",
        "        self.state['best_epoch'] = int(self.state['best_val_iter'] / self.args.total_iter_per_epoch)\n",
        "        self.epoch = int(self.state['current_iter'] / self.args.total_iter_per_epoch)\n",
        "        self.augment_flag = 'omniglot' in self.args.dataset_name.lower()\n",
        "        self.start_time = time.time()\n",
        "        self.epochs_done_in_this_run = 0\n",
        "        print(self.state['current_iter'], int(self.args.total_iter_per_epoch * self.args.total_epochs))\n",
        "\n",
        "    def build_summary_dict(self, total_losses, phase, summary_losses=None):\n",
        "        \"\"\"\n",
        "        Builds/Updates a summary dict directly from the metric dict of the current iteration.\n",
        "        :param total_losses: Current dict with total losses (not aggregations) from experiment\n",
        "        :param phase: Current training phase\n",
        "        :param summary_losses: Current summarised (aggregated/summarised) losses stats means, stdv etc.\n",
        "        :return: A new summary dict with the updated summary statistics information.\n",
        "        \"\"\"\n",
        "        if summary_losses is None:\n",
        "            summary_losses = {}\n",
        "\n",
        "        for key in total_losses:\n",
        "            summary_losses[\"{}_{}_mean\".format(phase, key)] = np.mean(total_losses[key])\n",
        "            summary_losses[\"{}_{}_std\".format(phase, key)] = np.std(total_losses[key])\n",
        "\n",
        "        return summary_losses\n",
        "\n",
        "    def build_loss_summary_string(self, summary_losses):\n",
        "        \"\"\"\n",
        "        Builds a progress bar summary string given current summary losses dictionary\n",
        "        :param summary_losses: Current summary statistics\n",
        "        :return: A summary string ready to be shown to humans.\n",
        "        \"\"\"\n",
        "        output_update = \"\"\n",
        "        for key, value in zip(list(summary_losses.keys()), list(summary_losses.values())):\n",
        "            if \"loss\" in key or \"accuracy\" in key:\n",
        "                value = float(value)\n",
        "                output_update += \"{}: {:.4f}, \".format(key, value)\n",
        "\n",
        "        return output_update\n",
        "\n",
        "    def merge_two_dicts(self, first_dict, second_dict):\n",
        "        \"\"\"Given two dicts, merge them into a new dict as a shallow copy.\"\"\"\n",
        "        z = first_dict.copy()\n",
        "        z.update(second_dict)\n",
        "        return z\n",
        "\n",
        "    def train_iteration(self, train_sample, sample_idx, epoch_idx, total_losses, current_iter, pbar_train):\n",
        "        \"\"\"\n",
        "        Runs a training iteration, updates the progress bar and returns the total and current epoch train losses.\n",
        "        :param train_sample: A sample from the data provider\n",
        "        :param sample_idx: The index of the incoming sample, in relation to the current training run.\n",
        "        :param epoch_idx: The epoch index.\n",
        "        :param total_losses: The current total losses dictionary to be updated.\n",
        "        :param current_iter: The current training iteration in relation to the whole experiment.\n",
        "        :param pbar_train: The progress bar of the training.\n",
        "        :return: Updates total_losses, train_losses, current_iter\n",
        "        \"\"\"\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
        "        data_batch = (x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        if sample_idx == 0:\n",
        "            print(\"shape of data\", x_support_set.shape, x_target_set.shape, y_support_set.shape,\n",
        "                  y_target_set.shape)\n",
        "\n",
        "        losses, _ = self.model.run_train_iter(data_batch=data_batch, epoch=epoch_idx)\n",
        "\n",
        "        for key, value in zip(list(losses.keys()), list(losses.values())):\n",
        "            if key not in total_losses:\n",
        "                total_losses[key] = [float(value)]\n",
        "            else:\n",
        "                total_losses[key].append(float(value))\n",
        "\n",
        "        train_losses = self.build_summary_dict(total_losses=total_losses, phase=\"train\")\n",
        "        train_output_update = self.build_loss_summary_string(losses)\n",
        "\n",
        "        if (current_iter % 10 == 0):\n",
        "            pbar_train.update(1)\n",
        "        # pbar_train.set_description(\"training phase {} -> {}\".format(self.epoch, train_output_update))\n",
        "\n",
        "        current_iter += 1\n",
        "\n",
        "        return train_losses, total_losses, current_iter\n",
        "\n",
        "    def evaluation_iteration(self, val_sample, total_losses, pbar_val, phase):\n",
        "        \"\"\"\n",
        "        Runs a validation iteration, updates the progress bar and returns the total and current epoch val losses.\n",
        "        :param val_sample: A sample from the data provider\n",
        "        :param total_losses: The current total losses dictionary to be updated.\n",
        "        :param pbar_val: The progress bar of the val stage.\n",
        "        :return: The updated val_losses, total_losses\n",
        "        \"\"\"\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set, seed = val_sample\n",
        "        data_batch = (\n",
        "            x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        losses, _ = self.model.run_validation_iter(data_batch=data_batch)\n",
        "        for key, value in zip(list(losses.keys()), list(losses.values())):\n",
        "            if key not in total_losses:\n",
        "                total_losses[key] = [float(value)]\n",
        "            else:\n",
        "                total_losses[key].append(float(value))\n",
        "\n",
        "        val_losses = self.build_summary_dict(total_losses=total_losses, phase=phase)\n",
        "        val_output_update = self.build_loss_summary_string(losses)\n",
        "\n",
        "        pbar_val.update(1)\n",
        "        # pbar_val.set_description(\n",
        "            # \"val_phase {} -> {}\".format(self.epoch, val_output_update))\n",
        "\n",
        "        return val_losses, total_losses\n",
        "\n",
        "    def test_evaluation_iteration(self, val_sample, model_idx, sample_idx, per_model_per_batch_preds, pbar_test):\n",
        "        \"\"\"\n",
        "        Runs a validation iteration, updates the progress bar and returns the total and current epoch val losses.\n",
        "        :param val_sample: A sample from the data provider\n",
        "        :param total_losses: The current total losses dictionary to be updated.\n",
        "        :param pbar_test: The progress bar of the val stage.\n",
        "        :return: The updated val_losses, total_losses\n",
        "        \"\"\"\n",
        "        x_support_set, x_target_set, y_support_set, y_target_set, seed = val_sample\n",
        "        data_batch = (\n",
        "            x_support_set, x_target_set, y_support_set, y_target_set)\n",
        "\n",
        "        losses, per_task_preds = self.model.run_validation_iter(data_batch=data_batch)\n",
        "\n",
        "        per_model_per_batch_preds[model_idx].extend(list(per_task_preds))\n",
        "\n",
        "        test_output_update = self.build_loss_summary_string(losses)\n",
        "\n",
        "        pbar_test.update(1)\n",
        "        pbar_test.set_description(\n",
        "            \"test_phase {} -> {}\".format(self.epoch, test_output_update))\n",
        "\n",
        "        return per_model_per_batch_preds\n",
        "\n",
        "    def save_models(self, model, epoch, state):\n",
        "        \"\"\"\n",
        "        Saves two separate instances of the current model. One to be kept for history and reloading later and another\n",
        "        one marked as \"latest\" to be used by the system for the next epoch training. Useful when the training/val\n",
        "        process is interrupted or stopped. Leads to fault tolerant training and validation systems that can continue\n",
        "        from where they left off before.\n",
        "        :param model: Current meta learning model of any instance within the few_shot_learning_system.py\n",
        "        :param epoch: Current epoch\n",
        "        :param state: Current model and experiment state dict.\n",
        "        \"\"\"\n",
        "        model.save_model(model_save_dir=os.path.join(self.saved_models_filepath, \"train_model_{}\".format(int(epoch))),\n",
        "                         state=state)\n",
        "\n",
        "        model.save_model(model_save_dir=os.path.join(self.saved_models_filepath, \"train_model_latest\"),\n",
        "                         state=state)\n",
        "\n",
        "        print(\"saved models to\", self.saved_models_filepath)\n",
        "\n",
        "    def pack_and_save_metrics(self, start_time, create_summary_csv, train_losses, val_losses, state):\n",
        "        \"\"\"\n",
        "        Given current epochs start_time, train losses, val losses and whether to create a new stats csv file, pack stats\n",
        "        and save into a statistics csv file. Return a new start time for the new epoch.\n",
        "        :param start_time: The start time of the current epoch\n",
        "        :param create_summary_csv: A boolean variable indicating whether to create a new statistics file or\n",
        "        append results to existing one\n",
        "        :param train_losses: A dictionary with the current train losses\n",
        "        :param val_losses: A dictionary with the currrent val loss\n",
        "        :return: The current time, to be used for the next epoch.\n",
        "        \"\"\"\n",
        "        epoch_summary_losses = self.merge_two_dicts(first_dict=train_losses, second_dict=val_losses)\n",
        "\n",
        "        if 'per_epoch_statistics' not in state:\n",
        "            state['per_epoch_statistics'] = {}\n",
        "\n",
        "        for key, value in epoch_summary_losses.items():\n",
        "\n",
        "            if key not in state['per_epoch_statistics']:\n",
        "                state['per_epoch_statistics'][key] = [value]\n",
        "            else:\n",
        "                state['per_epoch_statistics'][key].append(value)\n",
        "\n",
        "        epoch_summary_string = self.build_loss_summary_string(epoch_summary_losses)\n",
        "        epoch_summary_losses[\"epoch\"] = self.epoch\n",
        "        epoch_summary_losses['epoch_run_time'] = time.time() - start_time\n",
        "\n",
        "        if create_summary_csv:\n",
        "            self.summary_statistics_filepath = save_statistics(self.logs_filepath, list(epoch_summary_losses.keys()),\n",
        "                                                               create=True)\n",
        "            self.create_summary_csv = False\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"epoch {} -> {}\".format(epoch_summary_losses[\"epoch\"], epoch_summary_string))\n",
        "\n",
        "        self.summary_statistics_filepath = save_statistics(self.logs_filepath,\n",
        "                                                           list(epoch_summary_losses.values()))\n",
        "\n",
        "        with open('summary_stats.txt', 'a+') as f:\n",
        "            f.write(epoch_summary_string)\n",
        "            f.close()\n",
        "\n",
        "        return start_time, state\n",
        "\n",
        "    def evaluated_test_set_using_the_best_models(self, top_n_models):\n",
        "        per_epoch_statistics = self.state['per_epoch_statistics']\n",
        "        val_acc = np.copy(per_epoch_statistics['val_accuracy_mean'])\n",
        "        val_idx = np.array([i for i in range(len(val_acc))])\n",
        "        sorted_idx = np.argsort(val_acc, axis=0).astype(dtype=np.int32)[::-1][:top_n_models]\n",
        "\n",
        "        sorted_val_acc = val_acc[sorted_idx]\n",
        "        val_idx = val_idx[sorted_idx]\n",
        "        print(sorted_idx)\n",
        "        print(sorted_val_acc)\n",
        "\n",
        "        top_n_idx = val_idx[:top_n_models]\n",
        "        per_model_per_batch_preds = [[] for i in range(top_n_models)]\n",
        "        per_model_per_batch_targets = [[] for i in range(top_n_models)]\n",
        "        test_losses = [dict() for i in range(top_n_models)]\n",
        "        for idx, model_idx in enumerate(top_n_idx):\n",
        "            self.state = \\\n",
        "                self.model.load_model(model_save_dir=self.saved_models_filepath, model_name=\"train_model\",\n",
        "                                      model_idx=model_idx + 1)\n",
        "            with tqdm.tqdm(total=int(self.args.num_evaluation_tasks / self.args.batch_size)) as pbar_test:\n",
        "                for sample_idx, test_sample in enumerate(\n",
        "                        self.data.get_test_batches(total_batches=int(self.args.num_evaluation_tasks / self.args.batch_size),\n",
        "                                                   augment_images=False)):\n",
        "                    #print(test_sample[4])\n",
        "                    per_model_per_batch_targets[idx].extend(np.array(test_sample[3]))\n",
        "                    per_model_per_batch_preds = self.test_evaluation_iteration(val_sample=test_sample,\n",
        "                                                                               sample_idx=sample_idx,\n",
        "                                                                               model_idx=idx,\n",
        "                                                                               per_model_per_batch_preds=per_model_per_batch_preds,\n",
        "                                                                               pbar_test=pbar_test)\n",
        "        # for i in range(top_n_models):\n",
        "        #     print(\"test assertion\", 0)\n",
        "        #     print(per_model_per_batch_targets[0], per_model_per_batch_targets[i])\n",
        "        #     assert np.equal(np.array(per_model_per_batch_targets[0]), np.array(per_model_per_batch_targets[i]))\n",
        "\n",
        "        per_batch_preds = np.mean(per_model_per_batch_preds, axis=0)\n",
        "        #print(per_batch_preds.shape)\n",
        "        per_batch_max = np.argmax(per_batch_preds, axis=2)\n",
        "        per_batch_targets = np.array(per_model_per_batch_targets[0]).reshape(per_batch_max.shape)\n",
        "        #print(per_batch_max)\n",
        "        accuracy = np.mean(np.equal(per_batch_targets, per_batch_max))\n",
        "        accuracy_std = np.std(np.equal(per_batch_targets, per_batch_max))\n",
        "\n",
        "        test_losses = {\"test_accuracy_mean\": accuracy, \"test_accuracy_std\": accuracy_std}\n",
        "\n",
        "        _ = save_statistics(self.logs_filepath,\n",
        "                            list(test_losses.keys()),\n",
        "                            create=True, filename=\"test_summary.csv\")\n",
        "\n",
        "        summary_statistics_filepath = save_statistics(self.logs_filepath,\n",
        "                                                      list(test_losses.values()),\n",
        "                                                      create=False, filename=\"test_summary.csv\")\n",
        "        print(test_losses)\n",
        "        print(\"saved test performance at\", summary_statistics_filepath)\n",
        "\n",
        "    def run_experiment(self):\n",
        "        \"\"\"\n",
        "        Runs a full training experiment with evaluations of the model on the val set at every epoch. Furthermore,\n",
        "        will return the test set evaluation results on the best performing validation model.\n",
        "        \"\"\"\n",
        "        with tqdm.tqdm(initial=self.state['current_iter'],\n",
        "                           total=int(self.args.total_iter_per_epoch * self.args.total_epochs)) as pbar_train:\n",
        "\n",
        "            while (self.state['current_iter'] < (self.args.total_epochs * self.args.total_iter_per_epoch)) and (self.args.evaluate_on_test_set_only == False):\n",
        "\n",
        "                for train_sample_idx, train_sample in enumerate(\n",
        "                        self.data.get_train_batches(total_batches=int(self.args.total_iter_per_epoch *\n",
        "                                                                      self.args.total_epochs) - self.state[\n",
        "                                                                      'current_iter'],\n",
        "                                                    augment_images=self.augment_flag)):\n",
        "                    # print(self.state['current_iter'], (self.args.total_epochs * self.args.total_iter_per_epoch))\n",
        "                    train_losses, total_losses, self.state['current_iter'] = self.train_iteration(\n",
        "                        train_sample=train_sample,\n",
        "                        total_losses=self.total_losses,\n",
        "                        epoch_idx=(self.state['current_iter'] /\n",
        "                                   self.args.total_iter_per_epoch),\n",
        "                        pbar_train=pbar_train,\n",
        "                        current_iter=self.state['current_iter'],\n",
        "                        sample_idx=self.state['current_iter'])\n",
        "\n",
        "                    if self.state['current_iter'] % self.args.total_iter_per_epoch == 0:\n",
        "\n",
        "                        total_losses = {}\n",
        "                        val_losses = {}\n",
        "                        with tqdm.tqdm(total=int(self.args.num_evaluation_tasks / self.args.batch_size)) as pbar_val:\n",
        "                            for _, val_sample in enumerate(\n",
        "                                    self.data.get_val_batches(total_batches=int(self.args.num_evaluation_tasks / self.args.batch_size),\n",
        "                                                              augment_images=False)):\n",
        "                                val_losses, total_losses = self.evaluation_iteration(val_sample=val_sample,\n",
        "                                                                                     total_losses=total_losses,\n",
        "                                                                                     pbar_val=pbar_val, phase='val')\n",
        "\n",
        "                            if val_losses[\"val_accuracy_mean\"] > self.state['best_val_acc']:\n",
        "                                print(\"Best validation accuracy\", val_losses[\"val_accuracy_mean\"])\n",
        "                                self.state['best_val_acc'] = val_losses[\"val_accuracy_mean\"]\n",
        "                                self.state['best_val_iter'] = self.state['current_iter']\n",
        "                                self.state['best_epoch'] = int(\n",
        "                                    self.state['best_val_iter'] / self.args.total_iter_per_epoch)\n",
        "\n",
        "\n",
        "                        self.epoch += 1\n",
        "                        self.state = self.merge_two_dicts(first_dict=self.merge_two_dicts(first_dict=self.state,\n",
        "                                                                                          second_dict=train_losses),\n",
        "                                                          second_dict=val_losses)\n",
        "\n",
        "                        self.save_models(model=self.model, epoch=self.epoch, state=self.state)\n",
        "\n",
        "                        self.start_time, self.state = self.pack_and_save_metrics(start_time=self.start_time,\n",
        "                                                                                 create_summary_csv=self.create_summary_csv,\n",
        "                                                                                 train_losses=train_losses,\n",
        "                                                                                 val_losses=val_losses,\n",
        "                                                                                 state=self.state)\n",
        "\n",
        "                        self.total_losses = {}\n",
        "\n",
        "                        self.epochs_done_in_this_run += 1\n",
        "\n",
        "                        save_to_json(filename=os.path.join(self.logs_filepath, \"summary_statistics.json\"),\n",
        "                                     dict_to_store=self.state['per_epoch_statistics'])\n",
        "\n",
        "                        if self.epochs_done_in_this_run >= self.total_epochs_before_pause:\n",
        "                            print(\"train_seed {}, val_seed: {}, at pause time\".format(self.data.dataset.seed[\"train\"],\n",
        "                                                                                      self.data.dataset.seed[\"val\"]))\n",
        "                            sys.exit()\n",
        "            self.evaluated_test_set_using_the_best_models(top_n_models=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcgAQA_LvTZc"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6C_DqsYZvVpv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm\n",
        "import concurrent.futures\n",
        "import pickle\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# from utils.parser_utils import get_args\n",
        "\n",
        "\n",
        "class rotate_image(object):\n",
        "\n",
        "    def __init__(self, k, channels):\n",
        "        self.k = k\n",
        "        self.channels = channels\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if self.channels == 1:\n",
        "            if len(image.shape) == 3:\n",
        "                image = image[:, :, 0]\n",
        "                image = np.expand_dims(image, axis=2)\n",
        "\n",
        "            elif len(image.shape) == 4:\n",
        "                image = image[:, :, :, 0]\n",
        "                image = np.expand_dims(image, axis=3)\n",
        "\n",
        "        image = np.rot90(image, k=self.k).copy()\n",
        "        return image\n",
        "\n",
        "\n",
        "class torch_rotate_image(object):\n",
        "\n",
        "    def __init__(self, k, channels):\n",
        "        self.k = k\n",
        "        self.channels = channels\n",
        "\n",
        "    def __call__(self, image):\n",
        "        rotate = transforms.RandomRotation(degrees=self.k * 90)\n",
        "        if image.shape[-1] == 1:\n",
        "            image = image[:, :, 0]\n",
        "        image = Image.fromarray(image)\n",
        "        image = rotate(image)\n",
        "        image = np.array(image)\n",
        "        if len(image.shape) == 2:\n",
        "            image = np.expand_dims(image, axis=2)\n",
        "        return image\n",
        "\n",
        "\n",
        "def augment_image(image, k, channels, augment_bool, args, dataset_name):\n",
        "    transform_train, transform_evaluation = get_transforms_for_dataset(dataset_name=dataset_name,\n",
        "                                                                       args=args, k=k)\n",
        "    if len(image.shape) > 3:\n",
        "        images = [item for item in image]\n",
        "        output_images = []\n",
        "        for image in images:\n",
        "            if augment_bool is True:\n",
        "                for transform_current in transform_train:\n",
        "                    image = transform_current(image)\n",
        "            else:\n",
        "                for transform_current in transform_evaluation:\n",
        "                    image = transform_current(image)\n",
        "            output_images.append(image)\n",
        "        image = torch.stack(output_images)\n",
        "    elif augment_bool is True:\n",
        "        # meanstd transformation\n",
        "        for transform_current in transform_train:\n",
        "            image = transform_current(image)\n",
        "    else:\n",
        "        for transform_current in transform_evaluation:\n",
        "            image = transform_current(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def get_transforms_for_dataset(dataset_name, args, k):\n",
        "    if \"cifar10\" in dataset_name or \"cifar100\" in dataset_name:\n",
        "        transform_train = [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(args.classification_mean, args.classification_std)]\n",
        "\n",
        "        transform_evaluate = [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(args.classification_mean, args.classification_std)]\n",
        "\n",
        "    elif 'omniglot' in dataset_name:\n",
        "\n",
        "        transform_train = [rotate_image(k=k, channels=args.image_channels), transforms.ToTensor()]\n",
        "        transform_evaluate = [transforms.ToTensor()]\n",
        "\n",
        "\n",
        "    elif 'imagenet' in dataset_name:\n",
        "\n",
        "        transform_train = [transforms.Compose([\n",
        "\n",
        "            transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])]\n",
        "\n",
        "        transform_evaluate = [transforms.Compose([\n",
        "\n",
        "            transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])]\n",
        "\n",
        "    return transform_train, transform_evaluate\n",
        "\n",
        "\n",
        "class FewShotLearningDatasetParallel(Dataset):\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        A data provider class inheriting from Pytorch's Dataset class. It takes care of creating task sets for\n",
        "        our few-shot learning model training and evaluation\n",
        "        :param args: Arguments in the form of a Bunch object. Includes all hyperparameters necessary for the\n",
        "        data-provider. For transparency and readability reasons to explicitly set as self.object_name all arguments\n",
        "        required for the data provider, such that the reader knows exactly what is necessary for the data provider/\n",
        "        \"\"\"\n",
        "        self.data_path = args.dataset_path\n",
        "        self.dataset_name = args.dataset_name\n",
        "        self.data_loaded_in_memory = False\n",
        "        self.image_height, self.image_width, self.image_channel = args.image_height, args.image_width, args.image_channels\n",
        "        self.args = args\n",
        "        self.indexes_of_folders_indicating_class = args.indexes_of_folders_indicating_class\n",
        "        self.reverse_channels = args.reverse_channels\n",
        "        self.labels_as_int = args.labels_as_int\n",
        "        self.train_val_test_split = args.train_val_test_split\n",
        "        self.current_set_name = \"train\"\n",
        "        self.num_target_samples = args.num_target_samples\n",
        "        self.reset_stored_filepaths = args.reset_stored_filepaths\n",
        "        val_rng = np.random.RandomState(seed=args.val_seed)\n",
        "        val_seed = val_rng.randint(1, 999999)\n",
        "        train_rng = np.random.RandomState(seed=args.train_seed)\n",
        "        train_seed = train_rng.randint(1, 999999)\n",
        "        test_rng = np.random.RandomState(seed=args.val_seed)\n",
        "        test_seed = test_rng.randint(1, 999999)\n",
        "        args.val_seed = val_seed\n",
        "        args.train_seed = train_seed\n",
        "        args.test_seed = test_seed\n",
        "        self.init_seed = {\"train\": args.train_seed, \"val\": args.val_seed, 'test': args.val_seed}\n",
        "        self.seed = {\"train\": args.train_seed, \"val\": args.val_seed, 'test': args.val_seed}\n",
        "        self.num_of_gpus = args.num_of_gpus\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.train_index = 0\n",
        "        self.val_index = 0\n",
        "        self.test_index = 0\n",
        "\n",
        "        self.augment_images = False\n",
        "        self.num_samples_per_class = args.num_samples_per_class\n",
        "        self.num_classes_per_set = args.num_classes_per_set\n",
        "\n",
        "        self.rng = np.random.RandomState(seed=self.seed['val'])\n",
        "        self.datasets = self.load_dataset()\n",
        "\n",
        "        self.indexes = {\"train\": 0, \"val\": 0, 'test': 0}\n",
        "        self.dataset_size_dict = {\n",
        "            \"train\": {key: len(self.datasets['train'][key]) for key in list(self.datasets['train'].keys())},\n",
        "            \"val\": {key: len(self.datasets['val'][key]) for key in list(self.datasets['val'].keys())},\n",
        "            'test': {key: len(self.datasets['test'][key]) for key in list(self.datasets['test'].keys())}}\n",
        "        self.label_set = self.get_label_set()\n",
        "        self.data_length = {name: np.sum([len(self.datasets[name][key])\n",
        "                                          for key in self.datasets[name]]) for name in self.datasets.keys()}\n",
        "\n",
        "        print(\"data\", self.data_length)\n",
        "        self.observed_seed_set = None\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"\n",
        "        Loads a dataset's dictionary files and splits the data according to the train_val_test_split variable stored\n",
        "        in the args object.\n",
        "        :return: Three sets, the training set, validation set and test sets (referred to as the meta-train,\n",
        "        meta-val and meta-test in the paper)\n",
        "        \"\"\"\n",
        "        rng = np.random.RandomState(seed=self.seed['val'])\n",
        "\n",
        "        if self.args.sets_are_pre_split == True:\n",
        "            print(\"Loading pre-split data\")\n",
        "            data_image_paths, index_to_label_name_dict_file, label_to_index = self.load_datapaths()\n",
        "            dataset_splits = {}\n",
        "            for key, value in data_image_paths.items():\n",
        "                key = self.get_label_from_index(index=key)\n",
        "                bits = key.split(\"/\")\n",
        "                set_name = bits[0]\n",
        "                class_label = bits[1]\n",
        "                if set_name not in dataset_splits:\n",
        "                    dataset_splits[set_name] = {class_label: value}\n",
        "                else:\n",
        "                    dataset_splits[set_name][class_label] = value\n",
        "        else:\n",
        "            data_image_paths, index_to_label_name_dict_file, label_to_index = self.load_datapaths()\n",
        "            total_label_types = len(data_image_paths)\n",
        "            num_classes_idx = np.arange(len(data_image_paths.keys()), dtype=np.int32)\n",
        "            rng.shuffle(num_classes_idx)\n",
        "            keys = list(data_image_paths.keys())\n",
        "            values = list(data_image_paths.values())\n",
        "            new_keys = [keys[idx] for idx in num_classes_idx]\n",
        "            new_values = [values[idx] for idx in num_classes_idx]\n",
        "            data_image_paths = dict(zip(new_keys, new_values))\n",
        "            # data_image_paths = self.shuffle(data_image_paths)\n",
        "            x_train_id, x_val_id, x_test_id = int(self.train_val_test_split[0] * total_label_types), \\\n",
        "                                              int(np.sum(self.train_val_test_split[:2]) * total_label_types), \\\n",
        "                                              int(total_label_types)\n",
        "            # print(x_train_id, x_val_id, x_test_id)\n",
        "            # print(\"DATA IMAGE PATH FIRST KEY\")\n",
        "            test_first_class_key = list(data_image_paths.keys())[0]\n",
        "            # print(test_first_class_key)\n",
        "            # print(data_image_paths[test_first_class_key])\n",
        "            x_train_classes = (class_key for class_key in list(data_image_paths.keys())[:x_train_id])\n",
        "            x_val_classes = (class_key for class_key in list(data_image_paths.keys())[x_train_id:x_val_id])\n",
        "            x_test_classes = (class_key for class_key in list(data_image_paths.keys())[x_val_id:x_test_id])\n",
        "            x_train, x_val, x_test = {class_key: data_image_paths[class_key] for class_key in x_train_classes}, \\\n",
        "                                     {class_key: data_image_paths[class_key] for class_key in x_val_classes}, \\\n",
        "                                     {class_key: data_image_paths[class_key] for class_key in x_test_classes},\n",
        "            dataset_splits = {\"train\": x_train, \"val\":x_val , \"test\": x_test}\n",
        "\n",
        "        if self.args.load_into_memory is True:\n",
        "\n",
        "            print(\"Loading data into RAM\")\n",
        "            x_loaded = {\"train\": [], \"val\": [], \"test\": []}\n",
        "\n",
        "            for set_key, set_value in dataset_splits.items():\n",
        "                print(\"Currently loading into memory the {} set\".format(set_key))\n",
        "                # print(\"Set value is {}\".format(set_value))\n",
        "                x_loaded[set_key] = {key: np.zeros(len(value), ) for key, value in set_value.items()}\n",
        "                # for class_key, class_value in set_value.items():\n",
        "                with tqdm.tqdm(total=len(set_value)) as pbar_memory_load:\n",
        "                    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "                        # Process the list of files, but split the work across the process pool to use all CPUs!\n",
        "                        for (class_label, class_images_loaded) in executor.map(self.load_parallel_batch, (set_value.items())):\n",
        "                            x_loaded[set_key][class_label] = class_images_loaded\n",
        "                            pbar_memory_load.update(1)\n",
        "\n",
        "            dataset_splits = x_loaded\n",
        "            self.data_loaded_in_memory = True\n",
        "\n",
        "        return dataset_splits\n",
        "\n",
        "    def load_datapaths(self):\n",
        "        \"\"\"\n",
        "        If saved json dictionaries of the data are available, then this method loads the dictionaries such that the\n",
        "        data is ready to be read. If the json dictionaries do not exist, then this method calls get_data_paths()\n",
        "        which will build the json dictionary containing the class to filepath samples, and then store them.\n",
        "        :return: data_image_paths: dict containing class to filepath list pairs.\n",
        "                 index_to_label_name_dict_file: dict containing numerical indexes mapped to the human understandable\n",
        "                 string-names of the class\n",
        "                 label_to_index: dictionary containing human understandable string mapped to numerical indexes\n",
        "        \"\"\"\n",
        "        dataset_dir = config[\"dataset_path\"]\n",
        "        data_path_file = \"{}/{}.json\".format(dataset_dir, self.dataset_name)\n",
        "        self.index_to_label_name_dict_file = \"{}/map_to_label_name_{}.json\".format(dataset_dir, self.dataset_name)\n",
        "        # print(self.index_to_label_name_dict_file)\n",
        "        self.label_name_to_map_dict_file = \"{}/label_name_to_map_{}.json\".format(dataset_dir, self.dataset_name)\n",
        "        # print(self.label_name_to_map_dict_file)\n",
        "\n",
        "        if not os.path.exists(data_path_file):\n",
        "            self.reset_stored_filepaths = True\n",
        "\n",
        "        if self.reset_stored_filepaths == True:\n",
        "            if os.path.exists(data_path_file):\n",
        "                os.remove(data_path_file)\n",
        "            self.reset_stored_filepaths = False\n",
        "\n",
        "        try:\n",
        "            data_image_paths = self.load_from_json(filename=data_path_file)\n",
        "            #json name difference; takes in /content/datasets...\n",
        "            #changed to datasets/... which is appended to new path\n",
        "            label_to_index = self.load_from_json(filename=self.label_name_to_map_dict_file)\n",
        "            index_to_label_name_dict_file = self.load_from_json(filename=self.index_to_label_name_dict_file)\n",
        "\n",
        "\n",
        "            # print(data_image_paths)\n",
        "            # print(index_to_label_name_dict_file)\n",
        "            # print(label_to_index)\n",
        "            return data_image_paths, index_to_label_name_dict_file, label_to_index\n",
        "        except:\n",
        "            print(\"Mapped data paths can't be found, remapping paths..\")\n",
        "            data_image_paths, code_to_label_name, label_name_to_code = self.get_data_paths()\n",
        "            self.save_to_json(dict_to_store=data_image_paths, filename=data_path_file)\n",
        "            self.save_to_json(dict_to_store=code_to_label_name, filename=self.index_to_label_name_dict_file)\n",
        "            self.save_to_json(dict_to_store=label_name_to_code, filename=self.label_name_to_map_dict_file)\n",
        "            return self.load_datapaths()\n",
        "\n",
        "    def save_to_json(self, filename, dict_to_store):\n",
        "        with open(os.path.abspath(filename), 'w') as f:\n",
        "            json.dump(dict_to_store, fp=f)\n",
        "\n",
        "    def load_from_json(self, filename):\n",
        "        with open(filename, mode=\"r\") as f:\n",
        "            load_dict = json.load(fp=f)\n",
        "\n",
        "        return load_dict\n",
        "\n",
        "    def load_test_image(self, filepath):\n",
        "        \"\"\"\n",
        "        Tests whether a target filepath contains an uncorrupted image. If image is corrupted, attempt to fix.\n",
        "        :param filepath: Filepath of image to be tested\n",
        "        :return: Return filepath of image if image exists and is uncorrupted (or attempt to fix has succeeded),\n",
        "        else return None\n",
        "        \"\"\"\n",
        "        image = None\n",
        "        try:\n",
        "            image = Image.open(filepath)\n",
        "        except RuntimeWarning:\n",
        "            os.system(\"convert {} -strip {}\".format(filepath, filepath))\n",
        "            print(\"converting\")\n",
        "            image = Image.open(filepath)\n",
        "        except:\n",
        "            print(\"Broken image\")\n",
        "\n",
        "        if image is not None:\n",
        "            return filepath\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_data_paths(self):\n",
        "        \"\"\"\n",
        "        Method that scans the dataset directory and generates class to image-filepath list dictionaries.\n",
        "        :return: data_image_paths: dict containing class to filepath list pairs.\n",
        "                 index_to_label_name_dict_file: dict containing numerical indexes mapped to the human understandable\n",
        "                 string-names of the class\n",
        "                 label_to_index: dictionary containing human understandable string mapped to numerical indexes\n",
        "        \"\"\"\n",
        "        print(\"Get images from\", self.data_path)\n",
        "        data_image_path_list_raw = []\n",
        "        labels = set()\n",
        "        for subdir, dir, files in os.walk(self.data_path):\n",
        "            for file in files:\n",
        "                if (\".jpeg\") in file.lower() or (\".png\") in file.lower() or (\".jpg\") in file.lower():\n",
        "                    filepath = os.path.abspath(os.path.join(subdir, file))\n",
        "                    label = self.get_label_from_path(filepath)\n",
        "                    data_image_path_list_raw.append(filepath)\n",
        "                    labels.add(label)\n",
        "\n",
        "        labels = sorted(labels)\n",
        "        idx_to_label_name = {idx: label for idx, label in enumerate(labels)}\n",
        "        label_name_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
        "        data_image_path_dict = {idx: [] for idx in list(idx_to_label_name.keys())}\n",
        "        with tqdm.tqdm(total=len(data_image_path_list_raw)) as pbar_error:\n",
        "            with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "                # Process the list of files, but split the work across the process pool to use all CPUs!\n",
        "                for image_file in executor.map(self.load_test_image, (data_image_path_list_raw)):\n",
        "                    pbar_error.update(1)\n",
        "                    if image_file is not None:\n",
        "                        label = self.get_label_from_path(image_file)\n",
        "                        data_image_path_dict[label_name_to_idx[label]].append(image_file)\n",
        "\n",
        "        return data_image_path_dict, idx_to_label_name, label_name_to_idx\n",
        "\n",
        "    def get_label_set(self):\n",
        "        \"\"\"\n",
        "        Generates a set containing all class numerical indexes\n",
        "        :return: A set containing all class numerical indexes\n",
        "        \"\"\"\n",
        "        index_to_label_name_dict_file = self.load_from_json(filename=self.index_to_label_name_dict_file)\n",
        "        return set(list(index_to_label_name_dict_file.keys()))\n",
        "\n",
        "    def get_index_from_label(self, label):\n",
        "        \"\"\"\n",
        "        Given a class's (human understandable) string, returns the numerical index of that class\n",
        "        :param label: A string of a human understandable class contained in the dataset\n",
        "        :return: An int containing the numerical index of the given class-string\n",
        "        \"\"\"\n",
        "        label_to_index = self.load_from_json(filename=self.label_name_to_map_dict_file)\n",
        "        return label_to_index[label]\n",
        "\n",
        "    def get_label_from_index(self, index):\n",
        "        \"\"\"\n",
        "        Given an index return the human understandable label mapping to it.\n",
        "        :param index: A numerical index (int)\n",
        "        :return: A human understandable label (str)\n",
        "        \"\"\"\n",
        "        index_to_label_name = self.load_from_json(filename=self.index_to_label_name_dict_file)\n",
        "        return index_to_label_name[index]\n",
        "\n",
        "    def get_label_from_path(self, filepath):\n",
        "        \"\"\"\n",
        "        Given a path of an image generate the human understandable label for that image.\n",
        "        :param filepath: The image's filepath\n",
        "        :return: A human understandable label.\n",
        "        \"\"\"\n",
        "        label_bits = filepath.split(\"/\")\n",
        "        label = \"/\".join([label_bits[idx] for idx in self.indexes_of_folders_indicating_class])\n",
        "        if self.labels_as_int:\n",
        "            label = int(label)\n",
        "        return label\n",
        "\n",
        "    def load_image(self, image_path, channels):\n",
        "        \"\"\"\n",
        "        Given an image filepath and the number of channels to keep, load an image and keep the specified channels\n",
        "        :param image_path: The image's filepath\n",
        "        :param channels: The number of channels to keep\n",
        "        :return: An image array of shape (h, w, channels), whose values range between 0.0 and 1.0.\n",
        "        \"\"\"\n",
        "        if not self.data_loaded_in_memory:\n",
        "            image = Image.open(image_path)\n",
        "            if 'omniglot' in self.dataset_name:\n",
        "                image = image.resize((self.image_height, self.image_width), resample=Image.LANCZOS)\n",
        "                image = np.array(image, np.float32)\n",
        "                if channels == 1:\n",
        "                    image = np.expand_dims(image, axis=2)\n",
        "            else:\n",
        "                image = image.resize((self.image_height, self.image_width)).convert('RGB')\n",
        "                image = np.array(image, np.float32)\n",
        "                image = image / 255.0\n",
        "        else:\n",
        "            image = image_path\n",
        "\n",
        "        return image\n",
        "\n",
        "    def load_batch(self, batch_image_paths):\n",
        "        \"\"\"\n",
        "        Load a batch of images, given a list of filepaths\n",
        "        :param batch_image_paths: A list of filepaths\n",
        "        :return: A numpy array of images of shape batch, height, width, channels\n",
        "        \"\"\"\n",
        "        image_batch = []\n",
        "\n",
        "        if self.data_loaded_in_memory:\n",
        "            for image_path in batch_image_paths:\n",
        "                image_batch.append(image_path)\n",
        "            image_batch = np.array(image_batch, dtype=np.float32)\n",
        "            #print(image_batch.shape)\n",
        "        else:\n",
        "            print(\"BATCH IMAGE PATH (no content?):\")\n",
        "            print(image_path)\n",
        "            image_batch = [self.load_image(image_path=image_path, channels=self.image_channel)\n",
        "                           for image_path in batch_image_paths]\n",
        "            image_batch = np.array(image_batch, dtype=np.float32)\n",
        "            image_batch = self.preprocess_data(image_batch)\n",
        "\n",
        "        return image_batch\n",
        "\n",
        "    def load_parallel_batch(self, inputs):\n",
        "        \"\"\"\n",
        "        Load a batch of images, given a list of filepaths\n",
        "        :param batch_image_paths: A list of filepaths\n",
        "        :return: A numpy array of images of shape batch, height, width, channels\n",
        "        \"\"\"\n",
        "        class_label, batch_image_paths = inputs\n",
        "        image_batch = []\n",
        "\n",
        "        if self.data_loaded_in_memory:\n",
        "            for image_path in batch_image_paths:\n",
        "                image_batch.append(np.copy(image_path))\n",
        "            image_batch = np.array(image_batch, dtype=np.float32)\n",
        "        else:\n",
        "            #with tqdm.tqdm(total=1) as load_pbar:\n",
        "            image_batch = [self.load_image(image_path=image_path, channels=self.image_channel)\n",
        "                           for image_path in batch_image_paths]\n",
        "                #load_pbar.update(1)\n",
        "\n",
        "            image_batch = np.array(image_batch, dtype=np.float32)\n",
        "            image_batch = self.preprocess_data(image_batch)\n",
        "\n",
        "        return class_label, image_batch\n",
        "\n",
        "    def preprocess_data(self, x):\n",
        "        \"\"\"\n",
        "        Preprocesses data such that their shapes match the specified structures\n",
        "        :param x: A data batch to preprocess\n",
        "        :return: A preprocessed data batch\n",
        "        \"\"\"\n",
        "        x_shape = x.shape\n",
        "        x = np.reshape(x, (-1, x_shape[-3], x_shape[-2], x_shape[-1]))\n",
        "        if self.reverse_channels is True:\n",
        "            reverse_photos = np.ones(shape=x.shape)\n",
        "            for channel in range(x.shape[-1]):\n",
        "                reverse_photos[:, :, :, x.shape[-1] - 1 - channel] = x[:, :, :, channel]\n",
        "            x = reverse_photos\n",
        "        x = x.reshape(x_shape)\n",
        "        return x\n",
        "\n",
        "    def reconstruct_original(self, x):\n",
        "        \"\"\"\n",
        "        Applies the reverse operations that preprocess_data() applies such that the data returns to their original form\n",
        "        :param x: A batch of data to reconstruct\n",
        "        :return: A reconstructed batch of data\n",
        "        \"\"\"\n",
        "        x = x * 255.0\n",
        "        return x\n",
        "\n",
        "    def shuffle(self, x, rng):\n",
        "        \"\"\"\n",
        "        Shuffles the data batch along it's first axis\n",
        "        :param x: A data batch\n",
        "        :return: A shuffled data batch\n",
        "        \"\"\"\n",
        "        indices = np.arange(len(x))\n",
        "        rng.shuffle(indices)\n",
        "        x = x[indices]\n",
        "        return x\n",
        "\n",
        "    def get_set(self, dataset_name, seed, augment_images=False):\n",
        "        \"\"\"\n",
        "        Generates a task-set to be used for training or evaluation\n",
        "        :param set_name: The name of the set to use, e.g. \"train\", \"val\" etc.\n",
        "        :return: A task-set containing an image and label support set, and an image and label target set.\n",
        "        \"\"\"\n",
        "        #seed = seed % self.args.total_unique_tasks\n",
        "        rng = np.random.RandomState(seed)\n",
        "\n",
        "        # print(self.dataset_size_dict)\n",
        "        selected_classes = rng.choice(list(self.dataset_size_dict[dataset_name].keys()),\n",
        "                                      size=self.num_classes_per_set, replace=False)\n",
        "        rng.shuffle(selected_classes)\n",
        "        k_list = rng.randint(0, 4, size=self.num_classes_per_set)\n",
        "        k_dict = {selected_class: k_item for (selected_class, k_item) in zip(selected_classes, k_list)}\n",
        "        episode_labels = [i for i in range(self.num_classes_per_set)]\n",
        "        class_to_episode_label = {selected_class: episode_label for (selected_class, episode_label) in\n",
        "                                  zip(selected_classes, episode_labels)}\n",
        "\n",
        "        x_images = []\n",
        "        y_labels = []\n",
        "\n",
        "        for class_entry in selected_classes:\n",
        "            choose_samples_list = rng.choice(self.dataset_size_dict[dataset_name][class_entry],\n",
        "                                             size=self.num_samples_per_class + self.num_target_samples, replace=False)\n",
        "            class_image_samples = []\n",
        "            class_labels = []\n",
        "            for sample in choose_samples_list:\n",
        "                choose_samples = self.datasets[dataset_name][class_entry][sample]\n",
        "                x_class_data = self.load_batch([choose_samples])[0]\n",
        "                k = k_dict[class_entry]\n",
        "                x_class_data = augment_image(image=x_class_data, k=k,\n",
        "                                             channels=self.image_channel, augment_bool=augment_images,\n",
        "                                             dataset_name=self.dataset_name, args=self.args)\n",
        "                class_image_samples.append(x_class_data)\n",
        "                class_labels.append(int(class_to_episode_label[class_entry]))\n",
        "            class_image_samples = torch.stack(class_image_samples)\n",
        "            x_images.append(class_image_samples)\n",
        "            y_labels.append(class_labels)\n",
        "\n",
        "        x_images = torch.stack(x_images)\n",
        "        y_labels = np.array(y_labels, dtype=np.float32)\n",
        "\n",
        "        support_set_images = x_images[:, :self.num_samples_per_class]\n",
        "        support_set_labels = y_labels[:, :self.num_samples_per_class]\n",
        "        target_set_images = x_images[:, self.num_samples_per_class:]\n",
        "        target_set_labels = y_labels[:, self.num_samples_per_class:]\n",
        "\n",
        "        return support_set_images, target_set_images, support_set_labels, target_set_labels, seed\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_length[self.current_set_name]\n",
        "\n",
        "    def length(self, set_name):\n",
        "        self.switch_set(set_name=set_name)\n",
        "        return len(self)\n",
        "\n",
        "    def set_augmentation(self, augment_images):\n",
        "        self.augment_images = augment_images\n",
        "\n",
        "    def switch_set(self, set_name, current_iter=None):\n",
        "        self.current_set_name = set_name\n",
        "        if set_name == \"train\":\n",
        "            self.update_seed(dataset_name=set_name, seed=self.init_seed[set_name] + current_iter)\n",
        "\n",
        "    def update_seed(self, dataset_name, seed=100):\n",
        "        self.seed[dataset_name] = seed\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        support_set_images, target_set_image, support_set_labels, target_set_label, seed = \\\n",
        "            self.get_set(self.current_set_name, seed=self.seed[self.current_set_name] + idx,\n",
        "                         augment_images=self.augment_images)\n",
        "\n",
        "        return support_set_images, target_set_image, support_set_labels, target_set_label, seed\n",
        "\n",
        "    def reset_seed(self):\n",
        "        self.seed = self.init_seed\n",
        "\n",
        "\n",
        "class MetaLearningSystemDataLoader(object):\n",
        "    def __init__(self, args, current_iter=0):\n",
        "        \"\"\"\n",
        "        Initializes a meta learning system dataloader. The data loader uses the Pytorch DataLoader class to parallelize\n",
        "        batch sampling and preprocessing.\n",
        "        :param args: An arguments NamedTuple containing all the required arguments.\n",
        "        :param current_iter: Current iter of experiment. Is used to make sure the data loader continues where it left\n",
        "        of previously.\n",
        "        \"\"\"\n",
        "        self.num_of_gpus = args.num_of_gpus\n",
        "        self.batch_size = args.batch_size\n",
        "        self.samples_per_iter = args.samples_per_iter\n",
        "        self.num_workers = args.num_dataprovider_workers\n",
        "        self.total_train_iters_produced = 0\n",
        "        self.dataset = FewShotLearningDatasetParallel(args=args)\n",
        "        self.batches_per_iter = args.samples_per_iter\n",
        "        self.full_data_length = self.dataset.data_length\n",
        "        self.continue_from_iter(current_iter=current_iter)\n",
        "        self.args = args\n",
        "\n",
        "    def get_dataloader(self):\n",
        "        \"\"\"\n",
        "        Returns a data loader with the correct set (train, val or test), continuing from the current iter.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return DataLoader(self.dataset, batch_size=(self.num_of_gpus * self.batch_size * self.samples_per_iter),\n",
        "                          shuffle=False, num_workers=self.num_workers, drop_last=True)\n",
        "\n",
        "    def continue_from_iter(self, current_iter):\n",
        "        \"\"\"\n",
        "        Makes sure the data provider is aware of where we are in terms of training iterations in the experiment.\n",
        "        :param current_iter:\n",
        "        \"\"\"\n",
        "        self.total_train_iters_produced += (current_iter * (self.num_of_gpus * self.batch_size * self.samples_per_iter))\n",
        "\n",
        "    def get_train_batches(self, total_batches=-1, augment_images=False):\n",
        "        \"\"\"\n",
        "        Returns a training batches data_loader\n",
        "        :param total_batches: The number of batches we want the data loader to sample\n",
        "        :param augment_images: Whether we want the images to be augmented.\n",
        "        \"\"\"\n",
        "        if total_batches == -1:\n",
        "            self.dataset.data_length = self.full_data_length\n",
        "        else:\n",
        "            self.dataset.data_length[\"train\"] = total_batches * self.dataset.batch_size\n",
        "        self.dataset.switch_set(set_name=\"train\", current_iter=self.total_train_iters_produced)\n",
        "        self.dataset.set_augmentation(augment_images=augment_images)\n",
        "        self.total_train_iters_produced += (self.num_of_gpus * self.batch_size * self.samples_per_iter)\n",
        "        for sample_id, sample_batched in enumerate(self.get_dataloader()):\n",
        "            yield sample_batched\n",
        "\n",
        "\n",
        "    def get_val_batches(self, total_batches=-1, augment_images=False):\n",
        "        \"\"\"\n",
        "        Returns a validation batches data_loader\n",
        "        :param total_batches: The number of batches we want the data loader to sample\n",
        "        :param augment_images: Whether we want the images to be augmented.\n",
        "        \"\"\"\n",
        "        if total_batches == -1:\n",
        "            self.dataset.data_length = self.full_data_length\n",
        "        else:\n",
        "            self.dataset.data_length['val'] = total_batches * self.dataset.batch_size\n",
        "        self.dataset.switch_set(set_name=\"val\")\n",
        "        self.dataset.set_augmentation(augment_images=augment_images)\n",
        "        for sample_id, sample_batched in enumerate(self.get_dataloader()):\n",
        "            yield sample_batched\n",
        "\n",
        "\n",
        "    def get_test_batches(self, total_batches=-1, augment_images=False):\n",
        "        \"\"\"\n",
        "        Returns a testing batches data_loader\n",
        "        :param total_batches: The number of batches we want the data loader to sample\n",
        "        :param augment_images: Whether we want the images to be augmented.\n",
        "        \"\"\"\n",
        "        if total_batches == -1:\n",
        "            self.dataset.data_length = self.full_data_length\n",
        "        else:\n",
        "            self.dataset.data_length['test'] = total_batches * self.dataset.batch_size\n",
        "        self.dataset.switch_set(set_name='test')\n",
        "        self.dataset.set_augmentation(augment_images=augment_images)\n",
        "        for sample_id, sample_batched in enumerate(self.get_dataloader()):\n",
        "            yield sample_batched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27S6f3VZp0jo"
      },
      "source": [
        "# Train MAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wXc8OJUvwZp2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "  \"batch_size\":8,\n",
        "  \"image_height\":28,\n",
        "  \"image_width\":28,\n",
        "  \"image_channels\":1,\n",
        "  \"gpu_to_use\":0,\n",
        "  \"num_dataprovider_workers\":4,\n",
        "  \"max_models_to_save\":5,\n",
        "  \"dataset_name\":\"mini_imagenet\",\n",
        "  \"dataset_path\":\"/content/HowToTrainYourMAMLPytorch/datasets\",\n",
        "  \"reset_stored_paths\":False,\n",
        "  \"experiment_name\":\"mini-imagenet_5_2_0.01_48_5_2\",\n",
        "  \"train_seed\": 2, \"val_seed\": 0,\n",
        "  \"train_val_test_split\": [0.70918052988, 0.03080714725, 0.2606284658],\n",
        "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
        "  \"load_from_npz_files\": False,\n",
        "  \"sets_are_pre_split\": False,\n",
        "  \"load_into_memory\": True,\n",
        "  \"init_inner_loop_learning_rate\": 0.1,\n",
        "  \"train_in_stages\": False,\n",
        "  \"multi_step_loss_num_epochs\": 10,\n",
        "  \"minimum_per_task_contribution\": 0.01,\n",
        "  \"num_evaluation_tasks\":600,\n",
        "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
        "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
        "\n",
        "  \"total_epochs\": 150,\n",
        "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
        "  \"evaluate_on_test_set_only\": False,\n",
        "  \"max_pooling\": True,\n",
        "  \"per_step_bn_statistics\": True,\n",
        "  \"learnable_batch_norm_momentum\": False,\n",
        "  \"evalute_on_test_set_only\": False,\n",
        "  \"learnable_bn_gamma\": True,\n",
        "  \"learnable_bn_beta\": True,\n",
        "\n",
        "  \"weight_decay\": 0.0,\n",
        "  \"dropout_rate_value\":0.0,\n",
        "  \"min_learning_rate\":0.00001,\n",
        "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 150,\n",
        "  \"first_order_to_second_order_epoch\":-1,\n",
        "\n",
        "  \"norm_layer\":\"batch_norm\",\n",
        "  \"cnn_num_filters\":64,\n",
        "  \"num_stages\":4,\n",
        "  \"conv_padding\": True,\n",
        "  \"number_of_training_steps_per_iter\":5,\n",
        "  \"number_of_evaluation_steps_per_iter\":5,\n",
        "  \"cnn_blocks_per_stage\":1,\n",
        "  \"num_classes_per_set\":5,\n",
        "  \"num_samples_per_class\":5,\n",
        "  \"num_target_samples\": 1,\n",
        "\n",
        "  \"second_order\": True,\n",
        "  \"use_multi_step_loss_optimization\":True,\n",
        "\n",
        "\n",
        "  # \"seed\": 2,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "with open(\"omniglot_maml++-omniglot_5_8_0.1_64_20_2.json\", \"w\") as outfile:\n",
        "    json.dump(config, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "D0LK35JzzHRM"
      },
      "outputs": [],
      "source": [
        "# from torch import cuda\n",
        "\n",
        "\n",
        "# def get_args():\n",
        "#     import argparse\n",
        "#     import os\n",
        "#     import torch\n",
        "#     import json\n",
        "#     parser = argparse.ArgumentParser(description='Welcome to the MAML++ training and inference system')\n",
        "\n",
        "#     parser.add_argument('--batch_size', nargs=\"?\", type=int, default=32, help='Batch_size for experiment')\n",
        "#     parser.add_argument('--image_height', nargs=\"?\", type=int, default=28)\n",
        "#     parser.add_argument('--image_width', nargs=\"?\", type=int, default=28)\n",
        "#     parser.add_argument('--image_channels', nargs=\"?\", type=int, default=1)\n",
        "#     parser.add_argument('--reset_stored_filepaths', type=str, default=\"False\")\n",
        "#     parser.add_argument('--reverse_channels', type=str, default=\"False\")\n",
        "#     parser.add_argument('--num_of_gpus', type=int, default=1)\n",
        "#     parser.add_argument('--indexes_of_folders_indicating_class', nargs='+', default=[-2, -3])\n",
        "#     parser.add_argument('--train_val_test_split', nargs='+', default=[0.73982737361, 0.26, 0.13008631319])\n",
        "#     parser.add_argument('--samples_per_iter', nargs=\"?\", type=int, default=1)\n",
        "#     parser.add_argument('--labels_as_int', type=str, default=\"False\")\n",
        "#     parser.add_argument('--seed', type=int, default=104)\n",
        "\n",
        "#     parser.add_argument('--gpu_to_use', type=int)\n",
        "#     parser.add_argument('--num_dataprovider_workers', nargs=\"?\", type=int, default=4)\n",
        "#     parser.add_argument('--max_models_to_save', nargs=\"?\", type=int, default=5)\n",
        "#     parser.add_argument('--dataset_name', type=str, default=\"omniglot_dataset\")\n",
        "#     parser.add_argument('--dataset_path', type=str, default=\"datasets/omniglot_dataset\")\n",
        "#     parser.add_argument('--reset_stored_paths', type=str, default=\"False\")\n",
        "#     parser.add_argument('--experiment_name', nargs=\"?\", type=str, )\n",
        "#     parser.add_argument('--architecture_name', nargs=\"?\", type=str)\n",
        "#     parser.add_argument('--continue_from_epoch', nargs=\"?\", type=str, default='latest', help='Continue from checkpoint of epoch')\n",
        "#     parser.add_argument('--dropout_rate_value', type=float, default=0.3, help='Dropout_rate_value')\n",
        "#     parser.add_argument('--num_target_samples', type=int, default=15, help='Dropout_rate_value')\n",
        "#     parser.add_argument('--second_order', type=str, default=\"False\", help='Dropout_rate_value')\n",
        "#     parser.add_argument('--total_epochs', type=int, default=200, help='Number of epochs per experiment')\n",
        "#     parser.add_argument('--total_iter_per_epoch', type=int, default=500, help='Number of iters per epoch')\n",
        "#     parser.add_argument('--min_learning_rate', type=float, default=0.00001, help='Min learning rate')\n",
        "#     parser.add_argument('--meta_learning_rate', type=float, default=0.001, help='Learning rate of overall MAML system')\n",
        "#     parser.add_argument('--meta_opt_bn', type=str, default=\"False\")\n",
        "#     parser.add_argument('--task_learning_rate', type=float, default=0.1, help='Learning rate per task gradient step')\n",
        "\n",
        "#     parser.add_argument('--norm_layer', type=str, default=\"batch_norm\")\n",
        "#     parser.add_argument('--max_pooling', type=str, default=\"False\")\n",
        "#     parser.add_argument('--per_step_bn_statistics', type=str, default=\"False\")\n",
        "#     parser.add_argument('--num_classes_per_set', type=int, default=20, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--cnn_num_blocks', type=int, default=4, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--number_of_training_steps_per_iter', type=int, default=1, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--number_of_evaluation_steps_per_iter', type=int, default=1, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--cnn_num_filters', type=int, default=64, help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--cnn_blocks_per_stage', type=int, default=1,\n",
        "#                         help='Number of classes to sample per set')\n",
        "#     parser.add_argument('--num_samples_per_class', type=int, default=1, help='Number of samples per set to sample')\n",
        "#     parser.add_argument('--name_of_args_json_file', type=str, default=\"None\")\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "#     args_dict = vars(args)\n",
        "#     if args.name_of_args_json_file is not \"None\":\n",
        "#         args_dict = extract_args_from_json(args.name_of_args_json_file, args_dict)\n",
        "\n",
        "#     for key in list(args_dict.keys()):\n",
        "\n",
        "#         if str(args_dict[key]).lower() == \"true\":\n",
        "#             args_dict[key] = True\n",
        "#         elif str(args_dict[key]).lower() == \"false\":\n",
        "#             args_dict[key] = False\n",
        "#         if key == \"dataset_path\":\n",
        "#             args_dict[key] = os.path.join(os.environ['DATASET_DIR'], args_dict[key])\n",
        "#             print(key, os.path.join(os.environ['DATASET_DIR'], args_dict[key]))\n",
        "\n",
        "#         print(key, args_dict[key], type(args_dict[key]))\n",
        "\n",
        "#     args = Bunch(args_dict)\n",
        "\n",
        "\n",
        "#     args.use_cuda = torch.cuda.is_available()\n",
        "#     if torch.cuda.is_available():  # checks whether a cuda gpu is available and whether the gpu flag is True\n",
        "#         device = torch.cuda.current_device()\n",
        "\n",
        "#         print(\"use GPU\", device)\n",
        "#         print(\"GPU ID {}\".format(torch.cuda.current_device()))\n",
        "\n",
        "#     else:\n",
        "#         print(\"use CPU\")\n",
        "#         device = torch.device('cpu')  # sets the device to be CPU\n",
        "\n",
        "\n",
        "#     return args, device\n",
        "\n",
        "\n",
        "\n",
        "# class Bunch(object):\n",
        "#   def __init__(self, adict):\n",
        "#     self.__dict__.update(adict)\n",
        "\n",
        "# def extract_args_from_json(json_file_path, args_dict):\n",
        "#     import json\n",
        "#     summary_filename = json_file_path\n",
        "#     with open(summary_filename) as f:\n",
        "#         summary_dict = json.load(fp=f)\n",
        "\n",
        "#     for key in summary_dict.keys():\n",
        "#         if \"continue_from\" not in key and \"gpu_to_use\" not in key:\n",
        "#             args_dict[key] = summary_dict[key]\n",
        "\n",
        "#     return args_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RcsmAhVaCUNP"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "class Bunch(object):\n",
        "    def __init__(self, adict):\n",
        "        self.__dict__.update(adict)\n",
        "\n",
        "def load_args_from_json(json_file_path):\n",
        "    def extract_args_from_json(json_file_path, args_dict):\n",
        "        with open(json_file_path) as f:\n",
        "            summary_dict = json.load(fp=f)\n",
        "        for key, value in summary_dict.items():\n",
        "            if \"continue_from\" not in key and \"gpu_to_use\" not in key:\n",
        "                args_dict[key] = value\n",
        "        return args_dict\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Welcome to the MAML++ training and inference system')\n",
        "\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch_size for experiment')\n",
        "    parser.add_argument('--image_height', type=int, default=28)\n",
        "    parser.add_argument('--image_width', type=int, default=28)\n",
        "    parser.add_argument('--image_channels', type=int, default=1)\n",
        "    parser.add_argument('--reset_stored_filepaths', type=str, default=\"False\")\n",
        "    parser.add_argument('--reverse_channels', type=str, default=\"False\")\n",
        "    parser.add_argument('--num_of_gpus', type=int, default=1)\n",
        "    parser.add_argument('--indexes_of_folders_indicating_class', nargs='+', default=[-2, -3])\n",
        "    parser.add_argument('--train_val_test_split', nargs='+', default=[0.73982737361, 0.26, 0.13008631319])\n",
        "    parser.add_argument('--samples_per_iter', type=int, default=1)\n",
        "    parser.add_argument('--labels_as_int', type=str, default=\"False\")\n",
        "    parser.add_argument('--seed', type=int, default=104)\n",
        "\n",
        "    parser.add_argument('--gpu_to_use', type=int)\n",
        "    parser.add_argument('--num_dataprovider_workers', type=int, default=4)\n",
        "    parser.add_argument('--max_models_to_save', type=int, default=5)\n",
        "    parser.add_argument('--dataset_name', type=str, default=\"omniglot_dataset\")\n",
        "    parser.add_argument('--dataset_path', type=str, default=\"datasets/omniglot_dataset\")\n",
        "    parser.add_argument('--reset_stored_paths', type=str, default=\"False\")\n",
        "    parser.add_argument('--experiment_name', type=str)\n",
        "    parser.add_argument('--architecture_name', type=str)\n",
        "    parser.add_argument('--continue_from_epoch', type=str, default='latest', help='Continue from checkpoint of epoch')\n",
        "    parser.add_argument('--dropout_rate_value', type=float, default=0.3, help='Dropout_rate_value')\n",
        "    parser.add_argument('--num_target_samples', type=int, default=15, help='Dropout_rate_value')\n",
        "    parser.add_argument('--second_order', type=str, default=\"False\", help='Dropout_rate_value')\n",
        "    parser.add_argument('--total_epochs', type=int, default=200, help='Number of epochs per experiment')\n",
        "    parser.add_argument('--total_iter_per_epoch', type=int, default=500, help='Number of iters per epoch')\n",
        "    parser.add_argument('--min_learning_rate', type=float, default=0.00001, help='Min learning rate')\n",
        "    parser.add_argument('--meta_learning_rate', type=float, default=0.001, help='Learning rate of overall MAML system')\n",
        "    parser.add_argument('--meta_opt_bn', type=str, default=\"False\")\n",
        "    parser.add_argument('--task_learning_rate', type=float, default=0.1, help='Learning rate per task gradient step')\n",
        "\n",
        "    parser.add_argument('--norm_layer', type=str, default=\"batch_norm\")\n",
        "    parser.add_argument('--max_pooling', type=str, default=\"False\")\n",
        "    parser.add_argument('--per_step_bn_statistics', type=str, default=\"False\")\n",
        "    parser.add_argument('--num_classes_per_set', type=int, default=20, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--cnn_num_blocks', type=int, default=4, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--number_of_training_steps_per_iter', type=int, default=1, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--number_of_evaluation_steps_per_iter', type=int, default=1, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--cnn_num_filters', type=int, default=64, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--cnn_blocks_per_stage', type=int, default=1, help='Number of classes to sample per set')\n",
        "    parser.add_argument('--num_samples_per_class', type=int, default=1, help='Number of samples per set to sample')\n",
        "    parser.add_argument('--name_of_args_json_file', type=str, default=\"None\")\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "    args_dict = vars(args)\n",
        "\n",
        "    # Override args with JSON file values\n",
        "    if json_file_path:\n",
        "        args_dict = extract_args_from_json(json_file_path, args_dict)\n",
        "\n",
        "    # Convert string-based booleans to actual booleans\n",
        "    for key in args_dict:\n",
        "        if isinstance(args_dict[key], str) and args_dict[key].lower() == \"true\":\n",
        "            args_dict[key] = True\n",
        "        elif isinstance(args_dict[key], str) and args_dict[key].lower() == \"false\":\n",
        "            args_dict[key] = False\n",
        "\n",
        "    # Resolve dataset path if environment variable is set\n",
        "    if \"dataset_path\" in args_dict and config[\"dataset_path\"]:\n",
        "        args_dict[\"dataset_path\"] = os.path.join(config[\"dataset_path\"], args_dict[\"dataset_path\"])\n",
        "\n",
        "    args = Bunch(args_dict)\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    args.use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if args.use_cuda else 'cpu')\n",
        "\n",
        "    return args, device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Hb1S5Yh6zf6J"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def maybe_unzip_dataset(args):\n",
        "\n",
        "    datasets = [args.dataset_name]\n",
        "    dataset_paths = [args.dataset_path]\n",
        "    done = False\n",
        "\n",
        "    for dataset_idx, dataset_path in enumerate(dataset_paths):\n",
        "        if dataset_path.endswith('/'):\n",
        "            dataset_path = dataset_path[:-1]\n",
        "        # print(dataset_path)\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(\"Not found dataset folder structure.. searching for .tar.bz2 file\")\n",
        "            zip_directory = \"{}.tar.bz2\".format(os.path.join(config[\"dataset_path\"], datasets[dataset_idx]))\n",
        "\n",
        "            assert os.path.exists(os.path.abspath(zip_directory)), \"{} dataset zip file not found\" \\\n",
        "                                                  \"place dataset in datasets folder as explained in README\".format(os.path.abspath(zip_directory))\n",
        "            print(\"Found zip file, unpacking\")\n",
        "\n",
        "            unzip_file(filepath_pack=os.path.join(config[\"dataset_path\"], \"{}.tar.bz2\".format(datasets[dataset_idx])),\n",
        "                       filepath_to_store=config[\"dataset_path\"])\n",
        "\n",
        "\n",
        "\n",
        "            args.reset_stored_filepaths = True\n",
        "\n",
        "        total_files = 0\n",
        "        for subdir, dir, files in os.walk(dataset_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(\".jpeg\") or file.lower().endswith(\".jpg\") or file.lower().endswith(\n",
        "                        \".png\") or file.lower().endswith(\".pkl\"):\n",
        "                    total_files += 1\n",
        "        print(\"count stuff________________________________________\", total_files)\n",
        "        if (total_files == 1623 * 20 and datasets[dataset_idx] == 'omniglot_dataset') or (\n",
        "                total_files == 100 * 600 and 'mini_imagenet' in datasets[dataset_idx]) or (\n",
        "                total_files == 3 and 'mini_imagenet_pkl' in datasets[dataset_idx]):\n",
        "            print(\"file count is correct\")\n",
        "            done = True\n",
        "        elif datasets[dataset_idx] not in [\n",
        "            'omniglot_dataset',\n",
        "            'mini_imagenet',\n",
        "            'mini_imagenet_pkl',\n",
        "        ]:\n",
        "            done = True\n",
        "            print(\"using new dataset\")\n",
        "\n",
        "        if not done:\n",
        "            shutil.rmtree(dataset_path, ignore_errors=True)\n",
        "            maybe_unzip_dataset(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UKVzZ6KPXvzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31dddb1-f9c8-45b7-b8b8-a099d3bac472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "os.chdir('/content/HowToTrainYourMAMLPytorch')\n",
        "\n",
        "\n",
        "# Create parent directories\n",
        "os.makedirs('/home/antreas', exist_ok=True)\n",
        "\n",
        "# Create symbolic link from /content to /home/antreas\n",
        "os.system('ln -s /content/HowToTrainYourMAMLPytorch /home/antreas/HowToTrainYourMAMLPytorch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R1UWbkAXk_ZD"
      },
      "outputs": [],
      "source": [
        "!touch summary_stats.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LuRv8nXqp2zr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a93847d-e428-434e-d675-044b831981c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using max pooling\n",
            "torch.Size([2, 48, 84, 84])\n",
            "torch.Size([2, 48, 42, 42])\n",
            "torch.Size([2, 48, 21, 21])\n",
            "torch.Size([2, 48, 10, 10])\n",
            "VGGNetwork build torch.Size([2, 5])\n",
            "meta network params\n",
            "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
            "layer_dict.conv0.conv.bias torch.Size([48])\n",
            "layer_dict.conv0.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv1.conv.bias torch.Size([48])\n",
            "layer_dict.conv1.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv2.conv.bias torch.Size([48])\n",
            "layer_dict.conv2.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv3.conv.bias torch.Size([48])\n",
            "layer_dict.conv3.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.linear.weights torch.Size([5, 1200])\n",
            "layer_dict.linear.bias torch.Size([5])\n",
            "0.1\n",
            "Inner Loop parameters\n",
            "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
            "Outer Loop parameters\n",
            "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
            "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
            "classifier.layer_dict.conv0.norm_layer.bias torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv0.norm_layer.weight torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
            "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
            "classifier.layer_dict.conv1.norm_layer.bias torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv1.norm_layer.weight torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
            "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
            "classifier.layer_dict.conv2.norm_layer.bias torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv2.norm_layer.weight torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
            "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
            "classifier.layer_dict.conv3.norm_layer.bias torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv3.norm_layer.weight torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
            "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
            "the grinch added tensor([[[[ 0.0366, -0.0136,  0.0609],\n",
            "          [-0.1540, -0.1250,  0.0168],\n",
            "          [ 0.1609, -0.0529,  0.0982]],\n",
            "\n",
            "         [[-0.0242,  0.0808, -0.1334],\n",
            "          [-0.0563, -0.0312,  0.0892],\n",
            "          [ 0.0176,  0.0561, -0.0330]],\n",
            "\n",
            "         [[-0.1459, -0.1176, -0.2493],\n",
            "          [-0.0600, -0.0621, -0.1369],\n",
            "          [ 0.1923,  0.0177,  0.0440]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1760,  0.0874, -0.1096],\n",
            "          [-0.0036,  0.1494,  0.1443],\n",
            "          [-0.0694,  0.0341, -0.1425]],\n",
            "\n",
            "         [[ 0.1653, -0.0265,  0.0153],\n",
            "          [ 0.1271, -0.0411,  0.0855],\n",
            "          [ 0.1902, -0.0816,  0.0730]],\n",
            "\n",
            "         [[ 0.2127,  0.0577, -0.0395],\n",
            "          [ 0.1484, -0.0287,  0.0396],\n",
            "          [ 0.1332,  0.0251,  0.1973]]],\n",
            "\n",
            "\n",
            "        [[[-0.1183,  0.0057, -0.0192],\n",
            "          [ 0.1279,  0.2638,  0.1146],\n",
            "          [-0.0541,  0.0129, -0.1629]],\n",
            "\n",
            "         [[-0.0710,  0.0479, -0.1533],\n",
            "          [ 0.0442, -0.0056,  0.2875],\n",
            "          [ 0.1240, -0.1000, -0.1729]],\n",
            "\n",
            "         [[ 0.0252,  0.2132, -0.0816],\n",
            "          [-0.0205,  0.1736,  0.0616],\n",
            "          [-0.0889,  0.0257, -0.1043]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0232,  0.1204, -0.1061],\n",
            "          [ 0.0091,  0.0343, -0.1045],\n",
            "          [-0.2785,  0.0464, -0.1235]],\n",
            "\n",
            "         [[ 0.1366,  0.0312,  0.0463],\n",
            "          [-0.1432,  0.0007, -0.0992],\n",
            "          [-0.0737,  0.0553,  0.1026]],\n",
            "\n",
            "         [[-0.1049, -0.0084, -0.0797],\n",
            "          [ 0.0286,  0.0657, -0.0254],\n",
            "          [-0.0530,  0.0700, -0.1164]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0538, -0.0408, -0.0295],\n",
            "          [ 0.0137,  0.1900,  0.1125],\n",
            "          [ 0.1558, -0.0220, -0.0599]],\n",
            "\n",
            "         [[ 0.0542,  0.0260, -0.0246],\n",
            "          [-0.0372, -0.1192,  0.1789],\n",
            "          [ 0.0179, -0.1008, -0.1286]],\n",
            "\n",
            "         [[-0.0199,  0.2143,  0.0131],\n",
            "          [-0.1292, -0.2143, -0.0235],\n",
            "          [-0.1157,  0.1619, -0.0101]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0124, -0.0043,  0.0937],\n",
            "          [-0.0667, -0.0305,  0.0624],\n",
            "          [-0.0099, -0.0163,  0.0110]],\n",
            "\n",
            "         [[-0.0377,  0.0863, -0.0320],\n",
            "          [ 0.0440,  0.0498,  0.0040],\n",
            "          [-0.0918,  0.0936, -0.0183]],\n",
            "\n",
            "         [[ 0.0166, -0.0254,  0.0032],\n",
            "          [-0.0211,  0.0905, -0.0016],\n",
            "          [-0.1463,  0.0934,  0.0338]]]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0329, -0.1129, -0.0698,  0.0165, -0.0255, -0.0011,  0.0008,  0.1762,\n",
            "        -0.1926, -0.1850,  0.0173, -0.0748, -0.0418,  0.0461, -0.0176,  0.0081,\n",
            "         0.0341, -0.1659,  0.1051, -0.1049, -0.0008,  0.0026, -0.0767, -0.1087,\n",
            "         0.1760, -0.1560,  0.0080,  0.2486,  0.0475, -0.2210,  0.0139,  0.0203,\n",
            "         0.0108, -0.0943, -0.0394,  0.1519,  0.0041, -0.1591, -0.0849,  0.0076,\n",
            "         0.0263,  0.1186, -0.0979, -0.0605,  0.0592, -0.0535,  0.0010, -0.1637],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 3.7180e-02,  6.2472e-02,  7.9275e-02,  2.2362e-01,  1.3501e-01,\n",
            "         -4.2056e-02,  7.6042e-02,  5.8889e-02,  1.8096e-01, -6.3512e-02,\n",
            "          1.3052e-01, -1.2000e-02, -1.2475e-02, -4.9394e-02,  2.0063e-02,\n",
            "          1.8136e-02,  4.3669e-02, -9.4203e-02,  6.0531e-02, -7.6137e-02,\n",
            "          2.4204e-02,  7.9669e-02,  2.0765e-03, -1.2347e-01, -8.1067e-02,\n",
            "         -2.7022e-02, -1.2428e-03, -1.5044e-01, -4.0208e-02, -1.2774e-01,\n",
            "          4.8504e-02,  2.0418e-02,  2.2444e-02,  1.6849e-01, -6.8270e-02,\n",
            "          6.8006e-02,  3.9416e-03, -8.9345e-02,  4.3386e-02,  9.4794e-02,\n",
            "          1.4644e-01, -1.0986e-01, -4.8059e-03, -1.4768e-01,  4.7953e-02,\n",
            "         -8.2935e-02, -6.7297e-02,  8.9825e-02],\n",
            "        [-7.0267e-02,  1.3876e-01, -1.2532e-01, -1.0717e-01,  4.8036e-02,\n",
            "         -1.9017e-01, -8.2584e-02, -6.3544e-02,  7.2996e-02, -9.2871e-02,\n",
            "         -1.4883e-01,  1.4367e-01,  1.6129e-01,  8.7999e-02,  1.2850e-02,\n",
            "         -3.8407e-02,  1.6356e-01, -3.7712e-02, -6.5327e-02,  8.8089e-02,\n",
            "         -3.1550e-02, -1.2123e-01, -5.0105e-02, -1.3002e-02, -3.4253e-02,\n",
            "         -8.2850e-02, -9.8646e-02, -3.8362e-02,  9.8417e-02, -4.3977e-02,\n",
            "          2.0346e-02, -1.7787e-01, -5.0068e-02, -1.1617e-01,  6.0188e-02,\n",
            "          7.2818e-03, -8.7272e-02, -1.5902e-01,  2.6152e-02, -3.0838e-02,\n",
            "          1.3111e-01,  4.1489e-03, -2.5636e-01, -2.2688e-01,  2.4187e-02,\n",
            "          1.2483e-01,  4.9336e-03, -1.1846e-01],\n",
            "        [ 8.9995e-02,  9.1830e-02, -4.6226e-03,  5.4647e-02, -4.3764e-02,\n",
            "         -1.7092e-01,  1.5603e-01,  8.7969e-02,  2.4171e-02,  7.3101e-02,\n",
            "         -3.8795e-02,  1.6081e-01,  2.4910e-01,  7.1790e-02,  3.3119e-02,\n",
            "          7.7333e-02, -1.0430e-01,  1.9044e-02,  1.2697e-01,  2.4317e-01,\n",
            "         -9.1379e-02, -3.6836e-03,  3.9565e-02,  1.5913e-01,  3.4774e-02,\n",
            "         -2.1475e-02, -1.8883e-02,  1.7346e-02,  1.6455e-01, -1.3387e-01,\n",
            "         -1.1892e-01,  1.0757e-01, -8.7354e-02,  1.1371e-02, -1.2408e-01,\n",
            "         -1.4019e-01, -6.2066e-02, -6.7967e-02,  9.6435e-02,  1.5019e-01,\n",
            "         -8.5705e-02, -9.4630e-02,  1.0286e-01, -4.8589e-02, -2.0318e-02,\n",
            "          1.3786e-01,  9.6059e-02, -8.6803e-02],\n",
            "        [-1.5655e-02, -7.0537e-02, -9.1429e-03,  6.6965e-02, -1.1969e-01,\n",
            "          6.1911e-02,  7.1336e-02, -1.1044e-01,  3.9743e-02, -3.4038e-02,\n",
            "          1.1344e-01,  8.0599e-02, -8.0878e-02, -5.4776e-02, -7.9786e-02,\n",
            "         -1.5321e-02,  1.2332e-01,  4.6264e-02,  9.5713e-02,  1.2089e-01,\n",
            "         -3.1538e-02, -5.5854e-02,  3.6655e-02, -1.3421e-01,  1.8249e-03,\n",
            "          3.6031e-03, -1.8267e-02,  7.9242e-02, -1.3886e-02,  6.0055e-03,\n",
            "         -4.1960e-02, -7.2346e-02,  6.3188e-02,  1.2111e-01, -9.5764e-02,\n",
            "         -5.2670e-02, -4.4112e-02, -5.6687e-02,  5.9178e-02,  3.7174e-02,\n",
            "          2.2149e-01,  1.5181e-01, -6.2694e-02,  2.3092e-02,  1.2453e-01,\n",
            "         -9.3370e-02, -6.9155e-02, -1.7339e-02],\n",
            "        [-1.2525e-02, -1.7579e-02,  1.1126e-01,  2.2295e-02,  2.6561e-02,\n",
            "          3.7306e-02,  2.0370e-02, -7.8709e-02, -3.7493e-02,  1.0517e-01,\n",
            "          1.6909e-01,  5.1784e-02,  4.2377e-03, -8.1281e-02,  1.3313e-01,\n",
            "          2.3179e-02, -7.9419e-02,  9.3225e-02, -1.2626e-01,  2.5429e-01,\n",
            "         -7.2574e-03, -1.6142e-01, -1.9377e-01,  3.6210e-02,  1.0618e-01,\n",
            "         -1.3523e-01,  4.4247e-02, -9.0626e-02,  5.0371e-02, -3.9013e-02,\n",
            "          5.5932e-02,  5.7709e-02,  1.2547e-01,  5.6481e-02, -1.7325e-01,\n",
            "          3.7506e-04, -4.2417e-02, -5.6947e-02, -5.2692e-02, -6.1723e-02,\n",
            "         -7.4881e-02, -2.4069e-02, -5.2344e-02, -2.1867e-02, -7.6435e-02,\n",
            "          7.0718e-03,  1.4068e-01,  1.7055e-04]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[-8.7494e-02, -2.0637e-02, -4.4899e-03, -4.8741e-02,  1.6676e-01,\n",
            "         -8.4286e-02, -2.2599e-03,  1.5552e-01,  8.0627e-02,  2.0512e-02,\n",
            "          2.3096e-02, -1.2449e-01,  8.9230e-02, -8.0234e-02,  1.8384e-01,\n",
            "         -1.5322e-02, -1.9527e-01,  1.0610e-01, -5.2042e-02, -3.7085e-02,\n",
            "         -7.0506e-03,  1.9769e-03,  1.3130e-01, -1.9989e-01,  8.8067e-02,\n",
            "         -8.2662e-02, -2.0283e-01, -3.1105e-02,  2.2347e-01,  2.4947e-02,\n",
            "          1.5081e-02, -8.8467e-02, -1.6488e-01, -9.5514e-02,  4.8927e-02,\n",
            "          1.4175e-02,  1.7904e-01,  6.6340e-02,  1.7543e-01,  2.6605e-01,\n",
            "          1.2046e-01,  9.5850e-02, -8.7452e-02, -8.1311e-02, -7.9778e-02,\n",
            "          1.5461e-01, -7.6793e-02,  1.9515e-03],\n",
            "        [ 4.7396e-02, -8.8603e-02, -3.3250e-02,  1.9239e-04,  3.1990e-02,\n",
            "         -2.1646e-02, -3.3637e-02, -1.9025e-01,  1.9692e-02,  6.1355e-02,\n",
            "         -8.3333e-02, -7.2804e-02,  7.3740e-03,  3.0397e-03,  1.3464e-01,\n",
            "         -1.8901e-01, -1.3546e-01,  1.7179e-01,  1.2771e-01, -5.6367e-02,\n",
            "         -1.7893e-01,  7.0364e-02,  6.2526e-02,  7.8897e-02,  2.7154e-02,\n",
            "         -7.9279e-02, -1.1234e-01,  2.1846e-01,  4.9088e-02, -1.9869e-01,\n",
            "         -1.0779e-02, -2.7903e-02,  1.0076e-01, -1.0002e-01,  1.8162e-01,\n",
            "          2.4187e-02, -3.7152e-03,  6.1989e-02,  1.3901e-01,  4.8507e-04,\n",
            "          9.8674e-02, -4.4368e-02, -4.0059e-02, -5.0676e-02, -3.5736e-02,\n",
            "          3.3393e-02,  9.2080e-03, -1.0839e-03],\n",
            "        [-2.8780e-02, -1.0923e-01, -1.2057e-01, -1.4996e-01, -7.9963e-02,\n",
            "          3.7442e-02, -8.5170e-02, -1.0523e-01, -1.3555e-01,  1.3402e-01,\n",
            "         -9.7721e-04,  2.2256e-02, -1.8845e-02, -2.5045e-02,  2.1161e-02,\n",
            "          1.4021e-01, -8.1560e-02, -8.4254e-02,  8.8461e-03, -3.7614e-02,\n",
            "         -8.4182e-02, -6.1632e-02, -1.9448e-01, -4.6537e-02, -2.4877e-02,\n",
            "         -3.5444e-02, -6.4592e-02, -1.7037e-02, -5.8762e-02, -4.9061e-02,\n",
            "          6.1189e-02, -1.3159e-02,  1.7265e-01,  6.5419e-02,  8.1347e-02,\n",
            "          2.4320e-01,  4.8994e-02,  3.7469e-03, -6.4463e-02,  6.4616e-02,\n",
            "         -6.7563e-03, -1.2413e-01, -6.8978e-02, -3.3764e-02, -1.7137e-02,\n",
            "         -8.2019e-02,  4.4240e-03, -5.5150e-02],\n",
            "        [-1.6263e-02,  1.2450e-01,  6.3608e-02,  8.4387e-03,  4.6497e-03,\n",
            "          4.1836e-02,  3.9319e-02,  4.2876e-02,  5.3406e-02,  1.8542e-01,\n",
            "          7.7980e-02, -7.6645e-02, -3.8197e-02,  6.3520e-03,  2.2031e-02,\n",
            "         -9.7075e-02,  1.5850e-01, -1.3038e-02, -1.1305e-03,  8.5594e-02,\n",
            "          6.8732e-02, -8.2545e-02, -9.3282e-02,  6.3332e-02, -7.4911e-02,\n",
            "          1.0573e-01,  9.8219e-02,  2.7497e-02, -6.6704e-02, -4.8227e-04,\n",
            "         -6.1173e-02, -1.4382e-01, -1.9742e-03,  1.0600e-01, -1.1194e-01,\n",
            "         -1.1457e-01, -1.9570e-01,  1.1940e-02, -4.1474e-02, -8.1186e-02,\n",
            "          4.7412e-02,  8.4195e-02,  1.6238e-01,  1.0852e-01, -2.2425e-01,\n",
            "         -4.6980e-02,  4.6475e-02, -1.3551e-01],\n",
            "        [ 4.3573e-02, -7.8043e-02, -5.6239e-02, -2.1019e-01,  1.3714e-01,\n",
            "          1.1261e-01,  7.8170e-03, -8.8846e-02, -1.6708e-01, -2.1583e-02,\n",
            "          1.6745e-01,  6.6755e-04,  1.3611e-01, -1.2790e-01, -1.8655e-01,\n",
            "          3.5698e-02, -7.4861e-02, -1.7534e-02, -2.4207e-03,  2.2243e-01,\n",
            "          5.3036e-02, -1.0047e-01,  9.1334e-02,  1.1790e-01,  5.2742e-02,\n",
            "         -1.2201e-01,  6.1377e-02,  8.7714e-02,  2.0077e-02, -4.2411e-03,\n",
            "         -6.5421e-02,  1.8531e-02,  8.4937e-02, -1.1914e-01, -8.7909e-02,\n",
            "          7.7397e-02,  4.9868e-02, -9.5203e-02, -6.0212e-02, -4.3796e-02,\n",
            "         -9.7685e-03,  8.4496e-02, -2.8881e-02,  9.8502e-02, -1.5556e-01,\n",
            "         -1.6190e-02, -8.7906e-02, -6.4810e-02]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[[[-0.1006,  0.0870, -0.1242],\n",
            "          [ 0.1866, -0.0918, -0.1614],\n",
            "          [ 0.0554,  0.0360, -0.0403]],\n",
            "\n",
            "         [[ 0.1506, -0.1453,  0.0865],\n",
            "          [-0.0775,  0.1634,  0.1531],\n",
            "          [-0.1470, -0.1122, -0.0764]],\n",
            "\n",
            "         [[ 0.0284,  0.2263,  0.1402],\n",
            "          [-0.0222, -0.0239, -0.0496],\n",
            "          [-0.0999, -0.0072, -0.0473]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1501,  0.0830,  0.1170],\n",
            "          [-0.0409,  0.2528,  0.0848],\n",
            "          [ 0.0737,  0.0943,  0.1785]],\n",
            "\n",
            "         [[-0.0923,  0.1472, -0.0037],\n",
            "          [ 0.0707, -0.0134, -0.0323],\n",
            "          [-0.0029,  0.0825, -0.1458]],\n",
            "\n",
            "         [[-0.0043,  0.0517,  0.0912],\n",
            "          [ 0.0776,  0.0310,  0.0370],\n",
            "          [ 0.0690, -0.0893,  0.0729]]],\n",
            "\n",
            "\n",
            "        [[[-0.0600,  0.0115,  0.0342],\n",
            "          [-0.0963,  0.1543, -0.0445],\n",
            "          [-0.1098,  0.0542, -0.0286]],\n",
            "\n",
            "         [[ 0.1117, -0.1316, -0.0656],\n",
            "          [-0.1162,  0.1665,  0.0527],\n",
            "          [-0.0811,  0.0481, -0.0059]],\n",
            "\n",
            "         [[-0.0191, -0.1321, -0.0264],\n",
            "          [-0.0225, -0.1604,  0.1591],\n",
            "          [-0.0056, -0.0229, -0.0861]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0613,  0.0448, -0.0155],\n",
            "          [ 0.0362, -0.0444,  0.0374],\n",
            "          [-0.0753, -0.1668,  0.0996]],\n",
            "\n",
            "         [[-0.0474, -0.0290,  0.1478],\n",
            "          [-0.0961,  0.0300, -0.0589],\n",
            "          [ 0.1108, -0.0174, -0.0398]],\n",
            "\n",
            "         [[ 0.1193, -0.0968, -0.0168],\n",
            "          [ 0.0110, -0.0063, -0.1566],\n",
            "          [-0.1610, -0.1470,  0.1318]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0033,  0.1961, -0.0137],\n",
            "          [-0.0061,  0.0068, -0.0566],\n",
            "          [ 0.1012, -0.0974,  0.0297]],\n",
            "\n",
            "         [[-0.0766, -0.0445, -0.0191],\n",
            "          [-0.0433, -0.1352, -0.1109],\n",
            "          [ 0.0395,  0.1749, -0.0190]],\n",
            "\n",
            "         [[-0.1066,  0.0781,  0.1383],\n",
            "          [ 0.1411,  0.0915,  0.0290],\n",
            "          [ 0.1591,  0.0881, -0.1018]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0451, -0.0263,  0.0968],\n",
            "          [ 0.1119,  0.0334, -0.0476],\n",
            "          [ 0.1040, -0.0919, -0.0420]],\n",
            "\n",
            "         [[-0.1660, -0.0533,  0.0877],\n",
            "          [ 0.0276,  0.0591, -0.0424],\n",
            "          [-0.0637,  0.1116,  0.1094]],\n",
            "\n",
            "         [[-0.0292,  0.0399,  0.0497],\n",
            "          [ 0.0153, -0.0958, -0.0240],\n",
            "          [ 0.1130,  0.1238,  0.0689]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.1201, -0.0177, -0.1913],\n",
            "          [-0.1202, -0.0174,  0.0103],\n",
            "          [-0.1078,  0.0110,  0.0750]],\n",
            "\n",
            "         [[ 0.1186,  0.0014,  0.1050],\n",
            "          [ 0.0504, -0.0962, -0.0438],\n",
            "          [ 0.0237, -0.1602,  0.0211]],\n",
            "\n",
            "         [[ 0.0541,  0.0326, -0.0131],\n",
            "          [-0.0231,  0.0086, -0.0610],\n",
            "          [-0.1436, -0.0131,  0.1210]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0324, -0.0530,  0.0834],\n",
            "          [-0.1313, -0.0749,  0.0605],\n",
            "          [-0.0518,  0.0616,  0.0546]],\n",
            "\n",
            "         [[-0.0882, -0.0118,  0.1274],\n",
            "          [-0.0181, -0.0570, -0.0797],\n",
            "          [-0.1141,  0.1343, -0.1568]],\n",
            "\n",
            "         [[ 0.0020,  0.0188,  0.1470],\n",
            "          [ 0.0223, -0.1452,  0.1009],\n",
            "          [ 0.0713,  0.0545,  0.3064]]],\n",
            "\n",
            "\n",
            "        [[[-0.0991, -0.1089, -0.0380],\n",
            "          [-0.0972, -0.0589,  0.0940],\n",
            "          [-0.0262,  0.0071,  0.0726]],\n",
            "\n",
            "         [[ 0.1989, -0.0188, -0.1112],\n",
            "          [-0.0585, -0.0094,  0.1120],\n",
            "          [-0.1124, -0.0351, -0.0068]],\n",
            "\n",
            "         [[ 0.0243, -0.1181, -0.1421],\n",
            "          [ 0.1587, -0.0766,  0.0431],\n",
            "          [ 0.0316, -0.1887, -0.0332]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0246,  0.2174,  0.0179],\n",
            "          [ 0.0483, -0.0234, -0.0011],\n",
            "          [-0.1039, -0.1177,  0.0064]],\n",
            "\n",
            "         [[ 0.1725, -0.1088, -0.1049],\n",
            "          [ 0.1076,  0.0294, -0.0916],\n",
            "          [ 0.1317, -0.0328, -0.0154]],\n",
            "\n",
            "         [[ 0.0341,  0.0632,  0.0465],\n",
            "          [ 0.1320,  0.0847,  0.0131],\n",
            "          [ 0.0153,  0.1219,  0.1080]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0457, -0.0894,  0.0311],\n",
            "          [ 0.1014, -0.0057, -0.0773],\n",
            "          [ 0.1024, -0.0836,  0.0896]],\n",
            "\n",
            "         [[-0.0294,  0.0401,  0.0009],\n",
            "          [ 0.0472, -0.0531,  0.1743],\n",
            "          [ 0.0242,  0.1035, -0.0390]],\n",
            "\n",
            "         [[ 0.2373, -0.0053,  0.0467],\n",
            "          [-0.1696, -0.0515,  0.0823],\n",
            "          [ 0.0379, -0.0948, -0.0224]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0088, -0.0874,  0.1337],\n",
            "          [-0.0533, -0.1417, -0.1944],\n",
            "          [ 0.0869,  0.0330, -0.2278]],\n",
            "\n",
            "         [[ 0.0284, -0.1562,  0.1651],\n",
            "          [ 0.0572,  0.0349, -0.0300],\n",
            "          [ 0.0147, -0.1781, -0.0860]],\n",
            "\n",
            "         [[ 0.1379,  0.0616, -0.0944],\n",
            "          [ 0.0024, -0.0241,  0.0253],\n",
            "          [-0.0188, -0.0990,  0.1556]]]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.1709, -0.1132, -0.1258,  0.0394, -0.0004, -0.0501,  0.0603, -0.0890,\n",
            "         0.0814,  0.0656, -0.0830, -0.0328, -0.1131, -0.0462, -0.0244,  0.1048,\n",
            "         0.0418, -0.0435,  0.0468,  0.0402, -0.0374,  0.0421,  0.1161, -0.0230,\n",
            "        -0.0187,  0.1099,  0.1576,  0.0957,  0.0331,  0.0708,  0.0436, -0.1325,\n",
            "        -0.2541,  0.0776,  0.0646, -0.0565, -0.0325,  0.1837, -0.0953,  0.1936,\n",
            "         0.0791, -0.0628, -0.0592,  0.1693, -0.0212, -0.0536, -0.0520,  0.0441],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 3.0304e-02,  2.9118e-03,  1.0751e-02,  7.5644e-03, -1.8848e-01,\n",
            "          1.0196e-01, -6.8463e-02,  3.6689e-03, -1.0691e-01,  6.2915e-02,\n",
            "          3.3592e-02,  3.2944e-03, -7.0095e-02,  6.6980e-02,  7.5333e-02,\n",
            "          1.4490e-01,  7.8801e-02,  1.5452e-01,  1.2693e-02,  1.6641e-01,\n",
            "          1.1632e-01,  9.5886e-02,  5.3658e-03, -1.9156e-01, -1.7923e-02,\n",
            "         -2.1751e-01, -9.6325e-03,  9.2521e-03,  4.9209e-03, -1.0099e-01,\n",
            "         -7.1668e-02,  1.1149e-01,  1.8170e-01, -3.3223e-02, -8.0576e-02,\n",
            "         -9.3900e-02, -7.2689e-02, -1.4858e-01, -9.0430e-02,  9.1038e-02,\n",
            "          2.7095e-02,  5.6517e-02,  1.4079e-01, -1.3102e-01,  1.6406e-01,\n",
            "          9.8383e-02, -2.3490e-02,  2.0435e-01],\n",
            "        [ 1.0166e-01, -4.1933e-02,  1.0434e-01, -5.9291e-02, -2.9863e-02,\n",
            "         -8.0517e-02,  1.3675e-01, -6.0932e-02,  8.5338e-02, -7.3927e-02,\n",
            "         -7.8971e-03, -2.1636e-02,  3.8541e-02,  2.5544e-02, -6.2273e-02,\n",
            "         -1.0913e-01, -4.9334e-02, -7.9423e-02, -1.0288e-01, -1.0777e-02,\n",
            "          1.0470e-01,  4.2127e-03,  6.5743e-02, -5.4776e-02,  4.7957e-02,\n",
            "          2.0724e-01,  3.9398e-02,  4.0040e-02, -1.5870e-01,  1.8502e-01,\n",
            "         -1.2568e-02,  2.0840e-01, -1.2834e-01,  1.8038e-01,  8.8422e-03,\n",
            "         -7.7405e-03,  6.4027e-02, -4.1990e-02, -5.1127e-03, -3.0449e-02,\n",
            "          4.9720e-02,  4.9633e-02,  2.5308e-02, -9.8387e-02,  3.5035e-02,\n",
            "          6.0074e-02,  1.6565e-01,  1.7917e-02],\n",
            "        [-8.9137e-03, -7.2711e-02, -2.2894e-01, -5.5245e-02,  2.2160e-01,\n",
            "         -1.6838e-01,  4.3594e-02, -2.6534e-01, -4.1228e-02,  4.1105e-02,\n",
            "         -5.0905e-02,  1.6281e-01, -4.1351e-02, -2.1010e-01, -7.9109e-02,\n",
            "          7.1205e-02,  1.8865e-02,  6.3825e-02, -1.3224e-01, -7.6874e-02,\n",
            "          8.2245e-02,  5.2595e-02, -1.5097e-01,  6.7883e-02,  7.0615e-02,\n",
            "         -7.3829e-02,  1.7626e-02,  1.3823e-01, -1.0425e-01, -2.9474e-02,\n",
            "          2.0366e-01, -7.2091e-02, -3.2825e-02,  1.1886e-01, -6.7654e-02,\n",
            "          4.8415e-02,  1.9917e-02,  2.0153e-02, -2.2627e-02,  2.1064e-02,\n",
            "         -9.4899e-02,  5.8763e-02, -1.4063e-01,  1.0419e-01, -9.4999e-02,\n",
            "          2.0559e-01, -6.3086e-02, -1.5336e-02],\n",
            "        [-5.2378e-02,  1.0423e-01, -2.0079e-01,  1.4087e-02, -3.8244e-02,\n",
            "          1.4390e-01,  5.8587e-02, -1.3478e-01,  9.0397e-02, -2.8107e-02,\n",
            "          7.1619e-02,  6.2770e-02, -8.0400e-03,  1.2006e-01,  2.4948e-02,\n",
            "          6.2235e-02,  1.1729e-02,  1.6115e-01, -1.3156e-01,  3.8489e-02,\n",
            "          6.6062e-03, -4.7262e-03,  4.4516e-02,  3.0894e-02,  7.9363e-02,\n",
            "          8.9242e-02,  1.5102e-02, -1.7523e-02, -9.3804e-02, -7.1092e-03,\n",
            "          1.4000e-01, -9.6914e-02,  5.0309e-02, -1.2269e-02,  2.1009e-01,\n",
            "         -5.6265e-02, -2.4630e-01,  4.4222e-03, -3.6669e-02, -1.5491e-02,\n",
            "          5.2435e-02,  3.3603e-02,  7.2721e-02, -9.3002e-03, -5.4442e-02,\n",
            "         -9.9285e-02, -2.9680e-02, -9.2632e-02],\n",
            "        [-7.2641e-03, -6.0896e-02, -7.0025e-02, -2.7330e-02,  1.8357e-01,\n",
            "          1.8684e-02, -6.7809e-04,  9.9163e-02,  2.1819e-02, -8.1835e-02,\n",
            "         -4.1554e-02, -1.3128e-01,  1.3240e-01, -5.3613e-02, -2.7411e-02,\n",
            "         -8.7063e-02, -3.5259e-03, -1.1493e-02, -6.4067e-02,  1.1833e-01,\n",
            "         -1.0457e-02,  5.1661e-02, -1.9633e-01,  6.6882e-02, -1.2606e-01,\n",
            "          6.1715e-02,  2.3821e-01, -5.5469e-02, -5.7141e-02, -6.9782e-02,\n",
            "         -1.3675e-04, -7.1760e-03, -4.7530e-02,  2.8778e-02,  1.6538e-01,\n",
            "          1.7685e-01, -2.0683e-02,  3.4081e-02,  4.7245e-02, -4.8962e-02,\n",
            "          8.8744e-02, -8.7724e-02, -2.6202e-02, -1.7547e-01, -1.5802e-02,\n",
            "          9.0995e-02, -9.0772e-02,  1.8841e-02]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 0.0499,  0.0192,  0.2279,  0.0865,  0.0618, -0.1291,  0.0661,  0.0989,\n",
            "          0.0929,  0.0660,  0.0205, -0.1368,  0.1953,  0.0235,  0.0496,  0.0131,\n",
            "          0.0886,  0.0094,  0.0324, -0.0110,  0.0696,  0.0625,  0.0701, -0.1681,\n",
            "         -0.0317, -0.0403,  0.0340, -0.1299,  0.0518, -0.0470,  0.0616,  0.1248,\n",
            "         -0.0292, -0.1126,  0.0368, -0.1379, -0.1198, -0.1022,  0.0164, -0.0634,\n",
            "          0.0743, -0.0232,  0.0400, -0.1291, -0.0256,  0.0041, -0.0449,  0.0715],\n",
            "        [-0.0778, -0.0068,  0.0601,  0.1044,  0.0083,  0.0264,  0.0769, -0.0075,\n",
            "         -0.0255, -0.1392, -0.2298,  0.0139,  0.1621, -0.0737, -0.0171,  0.1555,\n",
            "          0.0477, -0.0510,  0.0290, -0.0125,  0.0728, -0.1146, -0.0693,  0.0058,\n",
            "          0.1505,  0.0633,  0.0934, -0.0668, -0.0263,  0.0170, -0.1128, -0.1472,\n",
            "         -0.1296,  0.0217, -0.0354, -0.1071, -0.0510, -0.0226, -0.0047,  0.1358,\n",
            "          0.0723, -0.0113,  0.0780,  0.1284,  0.1205, -0.1387,  0.0248,  0.0481],\n",
            "        [ 0.0369, -0.0669, -0.1135, -0.0709,  0.0169, -0.1846,  0.0506,  0.2378,\n",
            "         -0.0613, -0.0677, -0.0519,  0.1105, -0.1259, -0.0086, -0.0619, -0.1186,\n",
            "          0.2021, -0.0054, -0.0869,  0.0811,  0.0566,  0.0132,  0.1339,  0.1152,\n",
            "         -0.0715,  0.0855, -0.0030,  0.0522,  0.0372,  0.1000,  0.0735, -0.1576,\n",
            "          0.1450,  0.2866, -0.2321, -0.0158,  0.0571, -0.0182, -0.0730, -0.3024,\n",
            "         -0.0805,  0.0460,  0.0292, -0.1609, -0.0742, -0.0619,  0.0850,  0.0391],\n",
            "        [-0.0813, -0.0916, -0.0522,  0.0087,  0.0182, -0.0430,  0.0868, -0.0506,\n",
            "         -0.1307, -0.0951,  0.1123, -0.0560,  0.0034, -0.0551, -0.0072,  0.1550,\n",
            "          0.0511,  0.0530,  0.1050, -0.0527,  0.0892, -0.0900, -0.0012,  0.0636,\n",
            "          0.0866, -0.1256,  0.0941, -0.0126,  0.0597, -0.0329,  0.0466,  0.2216,\n",
            "         -0.0341,  0.1254, -0.0473,  0.0629, -0.0131, -0.0877,  0.0004, -0.0577,\n",
            "          0.1567,  0.2136, -0.0009,  0.0845, -0.0671,  0.0944,  0.0893,  0.1920],\n",
            "        [ 0.0731, -0.0004,  0.0708,  0.0028, -0.0627, -0.0588,  0.2633,  0.0974,\n",
            "         -0.0754, -0.0310, -0.0890,  0.0851, -0.0563,  0.0208, -0.1579, -0.0582,\n",
            "          0.0844,  0.0029,  0.1043, -0.1068, -0.1304, -0.2555, -0.0341, -0.1484,\n",
            "          0.0281,  0.0212, -0.0054, -0.0417,  0.0057,  0.1324,  0.0227, -0.0371,\n",
            "         -0.0535,  0.0926,  0.0224,  0.0878,  0.0275,  0.0722, -0.0458,  0.0827,\n",
            "          0.0068,  0.1608,  0.1362,  0.0786, -0.0329, -0.0098,  0.0248, -0.1674]],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[[[ 1.2680e-01,  1.3223e-02,  8.8561e-02],\n",
            "          [-2.1536e-01,  6.7797e-02,  5.5777e-02],\n",
            "          [ 1.3994e-01,  2.3411e-02,  8.7029e-02]],\n",
            "\n",
            "         [[-9.2546e-03,  9.1180e-02,  8.2252e-02],\n",
            "          [ 7.4659e-02,  1.9647e-02, -6.5078e-03],\n",
            "          [-1.9257e-04,  3.6556e-02, -3.6322e-02]],\n",
            "\n",
            "         [[ 1.7653e-01, -6.0661e-03, -4.7117e-02],\n",
            "          [ 5.5460e-02, -2.1261e-02,  8.0499e-02],\n",
            "          [ 1.3030e-01,  1.9077e-02, -1.6772e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3996e-01,  2.1506e-02,  9.8002e-02],\n",
            "          [ 2.2670e-02,  6.8826e-02,  4.6957e-02],\n",
            "          [-3.6888e-02,  8.4736e-02, -4.4553e-02]],\n",
            "\n",
            "         [[-3.4345e-02,  1.0671e-01, -1.2275e-01],\n",
            "          [-3.0260e-02, -2.6032e-02,  5.8852e-02],\n",
            "          [-1.0190e-01, -1.4729e-03,  3.3139e-02]],\n",
            "\n",
            "         [[ 1.4435e-01,  9.1734e-02, -8.8600e-02],\n",
            "          [-6.3056e-02, -9.5320e-02,  7.3391e-03],\n",
            "          [-4.0904e-02, -2.8177e-02,  1.4864e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.3400e-01, -1.9235e-02, -3.7287e-02],\n",
            "          [-8.6928e-02, -8.8830e-02,  6.6225e-03],\n",
            "          [ 5.3502e-02,  2.5617e-01, -8.5332e-02]],\n",
            "\n",
            "         [[ 3.4987e-02,  3.9076e-02,  9.8279e-03],\n",
            "          [-1.4601e-02,  2.3216e-01, -2.9655e-03],\n",
            "          [ 1.3515e-01,  8.1678e-02, -3.1649e-01]],\n",
            "\n",
            "         [[-2.7513e-02,  8.2682e-02,  3.3391e-02],\n",
            "          [-1.2451e-01, -4.8196e-02, -3.8240e-02],\n",
            "          [-7.2195e-02, -2.2267e-02, -1.0298e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.1279e-02,  1.5803e-01, -2.5163e-02],\n",
            "          [-6.9948e-02,  2.0949e-02, -8.0033e-02],\n",
            "          [-9.2420e-02,  2.4858e-01,  1.8293e-03]],\n",
            "\n",
            "         [[-7.4579e-03, -2.1691e-02,  2.6142e-02],\n",
            "          [-2.5694e-02, -7.5123e-02,  8.0234e-02],\n",
            "          [ 1.1311e-01, -5.4982e-02, -4.1965e-03]],\n",
            "\n",
            "         [[ 1.6003e-01,  7.0639e-02, -1.0003e-01],\n",
            "          [-1.6501e-01, -2.5800e-02,  6.8854e-02],\n",
            "          [-8.6452e-02, -5.0412e-02,  2.1488e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 6.2219e-02, -2.6961e-02, -3.5140e-02],\n",
            "          [-1.0562e-01,  7.6773e-02,  1.1013e-01],\n",
            "          [-6.1763e-02, -1.1708e-02, -8.4743e-02]],\n",
            "\n",
            "         [[-6.2801e-02,  1.2294e-01,  1.4304e-01],\n",
            "          [-5.3206e-02,  2.4705e-01, -3.0761e-02],\n",
            "          [-4.2897e-02, -2.2483e-01, -2.2175e-01]],\n",
            "\n",
            "         [[-1.0123e-02, -1.4404e-01, -8.1671e-02],\n",
            "          [-9.5452e-02, -1.4868e-01, -5.0169e-02],\n",
            "          [ 5.6700e-02,  1.2675e-02,  4.9469e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.7277e-03,  1.0687e-01, -4.5915e-02],\n",
            "          [-1.1268e-01, -2.7491e-02,  6.7164e-02],\n",
            "          [-5.9806e-02, -6.9534e-03, -1.0253e-01]],\n",
            "\n",
            "         [[ 1.9555e-02, -1.0032e-01,  4.6653e-02],\n",
            "          [-6.0243e-02,  1.2843e-01, -7.9533e-02],\n",
            "          [-1.1608e-01,  1.4869e-01, -2.1684e-01]],\n",
            "\n",
            "         [[-5.5787e-02, -1.2425e-03, -5.0765e-02],\n",
            "          [-1.2535e-01, -4.3728e-02, -1.7199e-01],\n",
            "          [-1.8697e-01, -3.1625e-02, -1.0510e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-9.0231e-02,  2.1081e-02, -3.4888e-02],\n",
            "          [-2.7246e-02,  4.6052e-02, -3.5270e-02],\n",
            "          [-1.7107e-02, -9.0336e-02, -6.9928e-02]],\n",
            "\n",
            "         [[-1.7405e-01,  8.4046e-02,  1.5255e-01],\n",
            "          [ 3.5560e-02, -8.1549e-02,  7.8360e-02],\n",
            "          [-1.1025e-01, -2.1358e-02, -3.9513e-02]],\n",
            "\n",
            "         [[ 2.9703e-02,  2.9374e-02, -1.1331e-01],\n",
            "          [-1.0079e-01,  5.3851e-02,  5.8087e-03],\n",
            "          [-5.4410e-02, -1.5190e-01,  4.2282e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4464e-02,  1.4859e-01,  1.0200e-01],\n",
            "          [-6.1974e-02, -3.5108e-02,  7.0570e-02],\n",
            "          [ 2.2906e-02,  2.3773e-02,  6.9560e-02]],\n",
            "\n",
            "         [[ 6.3261e-03, -1.3013e-01,  3.3543e-02],\n",
            "          [ 2.0437e-03,  1.1527e-01,  5.1171e-02],\n",
            "          [-2.3678e-01,  6.9224e-03,  1.9430e-01]],\n",
            "\n",
            "         [[ 1.8835e-01, -6.7815e-02, -7.6381e-02],\n",
            "          [ 1.5742e-01, -5.9078e-02,  6.1036e-02],\n",
            "          [ 6.8553e-02, -1.9374e-01,  1.2907e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.9045e-03, -6.5343e-02,  2.6158e-02],\n",
            "          [-7.6925e-02, -2.4573e-01,  2.6494e-02],\n",
            "          [ 1.2678e-01, -3.2027e-02,  1.1993e-01]],\n",
            "\n",
            "         [[-2.3262e-02,  1.4658e-01,  1.8705e-01],\n",
            "          [ 2.5329e-01,  3.9422e-02, -3.8778e-02],\n",
            "          [ 5.5537e-02,  1.5376e-01,  7.1061e-02]],\n",
            "\n",
            "         [[ 2.3464e-02,  3.5644e-02, -6.3899e-03],\n",
            "          [ 4.1659e-02, -2.9966e-03, -2.9616e-02],\n",
            "          [-6.0168e-02,  5.5254e-02, -6.1876e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7613e-02,  1.1428e-01, -1.0500e-01],\n",
            "          [-1.7408e-01,  1.2595e-02,  4.5825e-02],\n",
            "          [-1.6033e-01,  6.5982e-02,  1.1224e-01]],\n",
            "\n",
            "         [[ 3.4305e-02,  9.2257e-02, -1.9299e-01],\n",
            "          [ 1.6536e-02,  9.2517e-02, -2.7532e-02],\n",
            "          [-8.9014e-03,  9.9377e-03, -3.3046e-02]],\n",
            "\n",
            "         [[-1.2441e-01, -6.7910e-02,  6.8019e-02],\n",
            "          [-2.8445e-02, -1.7901e-02, -2.0560e-02],\n",
            "          [-5.0690e-02,  3.5882e-04, -5.4575e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.7072e-01,  3.9110e-02, -2.6472e-01],\n",
            "          [ 7.9992e-02, -3.3620e-02, -1.2856e-01],\n",
            "          [ 5.0510e-02, -3.5743e-02,  7.2582e-02]],\n",
            "\n",
            "         [[ 2.0571e-01,  1.5257e-01, -1.8326e-01],\n",
            "          [-9.0409e-02, -8.3831e-02, -3.4504e-02],\n",
            "          [-1.0050e-01,  4.7460e-02, -2.3634e-01]],\n",
            "\n",
            "         [[-4.4743e-02,  1.1331e-01,  1.2512e-01],\n",
            "          [-4.0758e-02, -7.7277e-02,  1.5724e-01],\n",
            "          [ 1.1236e-01, -5.4127e-02, -9.7320e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0249e-01, -1.6158e-01,  3.8365e-02],\n",
            "          [-3.2596e-02, -7.2566e-02, -3.8101e-02],\n",
            "          [ 1.5203e-01,  2.4359e-01,  1.0217e-01]],\n",
            "\n",
            "         [[ 8.0655e-02, -1.4672e-01, -7.8237e-02],\n",
            "          [-1.6051e-01, -6.9505e-02,  1.7641e-02],\n",
            "          [ 9.8161e-02,  1.2194e-01,  6.2030e-02]],\n",
            "\n",
            "         [[-1.2889e-02,  8.3097e-02, -2.3348e-02],\n",
            "          [ 2.2877e-02, -9.5211e-02,  4.6850e-02],\n",
            "          [ 1.3582e-01,  5.3995e-02, -1.7156e-01]]]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0103, -0.1613,  0.0441,  0.0428, -0.1097,  0.0955,  0.1899, -0.0504,\n",
            "        -0.0268,  0.1231,  0.0871, -0.0877, -0.0885,  0.2493, -0.0884, -0.0093,\n",
            "        -0.1115,  0.0378,  0.0945, -0.0730, -0.0210, -0.0667, -0.0195, -0.1033,\n",
            "        -0.0135,  0.0339, -0.0878,  0.0352, -0.0148, -0.0306, -0.0315, -0.0562,\n",
            "        -0.0127,  0.0759,  0.1058, -0.0418, -0.0708, -0.0146,  0.0090, -0.0950,\n",
            "        -0.0158,  0.1045,  0.0741,  0.1511,  0.0021, -0.2176, -0.0549, -0.1316],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[-0.0230,  0.1934, -0.1487,  0.1554,  0.0940, -0.0473, -0.0100,  0.1185,\n",
            "          0.0692, -0.0216,  0.1110, -0.0351,  0.0941, -0.0930,  0.1598,  0.0216,\n",
            "         -0.1007, -0.0206, -0.0029,  0.0272,  0.0509,  0.0232,  0.0711,  0.0781,\n",
            "          0.0643, -0.0032,  0.0411, -0.0526,  0.1942, -0.0936, -0.0346, -0.0514,\n",
            "          0.0938,  0.1359, -0.0860,  0.0112, -0.0535, -0.1592,  0.0529,  0.2107,\n",
            "         -0.1725, -0.0736, -0.0427,  0.0352, -0.0047, -0.0302,  0.1365,  0.0142],\n",
            "        [-0.0095, -0.0506,  0.0311,  0.2120, -0.0180, -0.2091, -0.0994,  0.0803,\n",
            "          0.1389,  0.0293, -0.0095, -0.1781,  0.0533,  0.0772,  0.0229,  0.0772,\n",
            "         -0.0698,  0.0815,  0.0937,  0.0881, -0.1765, -0.0431, -0.1082,  0.0800,\n",
            "          0.0981, -0.1668, -0.0048,  0.0637,  0.0230, -0.0137,  0.1131, -0.1009,\n",
            "          0.0022,  0.1256, -0.0495, -0.0613, -0.0046,  0.0870,  0.1821,  0.0638,\n",
            "          0.0747, -0.0955, -0.0676, -0.1320,  0.0467, -0.0365,  0.1519, -0.0178],\n",
            "        [ 0.1194,  0.0712, -0.1275,  0.0322,  0.1541, -0.0707,  0.0190, -0.0398,\n",
            "         -0.0437,  0.0171,  0.0219, -0.0305, -0.1365, -0.1556, -0.0695, -0.1058,\n",
            "          0.0686, -0.0464, -0.0433,  0.0086, -0.0463, -0.0653,  0.0681, -0.0086,\n",
            "          0.0398, -0.2465, -0.0497,  0.0714, -0.1103,  0.1268, -0.0288, -0.1033,\n",
            "         -0.1633, -0.1368, -0.0193,  0.0321,  0.0926, -0.1885,  0.0284, -0.0466,\n",
            "          0.1811,  0.0373, -0.0807,  0.0810,  0.0228, -0.0754, -0.0442, -0.0030],\n",
            "        [-0.0289,  0.0376,  0.0573,  0.0468,  0.1615, -0.1145, -0.0514, -0.0035,\n",
            "          0.0206, -0.0836, -0.0854,  0.0753,  0.0147,  0.1151,  0.0310,  0.0102,\n",
            "         -0.0562,  0.0975,  0.0719, -0.0813, -0.0091,  0.1807, -0.1496, -0.0339,\n",
            "         -0.1485, -0.0299,  0.0724,  0.0117, -0.1994,  0.0670, -0.0778,  0.0791,\n",
            "          0.1511,  0.2079,  0.2107,  0.0197, -0.0614, -0.0174,  0.0718,  0.1780,\n",
            "          0.0199, -0.0190,  0.0297, -0.0576, -0.0026, -0.1476, -0.0068, -0.1317],\n",
            "        [-0.1850,  0.0040,  0.2379,  0.0064, -0.0676,  0.0371, -0.0420, -0.1056,\n",
            "         -0.1885,  0.1130, -0.1101,  0.2387,  0.0415, -0.0044, -0.0021, -0.0552,\n",
            "         -0.0786, -0.0883,  0.0148,  0.0493, -0.0024,  0.0770,  0.0765, -0.0861,\n",
            "          0.1294, -0.0303, -0.0047,  0.0380,  0.2149,  0.1409, -0.0089,  0.0030,\n",
            "         -0.0427, -0.0408,  0.1342, -0.0180, -0.2847, -0.0416,  0.0751,  0.1481,\n",
            "         -0.0319, -0.0686,  0.0125, -0.0342, -0.1214,  0.0445, -0.0087, -0.1246]],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 0.1262,  0.0304,  0.0847,  0.0487,  0.1089, -0.0482, -0.0478,  0.0658,\n",
            "          0.1168, -0.1154, -0.0926,  0.0681,  0.1328,  0.0662,  0.1324,  0.0302,\n",
            "         -0.0490,  0.0259,  0.1407, -0.2921, -0.0190,  0.0252,  0.1085, -0.0987,\n",
            "         -0.0196, -0.0569,  0.0790, -0.0184, -0.0105, -0.1064,  0.1404, -0.0163,\n",
            "         -0.2242, -0.1018, -0.1693,  0.0352, -0.0470,  0.0344, -0.2054, -0.0899,\n",
            "          0.0798,  0.0414,  0.0377,  0.0460, -0.0903, -0.1596, -0.0254,  0.0278],\n",
            "        [-0.0935,  0.1488, -0.0033, -0.1064,  0.0224,  0.2250, -0.0210, -0.2139,\n",
            "         -0.0851, -0.0771,  0.0394,  0.0455, -0.0950,  0.2309, -0.0475, -0.0529,\n",
            "         -0.0744, -0.0501,  0.0684, -0.0779,  0.1046, -0.1086,  0.0074,  0.0012,\n",
            "          0.0719, -0.0024,  0.0364, -0.1431, -0.2571,  0.0282,  0.0794, -0.0784,\n",
            "          0.1469, -0.1786,  0.0379, -0.0091, -0.1226,  0.0708, -0.0078,  0.0112,\n",
            "          0.0838,  0.0385,  0.0061,  0.0186, -0.0196, -0.1638, -0.1466, -0.0456],\n",
            "        [ 0.0482, -0.0542, -0.1046,  0.0099, -0.0838,  0.0034,  0.0908, -0.0856,\n",
            "         -0.0371, -0.1422,  0.0824,  0.0779, -0.1049, -0.2439,  0.0600, -0.1187,\n",
            "          0.1599, -0.0187,  0.1161,  0.0733,  0.0790,  0.2267,  0.1159,  0.0591,\n",
            "         -0.0093,  0.0972, -0.0803, -0.0780,  0.0450,  0.1240,  0.0071,  0.0426,\n",
            "          0.1431,  0.1197, -0.0560, -0.0727,  0.0555,  0.1792,  0.0286,  0.0575,\n",
            "         -0.0813, -0.0106,  0.1370,  0.0604, -0.2004, -0.0809, -0.0816, -0.0127],\n",
            "        [ 0.0452, -0.0552,  0.0228,  0.0192,  0.0217, -0.0533, -0.0038,  0.1293,\n",
            "          0.1312,  0.1027, -0.0551, -0.0277,  0.1583,  0.2025,  0.1811,  0.0754,\n",
            "          0.0053, -0.0893, -0.0928, -0.0271, -0.0383,  0.0673,  0.1008, -0.0430,\n",
            "          0.2122, -0.0830, -0.0978,  0.0051, -0.0652,  0.0150, -0.0735, -0.0296,\n",
            "         -0.0425, -0.0372,  0.0049,  0.0410, -0.0318, -0.1230, -0.0130,  0.0339,\n",
            "          0.0122, -0.0614,  0.0756,  0.0108,  0.0917, -0.1263,  0.0278, -0.0332],\n",
            "        [-0.0640,  0.0719,  0.0886,  0.0107,  0.0702, -0.0130, -0.0080,  0.0077,\n",
            "          0.1312,  0.0050, -0.0655,  0.0216,  0.0809,  0.0376,  0.0330, -0.0230,\n",
            "         -0.1030, -0.0174,  0.0652, -0.0086,  0.0301, -0.0492, -0.0077,  0.1346,\n",
            "         -0.1611,  0.1274,  0.0179, -0.1416,  0.0642, -0.0582, -0.1668, -0.0159,\n",
            "         -0.0767, -0.0078,  0.0050, -0.0930, -0.0966, -0.0409, -0.0085,  0.0309,\n",
            "          0.0169, -0.1657, -0.1841, -0.1152, -0.1042,  0.0482,  0.0520,  0.1099]],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[[[-0.0242, -0.2257,  0.1709],\n",
            "          [ 0.0255, -0.0715,  0.0606],\n",
            "          [ 0.0470,  0.2381,  0.0596]],\n",
            "\n",
            "         [[-0.0326,  0.0027, -0.0874],\n",
            "          [-0.0421, -0.0159,  0.1292],\n",
            "          [ 0.0261,  0.0077,  0.1721]],\n",
            "\n",
            "         [[-0.0413, -0.0386, -0.0608],\n",
            "          [ 0.1616,  0.0262, -0.0716],\n",
            "          [ 0.0927, -0.0637, -0.1346]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0551,  0.0733, -0.0680],\n",
            "          [ 0.0288,  0.0427, -0.0267],\n",
            "          [ 0.1611, -0.0827,  0.1179]],\n",
            "\n",
            "         [[-0.0792, -0.0398, -0.2411],\n",
            "          [ 0.0581, -0.0472,  0.0297],\n",
            "          [-0.0637,  0.2420, -0.0058]],\n",
            "\n",
            "         [[ 0.0190,  0.0083,  0.0284],\n",
            "          [ 0.2018,  0.0912, -0.1129],\n",
            "          [-0.0520,  0.0157,  0.0470]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0899, -0.1068,  0.0573],\n",
            "          [-0.0166, -0.0798,  0.0448],\n",
            "          [-0.0764,  0.0483, -0.0818]],\n",
            "\n",
            "         [[-0.2106, -0.0536, -0.2385],\n",
            "          [-0.1799,  0.1009, -0.0612],\n",
            "          [ 0.1859,  0.1765, -0.0351]],\n",
            "\n",
            "         [[-0.0865,  0.0531, -0.1318],\n",
            "          [-0.2120, -0.0092,  0.0480],\n",
            "          [-0.0814, -0.1429,  0.1656]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0872, -0.0110,  0.0088],\n",
            "          [-0.1979, -0.2702,  0.0499],\n",
            "          [ 0.0137,  0.1954,  0.1250]],\n",
            "\n",
            "         [[ 0.0563, -0.0141, -0.1287],\n",
            "          [-0.0805,  0.2454, -0.1343],\n",
            "          [ 0.0757,  0.0918,  0.1117]],\n",
            "\n",
            "         [[ 0.1792,  0.0258,  0.0259],\n",
            "          [-0.1285, -0.0330,  0.0388],\n",
            "          [-0.0053, -0.0622, -0.0776]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1048, -0.0888,  0.1474],\n",
            "          [ 0.0842, -0.1369, -0.1145],\n",
            "          [ 0.2591,  0.0616, -0.0213]],\n",
            "\n",
            "         [[-0.0267, -0.0866,  0.0254],\n",
            "          [ 0.0658,  0.0909, -0.1070],\n",
            "          [ 0.1216,  0.0878, -0.0590]],\n",
            "\n",
            "         [[-0.0614, -0.1093, -0.0441],\n",
            "          [-0.0016, -0.0871,  0.0834],\n",
            "          [ 0.1636,  0.1113,  0.0583]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0186,  0.1560,  0.1286],\n",
            "          [ 0.0013, -0.1768,  0.1006],\n",
            "          [ 0.0497,  0.0966,  0.0372]],\n",
            "\n",
            "         [[ 0.0242,  0.1610,  0.0089],\n",
            "          [-0.1030,  0.1083,  0.0291],\n",
            "          [-0.0889, -0.0140, -0.0529]],\n",
            "\n",
            "         [[-0.0602, -0.0337,  0.0106],\n",
            "          [ 0.0970, -0.1246,  0.1137],\n",
            "          [-0.0116, -0.1311,  0.0514]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.1545, -0.2471,  0.1099],\n",
            "          [ 0.0357, -0.0165,  0.0896],\n",
            "          [ 0.0098, -0.0448,  0.0426]],\n",
            "\n",
            "         [[-0.0229,  0.0538,  0.1261],\n",
            "          [-0.0787, -0.0236, -0.0410],\n",
            "          [-0.0355, -0.0497,  0.0109]],\n",
            "\n",
            "         [[ 0.1396, -0.0582,  0.2221],\n",
            "          [-0.0272, -0.0627,  0.1262],\n",
            "          [-0.0971,  0.1389, -0.1234]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2090,  0.0472, -0.1242],\n",
            "          [-0.0801, -0.0270, -0.1458],\n",
            "          [ 0.0372,  0.0538, -0.1855]],\n",
            "\n",
            "         [[-0.2004, -0.0080,  0.1871],\n",
            "          [-0.0338,  0.0120,  0.0299],\n",
            "          [ 0.0263, -0.0042,  0.0187]],\n",
            "\n",
            "         [[-0.0104,  0.0524, -0.0249],\n",
            "          [-0.0160,  0.0708, -0.0020],\n",
            "          [-0.0946,  0.0666,  0.0087]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0480, -0.0989,  0.0841],\n",
            "          [ 0.1298, -0.1690,  0.0712],\n",
            "          [ 0.0815,  0.1038, -0.0133]],\n",
            "\n",
            "         [[ 0.0518,  0.0823,  0.0529],\n",
            "          [ 0.0898, -0.0664, -0.0633],\n",
            "          [-0.0690,  0.0405,  0.0734]],\n",
            "\n",
            "         [[-0.0744, -0.0593, -0.0452],\n",
            "          [-0.0489,  0.1223,  0.0492],\n",
            "          [-0.0090, -0.0973, -0.0453]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0557,  0.0146, -0.0406],\n",
            "          [ 0.0042, -0.1293, -0.1614],\n",
            "          [-0.1167, -0.0103,  0.0581]],\n",
            "\n",
            "         [[-0.0360,  0.0597, -0.2047],\n",
            "          [-0.2525,  0.0336, -0.1462],\n",
            "          [-0.0090, -0.0948,  0.0082]],\n",
            "\n",
            "         [[ 0.0669,  0.0536,  0.0797],\n",
            "          [-0.1355, -0.1218,  0.1559],\n",
            "          [ 0.0802, -0.0098, -0.0026]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0335, -0.0620, -0.1035],\n",
            "          [-0.1315, -0.0528,  0.0295],\n",
            "          [ 0.1664,  0.0141,  0.0514]],\n",
            "\n",
            "         [[-0.0396,  0.2136, -0.0352],\n",
            "          [ 0.1589,  0.1012, -0.0677],\n",
            "          [-0.1809,  0.1497, -0.0775]],\n",
            "\n",
            "         [[-0.2418,  0.0557,  0.0249],\n",
            "          [ 0.1245,  0.0425,  0.1021],\n",
            "          [-0.2269,  0.0600,  0.1752]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0333, -0.2342, -0.0374],\n",
            "          [-0.0321, -0.0148,  0.2117],\n",
            "          [-0.0040,  0.1350,  0.1473]],\n",
            "\n",
            "         [[ 0.0602,  0.0091,  0.1631],\n",
            "          [-0.0264,  0.0891, -0.0168],\n",
            "          [ 0.0213,  0.2515, -0.0228]],\n",
            "\n",
            "         [[ 0.0181, -0.0010,  0.0655],\n",
            "          [ 0.1321,  0.1466, -0.0550],\n",
            "          [-0.0799, -0.2208,  0.0334]]]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.1519,  0.0952,  0.0304, -0.0302,  0.0612,  0.0971,  0.0345,  0.1858,\n",
            "         0.0767,  0.0517, -0.0313,  0.2482,  0.1297, -0.1677, -0.1075, -0.0358,\n",
            "         0.0391, -0.0399,  0.0437, -0.1177,  0.0238,  0.2004,  0.0101,  0.1281,\n",
            "         0.0360, -0.0631, -0.0045,  0.1009,  0.0633, -0.1283,  0.0505,  0.0354,\n",
            "        -0.1755,  0.1003, -0.0413,  0.1007, -0.1314,  0.0031, -0.2170, -0.1068,\n",
            "        -0.0509,  0.1158, -0.0600, -0.0394,  0.0355, -0.0090, -0.1831,  0.1068],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[-1.8289e-01, -1.1757e-01, -2.8821e-02, -1.1347e-01,  7.1535e-02,\n",
            "         -7.6396e-02,  1.5928e-02, -1.3671e-01,  2.1809e-01, -5.5418e-02,\n",
            "         -3.4265e-03,  3.1585e-02,  2.1929e-02, -3.3835e-02,  1.7379e-01,\n",
            "         -9.2798e-02,  3.8340e-03,  1.4497e-01, -1.7992e-02, -4.6275e-02,\n",
            "          1.1872e-01,  1.3659e-01, -1.0015e-01, -1.4851e-01, -7.1347e-05,\n",
            "         -4.0334e-02, -5.7949e-02,  9.6463e-02,  1.0854e-01,  1.1844e-03,\n",
            "          1.0409e-01,  5.2626e-02,  4.9521e-03, -2.4454e-02, -1.7247e-01,\n",
            "         -5.4988e-02,  1.5178e-01, -9.5530e-02,  2.4496e-02, -4.3658e-02,\n",
            "         -4.8837e-02,  1.2343e-01,  1.5165e-01,  1.0391e-01, -1.6183e-01,\n",
            "         -4.0409e-02,  1.3479e-01,  7.6922e-02],\n",
            "        [-3.3264e-02,  2.7761e-01,  2.5876e-02, -3.0062e-02,  9.1801e-02,\n",
            "          8.4297e-02,  1.5179e-01, -2.3259e-01, -9.0814e-02,  1.4575e-01,\n",
            "          1.3724e-01,  2.1586e-02, -2.8061e-02, -2.3110e-01, -1.6136e-01,\n",
            "          1.3156e-02,  1.0638e-01,  3.2172e-02, -7.5044e-02,  5.7596e-02,\n",
            "         -6.9835e-02, -5.5666e-02, -1.4711e-01, -4.4263e-02, -1.3608e-01,\n",
            "          4.8302e-03, -5.1721e-02,  1.3066e-01, -3.1975e-02, -5.1868e-02,\n",
            "          1.1730e-01, -3.9104e-02, -1.4911e-01,  1.8226e-01, -1.0200e-02,\n",
            "          3.4755e-02, -2.6562e-02,  1.9639e-01, -7.0164e-02, -3.1651e-02,\n",
            "         -7.4535e-02,  3.3865e-02,  2.0571e-01,  2.1031e-02, -1.1780e-01,\n",
            "         -2.1755e-02, -2.1303e-02,  6.8994e-03],\n",
            "        [-1.0004e-01, -3.5988e-02,  8.0998e-03, -6.1619e-02, -5.2005e-02,\n",
            "         -1.8688e-01, -1.0261e-01, -9.8714e-02,  1.4333e-01, -1.3064e-01,\n",
            "         -8.4756e-02,  1.9791e-02,  1.9341e-01, -9.5464e-02, -1.5690e-01,\n",
            "          6.7803e-02,  4.0272e-02, -3.7256e-02,  6.2756e-02,  2.9095e-02,\n",
            "         -1.5062e-01,  7.5789e-02,  3.8339e-02,  3.4289e-02, -2.0524e-02,\n",
            "         -1.2894e-02,  1.0990e-01, -1.0067e-01, -5.8213e-02, -1.5134e-01,\n",
            "          2.6896e-01,  8.6820e-02,  6.3068e-03, -5.5071e-03,  1.2848e-02,\n",
            "          2.5261e-01,  6.8591e-02,  4.4944e-02, -8.1759e-02,  1.3690e-01,\n",
            "         -8.0418e-02, -5.5601e-02,  9.3056e-02,  1.4095e-01,  5.1051e-02,\n",
            "          4.7255e-02, -4.5880e-02,  4.8299e-02],\n",
            "        [-1.1498e-01, -1.2539e-01, -4.4617e-02,  8.5462e-02, -9.8930e-02,\n",
            "          8.4265e-02, -3.3034e-02, -1.1637e-01,  5.4928e-02, -5.6728e-02,\n",
            "          1.3867e-01, -3.8002e-02,  1.5309e-01,  4.9732e-03,  5.9652e-02,\n",
            "          3.3548e-02, -5.6458e-02, -1.1968e-01,  1.4255e-01, -2.0665e-01,\n",
            "         -8.7554e-02, -1.1687e-01,  4.1810e-03,  6.9031e-02,  2.1860e-01,\n",
            "         -1.5095e-02,  1.3047e-01,  1.2265e-01, -3.1115e-02, -3.3150e-02,\n",
            "         -7.1685e-02, -2.9065e-01, -6.3398e-02, -3.4241e-02,  1.5107e-01,\n",
            "          2.0254e-01, -2.1564e-02,  6.5036e-02, -4.1794e-02, -1.0006e-01,\n",
            "          6.6119e-03, -5.0589e-03,  1.2581e-03, -2.0101e-02,  8.2708e-03,\n",
            "          4.0845e-02, -6.4684e-02,  1.5079e-01],\n",
            "        [ 1.7206e-02,  7.6657e-02, -8.9731e-02,  7.0077e-02,  8.2574e-02,\n",
            "          1.2962e-01, -1.1082e-01, -1.7983e-03, -1.7235e-01,  1.3935e-02,\n",
            "          6.5814e-02,  8.5591e-03, -1.5236e-01,  6.2225e-02, -9.5952e-02,\n",
            "          5.0883e-02,  9.1026e-02,  1.8867e-02,  1.3722e-02,  1.4190e-02,\n",
            "          1.8044e-01,  6.0828e-02, -9.5591e-02,  4.9754e-02,  2.2064e-02,\n",
            "          2.2129e-01, -5.0121e-02, -1.2786e-01, -3.9585e-02, -1.4237e-01,\n",
            "          2.7603e-02, -5.7831e-02,  1.1215e-02,  2.2448e-01,  2.0200e-01,\n",
            "         -5.6999e-02, -2.4561e-01,  1.0968e-01, -1.8966e-01,  1.3993e-01,\n",
            "         -3.9659e-02, -3.8804e-02, -5.9602e-03, -4.6744e-02,  7.2734e-02,\n",
            "         -2.7972e-02, -1.5708e-01, -1.0566e-01]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 3.5186e-02,  8.7565e-02,  4.0972e-02, -1.2078e-01,  8.1530e-02,\n",
            "         -1.1147e-01, -7.8868e-02,  1.8730e-01,  4.4809e-03,  3.7696e-02,\n",
            "         -1.7821e-04,  1.1646e-01,  1.4685e-02, -7.5692e-02,  1.3232e-01,\n",
            "          4.8797e-02, -1.5775e-03, -1.3244e-01, -8.5958e-02, -3.4117e-02,\n",
            "         -2.0682e-02, -8.2059e-02, -8.7827e-02, -3.8934e-02,  5.0023e-02,\n",
            "         -2.8453e-02,  1.9044e-01, -1.4975e-01, -2.5768e-02,  1.1077e-02,\n",
            "         -2.9599e-02, -2.0017e-01,  2.1633e-01,  3.6372e-02,  5.1049e-02,\n",
            "          9.2830e-02,  1.3102e-02,  2.0601e-02, -9.2798e-03, -8.0380e-03,\n",
            "         -1.0351e-01,  1.2514e-01, -7.8872e-02, -8.9135e-02,  4.1732e-02,\n",
            "         -1.8188e-02,  1.3560e-01, -7.8524e-02],\n",
            "        [ 2.7718e-02, -1.8232e-01,  1.1389e-02, -5.8458e-02,  2.4482e-01,\n",
            "         -5.6781e-02,  1.2235e-01,  3.1741e-02, -6.2436e-02,  2.8957e-02,\n",
            "         -9.3340e-02, -6.6068e-02,  1.1777e-01,  1.9890e-01, -9.6648e-02,\n",
            "         -7.0001e-02, -1.1238e-01,  2.5250e-02, -7.8101e-02, -6.0590e-02,\n",
            "         -4.1437e-02, -7.8046e-02,  3.5839e-02,  4.4729e-02,  7.3859e-02,\n",
            "          2.0045e-02, -1.4146e-02,  1.1776e-01, -6.9913e-02,  2.2856e-02,\n",
            "         -2.4032e-01, -1.7866e-01, -2.9729e-02, -3.4620e-02,  1.2974e-01,\n",
            "          8.5024e-02,  9.5151e-02,  4.4720e-02, -9.8570e-02,  1.2293e-01,\n",
            "          1.1014e-01,  8.4720e-02,  7.5298e-02,  1.6731e-01,  1.5703e-02,\n",
            "          9.0639e-02, -4.3778e-02, -8.8247e-02],\n",
            "        [-1.1349e-01,  6.4485e-03, -1.6674e-01,  7.8661e-02, -1.3612e-01,\n",
            "         -2.2968e-01,  5.0833e-02,  1.6305e-02,  1.4809e-02, -4.8455e-02,\n",
            "         -9.2067e-02,  6.8845e-02,  1.6539e-01, -1.0523e-01,  2.5614e-02,\n",
            "          1.0537e-01, -3.8143e-02, -3.7079e-02,  9.1133e-03,  8.0542e-02,\n",
            "          3.4814e-02,  1.1315e-01,  1.3464e-01,  1.7223e-01,  8.1463e-02,\n",
            "          6.3524e-02, -4.6893e-02,  7.3272e-03, -6.0719e-02, -1.1833e-02,\n",
            "          4.3744e-02, -2.5026e-02,  1.1885e-01, -1.0457e-02, -9.2189e-02,\n",
            "          8.4430e-02,  1.0153e-01,  3.4198e-04,  2.6214e-01,  3.2020e-02,\n",
            "         -1.7445e-01,  5.9401e-03, -1.8095e-01,  3.5708e-03, -1.7746e-02,\n",
            "          1.6736e-01, -3.6650e-02, -1.8715e-02],\n",
            "        [ 1.9519e-02, -8.0364e-02,  4.7462e-02,  7.4594e-02, -9.7124e-02,\n",
            "         -1.2303e-01,  1.0680e-01, -1.0114e-01, -9.2228e-02,  9.3050e-02,\n",
            "          1.3879e-01, -3.0073e-02,  1.3571e-01, -6.5730e-02, -5.8325e-03,\n",
            "         -1.6186e-01, -3.3859e-03,  1.1733e-01,  1.2742e-01,  6.5908e-02,\n",
            "         -1.3571e-01,  1.5421e-01, -1.1466e-01, -5.3993e-03, -1.1823e-02,\n",
            "         -7.3196e-03, -1.8867e-02, -1.0218e-02, -1.0402e-01,  4.2626e-02,\n",
            "          2.3093e-02, -8.7665e-02, -1.5238e-01,  6.3216e-02,  4.3134e-02,\n",
            "          1.3836e-01, -2.7406e-02,  4.6250e-02,  6.4330e-02,  2.8686e-02,\n",
            "          1.0406e-01,  1.0335e-01,  6.1912e-02,  7.5206e-02,  8.9574e-02,\n",
            "         -1.0460e-01, -8.3580e-02, -1.1506e-01],\n",
            "        [ 1.4840e-01, -5.6033e-02, -1.8245e-02, -6.6504e-02,  4.4681e-02,\n",
            "         -1.8810e-01, -1.0010e-01, -3.8315e-02,  8.2917e-02, -6.6827e-02,\n",
            "         -1.6921e-01, -4.6523e-02, -2.1939e-01,  1.1118e-01, -1.8796e-02,\n",
            "          4.7694e-02, -9.3673e-03, -5.1850e-02, -2.1784e-02, -1.1893e-02,\n",
            "          2.0560e-03, -1.2971e-01,  6.2687e-02, -2.1028e-02,  8.8278e-02,\n",
            "         -7.1923e-02, -6.5180e-02, -6.2804e-03, -1.1069e-01, -4.6959e-02,\n",
            "         -5.4230e-02, -1.1876e-01,  8.4475e-02,  2.3164e-01,  6.9238e-02,\n",
            "          1.3724e-01,  4.3562e-02,  9.5832e-02, -5.2865e-02,  3.1461e-02,\n",
            "          2.1073e-01,  2.0913e-01, -8.3036e-02, -7.3248e-02, -3.8687e-02,\n",
            "          5.5923e-02, -1.4985e-01,  4.5063e-02]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[-0.0118, -0.0500,  0.0767,  ...,  0.0083, -0.1180, -0.0575],\n",
            "        [-0.0908,  0.1931, -0.0199,  ..., -0.0316, -0.0677, -0.1200],\n",
            "        [-0.1246,  0.0337,  0.0385,  ...,  0.0417, -0.0683,  0.1537],\n",
            "        [ 0.0166, -0.0264,  0.1385,  ..., -0.1854,  0.1430,  0.0177],\n",
            "        [ 0.0497, -0.2701,  0.1881,  ...,  0.0239,  0.0353, -0.0015]],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0116,  0.2291, -0.0212,  0.0672,  0.0589], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0166,  0.1837,  0.0968, -0.1098, -0.0192,  0.0113], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.1042,  0.1051,  0.0988,  0.1091,  0.0661, -0.0450], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0845,  0.1815, -0.2016, -0.0175,  0.0492, -0.0350], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0763, -0.0453, -0.0376,  0.1434, -0.0363,  0.0749], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0259, -0.1164,  0.1801,  0.1823, -0.0011, -0.0633], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0139, -0.1339, -0.0636, -0.0773, -0.0967,  0.0552], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0305,  0.0452,  0.1046, -0.0354,  0.1372, -0.0113], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0532, -0.1741, -0.0399, -0.1537, -0.0664, -0.0689], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0194,  0.0761, -0.0077, -0.1037,  0.0740,  0.0555], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0871, -0.1730,  0.0622, -0.0069, -0.1575,  0.0576], device='cuda:0') amount of noise :)\n",
            "attempting to find existing checkpoint\n",
            "Loading pre-split data\n",
            "Loading data into RAM\n",
            "Currently loading into memory the test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:12<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently loading into memory the train set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 12/64 [00:10<00:45,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-15e542ebc573>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m                         \u001b[0;31m# Process the list of files, but split the work across the process pool to use all CPUs!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_images_loaded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parallel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                             \u001b[0mx_loaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_images_loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-2630730195d2>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# maybe_unzip_dataset(args=args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetaLearningSystemDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmaml_system\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperimentBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# maml_system.run_experiment()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-e606da46e167>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, data, model, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current_iter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_iter_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current_iter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         print(\"train_seed {}, val_seed: {}, at start time\".format(self.data.dataset.seed[\"train\"],\n",
            "\u001b[0;32m<ipython-input-27-15e542ebc573>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, current_iter)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataprovider_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_train_iters_produced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFewShotLearningDatasetParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_per_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples_per_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_data_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-15e542ebc573>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-15e542ebc573>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;31m# for class_key, class_value in set_value.items():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar_memory_load\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                     \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m                         \u001b[0;31m# Process the list of files, but split the work across the process pool to use all CPUs!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_images_loaded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parallel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;31m# objects that use file descriptors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Combines the arguments, model, data and experiment builders to run an experiment\n",
        "\n",
        "# python train_maml_system.py --name_of_args_json_file experiment_config/ --gpu_to_use 1\n",
        "# import sys\n",
        "# # Simulate command line arguments\n",
        "# sys.argv = [  # Replace with the current file name\n",
        "#            '--name_of_args_json_file', 'content/omniglot_maml++-omniglot_5_8_0.1_64_5_2.json',\n",
        "#            '--gpu_to_use', '1',  # Dataset directory\n",
        "#           #  '--experiment_name', 'omniglot_experiment',  # Experiment name\n",
        "#           #  '--architecture_name', 'maml', # You'll likely need to provide an appropriate architecture name\n",
        "#            # ... add other necessary arguments\n",
        "#            ]\n",
        "\n",
        "# args, device = get_args()\n",
        "args, device = load_args_from_json(\"/content/HowToTrainYourMAMLPytorch/experiment_config/mini-imagenet_maml++-mini-imagenet_5_2_0.01_48_5_2.json\")\n",
        "\n",
        "model = MAMLFewShotClassifier(args=args, device=device,\n",
        "                              im_shape=(2, args.image_channels,\n",
        "                                        args.image_height, args.image_width))\n",
        "# maybe_unzip_dataset(args=args)\n",
        "data = MetaLearningSystemDataLoader\n",
        "# maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
        "# maml_system.run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = MAMLFewShotClassifier(args=args, device=device, im_shape=(2, args.image_channels, args.image_height, args.image_width))\n",
        "\n",
        "class ClassifierWrapper(nn.Module):\n",
        "    def __init__(self, classifier):\n",
        "        super().__init__()\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x, num_step=0, params=None, training=False, backup_running_statistics=False)\n",
        "\n",
        "wrapped_classifier = ClassifierWrapper(model.classifier).to(device)\n",
        "\n",
        "summary(wrapped_classifier, input_size=(args.image_channels, args.image_height, args.image_width))"
      ],
      "metadata": {
        "id": "JyapLpHiolg1",
        "outputId": "11958cfb-ab8a-41eb-970d-aca8830c0890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using max pooling\n",
            "torch.Size([2, 48, 84, 84])\n",
            "torch.Size([2, 48, 42, 42])\n",
            "torch.Size([2, 48, 21, 21])\n",
            "torch.Size([2, 48, 10, 10])\n",
            "VGGNetwork build torch.Size([2, 5])\n",
            "meta network params\n",
            "layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3])\n",
            "layer_dict.conv0.conv.bias torch.Size([48])\n",
            "layer_dict.conv0.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv0.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv1.conv.bias torch.Size([48])\n",
            "layer_dict.conv1.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv1.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv2.conv.bias torch.Size([48])\n",
            "layer_dict.conv2.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv2.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3])\n",
            "layer_dict.conv3.conv.bias torch.Size([48])\n",
            "layer_dict.conv3.norm_layer.running_mean torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.running_var torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.bias torch.Size([5, 48])\n",
            "layer_dict.conv3.norm_layer.weight torch.Size([5, 48])\n",
            "layer_dict.linear.weights torch.Size([5, 1200])\n",
            "layer_dict.linear.bias torch.Size([5])\n",
            "0.1\n",
            "Inner Loop parameters\n",
            "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
            "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
            "Outer Loop parameters\n",
            "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
            "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
            "classifier.layer_dict.conv0.norm_layer.bias torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv0.norm_layer.weight torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
            "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
            "classifier.layer_dict.conv1.norm_layer.bias torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv1.norm_layer.weight torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
            "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
            "classifier.layer_dict.conv2.norm_layer.bias torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv2.norm_layer.weight torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
            "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
            "classifier.layer_dict.conv3.norm_layer.bias torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.conv3.norm_layer.weight torch.Size([5, 48]) cuda:0 True\n",
            "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
            "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
            "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
            "the grinch added tensor([[[[ 0.0366, -0.0136,  0.0609],\n",
            "          [-0.1540, -0.1250,  0.0168],\n",
            "          [ 0.1609, -0.0529,  0.0982]],\n",
            "\n",
            "         [[-0.0242,  0.0808, -0.1334],\n",
            "          [-0.0563, -0.0312,  0.0892],\n",
            "          [ 0.0176,  0.0561, -0.0330]],\n",
            "\n",
            "         [[-0.1459, -0.1176, -0.2493],\n",
            "          [-0.0600, -0.0621, -0.1369],\n",
            "          [ 0.1923,  0.0177,  0.0440]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1760,  0.0874, -0.1096],\n",
            "          [-0.0036,  0.1494,  0.1443],\n",
            "          [-0.0694,  0.0341, -0.1425]],\n",
            "\n",
            "         [[ 0.1653, -0.0265,  0.0153],\n",
            "          [ 0.1271, -0.0411,  0.0855],\n",
            "          [ 0.1902, -0.0816,  0.0730]],\n",
            "\n",
            "         [[ 0.2127,  0.0577, -0.0395],\n",
            "          [ 0.1484, -0.0287,  0.0396],\n",
            "          [ 0.1332,  0.0251,  0.1973]]],\n",
            "\n",
            "\n",
            "        [[[-0.1183,  0.0057, -0.0192],\n",
            "          [ 0.1279,  0.2638,  0.1146],\n",
            "          [-0.0541,  0.0129, -0.1629]],\n",
            "\n",
            "         [[-0.0710,  0.0479, -0.1533],\n",
            "          [ 0.0442, -0.0056,  0.2875],\n",
            "          [ 0.1240, -0.1000, -0.1729]],\n",
            "\n",
            "         [[ 0.0252,  0.2132, -0.0816],\n",
            "          [-0.0205,  0.1736,  0.0616],\n",
            "          [-0.0889,  0.0257, -0.1043]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0232,  0.1204, -0.1061],\n",
            "          [ 0.0091,  0.0343, -0.1045],\n",
            "          [-0.2785,  0.0464, -0.1235]],\n",
            "\n",
            "         [[ 0.1366,  0.0312,  0.0463],\n",
            "          [-0.1432,  0.0007, -0.0992],\n",
            "          [-0.0737,  0.0553,  0.1026]],\n",
            "\n",
            "         [[-0.1049, -0.0084, -0.0797],\n",
            "          [ 0.0286,  0.0657, -0.0254],\n",
            "          [-0.0530,  0.0700, -0.1164]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0538, -0.0408, -0.0295],\n",
            "          [ 0.0137,  0.1900,  0.1125],\n",
            "          [ 0.1558, -0.0220, -0.0599]],\n",
            "\n",
            "         [[ 0.0542,  0.0260, -0.0246],\n",
            "          [-0.0372, -0.1192,  0.1789],\n",
            "          [ 0.0179, -0.1008, -0.1286]],\n",
            "\n",
            "         [[-0.0199,  0.2143,  0.0131],\n",
            "          [-0.1292, -0.2143, -0.0235],\n",
            "          [-0.1157,  0.1619, -0.0101]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0124, -0.0043,  0.0937],\n",
            "          [-0.0667, -0.0305,  0.0624],\n",
            "          [-0.0099, -0.0163,  0.0110]],\n",
            "\n",
            "         [[-0.0377,  0.0863, -0.0320],\n",
            "          [ 0.0440,  0.0498,  0.0040],\n",
            "          [-0.0918,  0.0936, -0.0183]],\n",
            "\n",
            "         [[ 0.0166, -0.0254,  0.0032],\n",
            "          [-0.0211,  0.0905, -0.0016],\n",
            "          [-0.1463,  0.0934,  0.0338]]]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0329, -0.1129, -0.0698,  0.0165, -0.0255, -0.0011,  0.0008,  0.1762,\n",
            "        -0.1926, -0.1850,  0.0173, -0.0748, -0.0418,  0.0461, -0.0176,  0.0081,\n",
            "         0.0341, -0.1659,  0.1051, -0.1049, -0.0008,  0.0026, -0.0767, -0.1087,\n",
            "         0.1760, -0.1560,  0.0080,  0.2486,  0.0475, -0.2210,  0.0139,  0.0203,\n",
            "         0.0108, -0.0943, -0.0394,  0.1519,  0.0041, -0.1591, -0.0849,  0.0076,\n",
            "         0.0263,  0.1186, -0.0979, -0.0605,  0.0592, -0.0535,  0.0010, -0.1637],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 3.7180e-02,  6.2472e-02,  7.9275e-02,  2.2362e-01,  1.3501e-01,\n",
            "         -4.2056e-02,  7.6042e-02,  5.8889e-02,  1.8096e-01, -6.3512e-02,\n",
            "          1.3052e-01, -1.2000e-02, -1.2475e-02, -4.9394e-02,  2.0063e-02,\n",
            "          1.8136e-02,  4.3669e-02, -9.4203e-02,  6.0531e-02, -7.6137e-02,\n",
            "          2.4204e-02,  7.9669e-02,  2.0765e-03, -1.2347e-01, -8.1067e-02,\n",
            "         -2.7022e-02, -1.2428e-03, -1.5044e-01, -4.0208e-02, -1.2774e-01,\n",
            "          4.8504e-02,  2.0418e-02,  2.2444e-02,  1.6849e-01, -6.8270e-02,\n",
            "          6.8006e-02,  3.9416e-03, -8.9345e-02,  4.3386e-02,  9.4794e-02,\n",
            "          1.4644e-01, -1.0986e-01, -4.8059e-03, -1.4768e-01,  4.7953e-02,\n",
            "         -8.2935e-02, -6.7297e-02,  8.9825e-02],\n",
            "        [-7.0267e-02,  1.3876e-01, -1.2532e-01, -1.0717e-01,  4.8036e-02,\n",
            "         -1.9017e-01, -8.2584e-02, -6.3544e-02,  7.2996e-02, -9.2871e-02,\n",
            "         -1.4883e-01,  1.4367e-01,  1.6129e-01,  8.7999e-02,  1.2850e-02,\n",
            "         -3.8407e-02,  1.6356e-01, -3.7712e-02, -6.5327e-02,  8.8089e-02,\n",
            "         -3.1550e-02, -1.2123e-01, -5.0105e-02, -1.3002e-02, -3.4253e-02,\n",
            "         -8.2850e-02, -9.8646e-02, -3.8362e-02,  9.8417e-02, -4.3977e-02,\n",
            "          2.0346e-02, -1.7787e-01, -5.0068e-02, -1.1617e-01,  6.0188e-02,\n",
            "          7.2818e-03, -8.7272e-02, -1.5902e-01,  2.6152e-02, -3.0838e-02,\n",
            "          1.3111e-01,  4.1489e-03, -2.5636e-01, -2.2688e-01,  2.4187e-02,\n",
            "          1.2483e-01,  4.9336e-03, -1.1846e-01],\n",
            "        [ 8.9995e-02,  9.1830e-02, -4.6226e-03,  5.4647e-02, -4.3764e-02,\n",
            "         -1.7092e-01,  1.5603e-01,  8.7969e-02,  2.4171e-02,  7.3101e-02,\n",
            "         -3.8795e-02,  1.6081e-01,  2.4910e-01,  7.1790e-02,  3.3119e-02,\n",
            "          7.7333e-02, -1.0430e-01,  1.9044e-02,  1.2697e-01,  2.4317e-01,\n",
            "         -9.1379e-02, -3.6836e-03,  3.9565e-02,  1.5913e-01,  3.4774e-02,\n",
            "         -2.1475e-02, -1.8883e-02,  1.7346e-02,  1.6455e-01, -1.3387e-01,\n",
            "         -1.1892e-01,  1.0757e-01, -8.7354e-02,  1.1371e-02, -1.2408e-01,\n",
            "         -1.4019e-01, -6.2066e-02, -6.7967e-02,  9.6435e-02,  1.5019e-01,\n",
            "         -8.5705e-02, -9.4630e-02,  1.0286e-01, -4.8589e-02, -2.0318e-02,\n",
            "          1.3786e-01,  9.6059e-02, -8.6803e-02],\n",
            "        [-1.5655e-02, -7.0537e-02, -9.1429e-03,  6.6965e-02, -1.1969e-01,\n",
            "          6.1911e-02,  7.1336e-02, -1.1044e-01,  3.9743e-02, -3.4038e-02,\n",
            "          1.1344e-01,  8.0599e-02, -8.0878e-02, -5.4776e-02, -7.9786e-02,\n",
            "         -1.5321e-02,  1.2332e-01,  4.6264e-02,  9.5713e-02,  1.2089e-01,\n",
            "         -3.1538e-02, -5.5854e-02,  3.6655e-02, -1.3421e-01,  1.8249e-03,\n",
            "          3.6031e-03, -1.8267e-02,  7.9242e-02, -1.3886e-02,  6.0055e-03,\n",
            "         -4.1960e-02, -7.2346e-02,  6.3188e-02,  1.2111e-01, -9.5764e-02,\n",
            "         -5.2670e-02, -4.4112e-02, -5.6687e-02,  5.9178e-02,  3.7174e-02,\n",
            "          2.2149e-01,  1.5181e-01, -6.2694e-02,  2.3092e-02,  1.2453e-01,\n",
            "         -9.3370e-02, -6.9155e-02, -1.7339e-02],\n",
            "        [-1.2525e-02, -1.7579e-02,  1.1126e-01,  2.2295e-02,  2.6561e-02,\n",
            "          3.7306e-02,  2.0370e-02, -7.8709e-02, -3.7493e-02,  1.0517e-01,\n",
            "          1.6909e-01,  5.1784e-02,  4.2377e-03, -8.1281e-02,  1.3313e-01,\n",
            "          2.3179e-02, -7.9419e-02,  9.3225e-02, -1.2626e-01,  2.5429e-01,\n",
            "         -7.2574e-03, -1.6142e-01, -1.9377e-01,  3.6210e-02,  1.0618e-01,\n",
            "         -1.3523e-01,  4.4247e-02, -9.0626e-02,  5.0371e-02, -3.9013e-02,\n",
            "          5.5932e-02,  5.7709e-02,  1.2547e-01,  5.6481e-02, -1.7325e-01,\n",
            "          3.7506e-04, -4.2417e-02, -5.6947e-02, -5.2692e-02, -6.1723e-02,\n",
            "         -7.4881e-02, -2.4069e-02, -5.2344e-02, -2.1867e-02, -7.6435e-02,\n",
            "          7.0718e-03,  1.4068e-01,  1.7055e-04]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[-8.7494e-02, -2.0637e-02, -4.4899e-03, -4.8741e-02,  1.6676e-01,\n",
            "         -8.4286e-02, -2.2599e-03,  1.5552e-01,  8.0627e-02,  2.0512e-02,\n",
            "          2.3096e-02, -1.2449e-01,  8.9230e-02, -8.0234e-02,  1.8384e-01,\n",
            "         -1.5322e-02, -1.9527e-01,  1.0610e-01, -5.2042e-02, -3.7085e-02,\n",
            "         -7.0506e-03,  1.9769e-03,  1.3130e-01, -1.9989e-01,  8.8067e-02,\n",
            "         -8.2662e-02, -2.0283e-01, -3.1105e-02,  2.2347e-01,  2.4947e-02,\n",
            "          1.5081e-02, -8.8467e-02, -1.6488e-01, -9.5514e-02,  4.8927e-02,\n",
            "          1.4175e-02,  1.7904e-01,  6.6340e-02,  1.7543e-01,  2.6605e-01,\n",
            "          1.2046e-01,  9.5850e-02, -8.7452e-02, -8.1311e-02, -7.9778e-02,\n",
            "          1.5461e-01, -7.6793e-02,  1.9515e-03],\n",
            "        [ 4.7396e-02, -8.8603e-02, -3.3250e-02,  1.9239e-04,  3.1990e-02,\n",
            "         -2.1646e-02, -3.3637e-02, -1.9025e-01,  1.9692e-02,  6.1355e-02,\n",
            "         -8.3333e-02, -7.2804e-02,  7.3740e-03,  3.0397e-03,  1.3464e-01,\n",
            "         -1.8901e-01, -1.3546e-01,  1.7179e-01,  1.2771e-01, -5.6367e-02,\n",
            "         -1.7893e-01,  7.0364e-02,  6.2526e-02,  7.8897e-02,  2.7154e-02,\n",
            "         -7.9279e-02, -1.1234e-01,  2.1846e-01,  4.9088e-02, -1.9869e-01,\n",
            "         -1.0779e-02, -2.7903e-02,  1.0076e-01, -1.0002e-01,  1.8162e-01,\n",
            "          2.4187e-02, -3.7152e-03,  6.1989e-02,  1.3901e-01,  4.8507e-04,\n",
            "          9.8674e-02, -4.4368e-02, -4.0059e-02, -5.0676e-02, -3.5736e-02,\n",
            "          3.3393e-02,  9.2080e-03, -1.0839e-03],\n",
            "        [-2.8780e-02, -1.0923e-01, -1.2057e-01, -1.4996e-01, -7.9963e-02,\n",
            "          3.7442e-02, -8.5170e-02, -1.0523e-01, -1.3555e-01,  1.3402e-01,\n",
            "         -9.7721e-04,  2.2256e-02, -1.8845e-02, -2.5045e-02,  2.1161e-02,\n",
            "          1.4021e-01, -8.1560e-02, -8.4254e-02,  8.8461e-03, -3.7614e-02,\n",
            "         -8.4182e-02, -6.1632e-02, -1.9448e-01, -4.6537e-02, -2.4877e-02,\n",
            "         -3.5444e-02, -6.4592e-02, -1.7037e-02, -5.8762e-02, -4.9061e-02,\n",
            "          6.1189e-02, -1.3159e-02,  1.7265e-01,  6.5419e-02,  8.1347e-02,\n",
            "          2.4320e-01,  4.8994e-02,  3.7469e-03, -6.4463e-02,  6.4616e-02,\n",
            "         -6.7563e-03, -1.2413e-01, -6.8978e-02, -3.3764e-02, -1.7137e-02,\n",
            "         -8.2019e-02,  4.4240e-03, -5.5150e-02],\n",
            "        [-1.6263e-02,  1.2450e-01,  6.3608e-02,  8.4387e-03,  4.6497e-03,\n",
            "          4.1836e-02,  3.9319e-02,  4.2876e-02,  5.3406e-02,  1.8542e-01,\n",
            "          7.7980e-02, -7.6645e-02, -3.8197e-02,  6.3520e-03,  2.2031e-02,\n",
            "         -9.7075e-02,  1.5850e-01, -1.3038e-02, -1.1305e-03,  8.5594e-02,\n",
            "          6.8732e-02, -8.2545e-02, -9.3282e-02,  6.3332e-02, -7.4911e-02,\n",
            "          1.0573e-01,  9.8219e-02,  2.7497e-02, -6.6704e-02, -4.8227e-04,\n",
            "         -6.1173e-02, -1.4382e-01, -1.9742e-03,  1.0600e-01, -1.1194e-01,\n",
            "         -1.1457e-01, -1.9570e-01,  1.1940e-02, -4.1474e-02, -8.1186e-02,\n",
            "          4.7412e-02,  8.4195e-02,  1.6238e-01,  1.0852e-01, -2.2425e-01,\n",
            "         -4.6980e-02,  4.6475e-02, -1.3551e-01],\n",
            "        [ 4.3573e-02, -7.8043e-02, -5.6239e-02, -2.1019e-01,  1.3714e-01,\n",
            "          1.1261e-01,  7.8170e-03, -8.8846e-02, -1.6708e-01, -2.1583e-02,\n",
            "          1.6745e-01,  6.6755e-04,  1.3611e-01, -1.2790e-01, -1.8655e-01,\n",
            "          3.5698e-02, -7.4861e-02, -1.7534e-02, -2.4207e-03,  2.2243e-01,\n",
            "          5.3036e-02, -1.0047e-01,  9.1334e-02,  1.1790e-01,  5.2742e-02,\n",
            "         -1.2201e-01,  6.1377e-02,  8.7714e-02,  2.0077e-02, -4.2411e-03,\n",
            "         -6.5421e-02,  1.8531e-02,  8.4937e-02, -1.1914e-01, -8.7909e-02,\n",
            "          7.7397e-02,  4.9868e-02, -9.5203e-02, -6.0212e-02, -4.3796e-02,\n",
            "         -9.7685e-03,  8.4496e-02, -2.8881e-02,  9.8502e-02, -1.5556e-01,\n",
            "         -1.6190e-02, -8.7906e-02, -6.4810e-02]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[[[-0.1006,  0.0870, -0.1242],\n",
            "          [ 0.1866, -0.0918, -0.1614],\n",
            "          [ 0.0554,  0.0360, -0.0403]],\n",
            "\n",
            "         [[ 0.1506, -0.1453,  0.0865],\n",
            "          [-0.0775,  0.1634,  0.1531],\n",
            "          [-0.1470, -0.1122, -0.0764]],\n",
            "\n",
            "         [[ 0.0284,  0.2263,  0.1402],\n",
            "          [-0.0222, -0.0239, -0.0496],\n",
            "          [-0.0999, -0.0072, -0.0473]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1501,  0.0830,  0.1170],\n",
            "          [-0.0409,  0.2528,  0.0848],\n",
            "          [ 0.0737,  0.0943,  0.1785]],\n",
            "\n",
            "         [[-0.0923,  0.1472, -0.0037],\n",
            "          [ 0.0707, -0.0134, -0.0323],\n",
            "          [-0.0029,  0.0825, -0.1458]],\n",
            "\n",
            "         [[-0.0043,  0.0517,  0.0912],\n",
            "          [ 0.0776,  0.0310,  0.0370],\n",
            "          [ 0.0690, -0.0893,  0.0729]]],\n",
            "\n",
            "\n",
            "        [[[-0.0600,  0.0115,  0.0342],\n",
            "          [-0.0963,  0.1543, -0.0445],\n",
            "          [-0.1098,  0.0542, -0.0286]],\n",
            "\n",
            "         [[ 0.1117, -0.1316, -0.0656],\n",
            "          [-0.1162,  0.1665,  0.0527],\n",
            "          [-0.0811,  0.0481, -0.0059]],\n",
            "\n",
            "         [[-0.0191, -0.1321, -0.0264],\n",
            "          [-0.0225, -0.1604,  0.1591],\n",
            "          [-0.0056, -0.0229, -0.0861]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0613,  0.0448, -0.0155],\n",
            "          [ 0.0362, -0.0444,  0.0374],\n",
            "          [-0.0753, -0.1668,  0.0996]],\n",
            "\n",
            "         [[-0.0474, -0.0290,  0.1478],\n",
            "          [-0.0961,  0.0300, -0.0589],\n",
            "          [ 0.1108, -0.0174, -0.0398]],\n",
            "\n",
            "         [[ 0.1193, -0.0968, -0.0168],\n",
            "          [ 0.0110, -0.0063, -0.1566],\n",
            "          [-0.1610, -0.1470,  0.1318]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0033,  0.1961, -0.0137],\n",
            "          [-0.0061,  0.0068, -0.0566],\n",
            "          [ 0.1012, -0.0974,  0.0297]],\n",
            "\n",
            "         [[-0.0766, -0.0445, -0.0191],\n",
            "          [-0.0433, -0.1352, -0.1109],\n",
            "          [ 0.0395,  0.1749, -0.0190]],\n",
            "\n",
            "         [[-0.1066,  0.0781,  0.1383],\n",
            "          [ 0.1411,  0.0915,  0.0290],\n",
            "          [ 0.1591,  0.0881, -0.1018]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0451, -0.0263,  0.0968],\n",
            "          [ 0.1119,  0.0334, -0.0476],\n",
            "          [ 0.1040, -0.0919, -0.0420]],\n",
            "\n",
            "         [[-0.1660, -0.0533,  0.0877],\n",
            "          [ 0.0276,  0.0591, -0.0424],\n",
            "          [-0.0637,  0.1116,  0.1094]],\n",
            "\n",
            "         [[-0.0292,  0.0399,  0.0497],\n",
            "          [ 0.0153, -0.0958, -0.0240],\n",
            "          [ 0.1130,  0.1238,  0.0689]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.1201, -0.0177, -0.1913],\n",
            "          [-0.1202, -0.0174,  0.0103],\n",
            "          [-0.1078,  0.0110,  0.0750]],\n",
            "\n",
            "         [[ 0.1186,  0.0014,  0.1050],\n",
            "          [ 0.0504, -0.0962, -0.0438],\n",
            "          [ 0.0237, -0.1602,  0.0211]],\n",
            "\n",
            "         [[ 0.0541,  0.0326, -0.0131],\n",
            "          [-0.0231,  0.0086, -0.0610],\n",
            "          [-0.1436, -0.0131,  0.1210]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0324, -0.0530,  0.0834],\n",
            "          [-0.1313, -0.0749,  0.0605],\n",
            "          [-0.0518,  0.0616,  0.0546]],\n",
            "\n",
            "         [[-0.0882, -0.0118,  0.1274],\n",
            "          [-0.0181, -0.0570, -0.0797],\n",
            "          [-0.1141,  0.1343, -0.1568]],\n",
            "\n",
            "         [[ 0.0020,  0.0188,  0.1470],\n",
            "          [ 0.0223, -0.1452,  0.1009],\n",
            "          [ 0.0713,  0.0545,  0.3064]]],\n",
            "\n",
            "\n",
            "        [[[-0.0991, -0.1089, -0.0380],\n",
            "          [-0.0972, -0.0589,  0.0940],\n",
            "          [-0.0262,  0.0071,  0.0726]],\n",
            "\n",
            "         [[ 0.1989, -0.0188, -0.1112],\n",
            "          [-0.0585, -0.0094,  0.1120],\n",
            "          [-0.1124, -0.0351, -0.0068]],\n",
            "\n",
            "         [[ 0.0243, -0.1181, -0.1421],\n",
            "          [ 0.1587, -0.0766,  0.0431],\n",
            "          [ 0.0316, -0.1887, -0.0332]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0246,  0.2174,  0.0179],\n",
            "          [ 0.0483, -0.0234, -0.0011],\n",
            "          [-0.1039, -0.1177,  0.0064]],\n",
            "\n",
            "         [[ 0.1725, -0.1088, -0.1049],\n",
            "          [ 0.1076,  0.0294, -0.0916],\n",
            "          [ 0.1317, -0.0328, -0.0154]],\n",
            "\n",
            "         [[ 0.0341,  0.0632,  0.0465],\n",
            "          [ 0.1320,  0.0847,  0.0131],\n",
            "          [ 0.0153,  0.1219,  0.1080]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0457, -0.0894,  0.0311],\n",
            "          [ 0.1014, -0.0057, -0.0773],\n",
            "          [ 0.1024, -0.0836,  0.0896]],\n",
            "\n",
            "         [[-0.0294,  0.0401,  0.0009],\n",
            "          [ 0.0472, -0.0531,  0.1743],\n",
            "          [ 0.0242,  0.1035, -0.0390]],\n",
            "\n",
            "         [[ 0.2373, -0.0053,  0.0467],\n",
            "          [-0.1696, -0.0515,  0.0823],\n",
            "          [ 0.0379, -0.0948, -0.0224]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0088, -0.0874,  0.1337],\n",
            "          [-0.0533, -0.1417, -0.1944],\n",
            "          [ 0.0869,  0.0330, -0.2278]],\n",
            "\n",
            "         [[ 0.0284, -0.1562,  0.1651],\n",
            "          [ 0.0572,  0.0349, -0.0300],\n",
            "          [ 0.0147, -0.1781, -0.0860]],\n",
            "\n",
            "         [[ 0.1379,  0.0616, -0.0944],\n",
            "          [ 0.0024, -0.0241,  0.0253],\n",
            "          [-0.0188, -0.0990,  0.1556]]]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.1709, -0.1132, -0.1258,  0.0394, -0.0004, -0.0501,  0.0603, -0.0890,\n",
            "         0.0814,  0.0656, -0.0830, -0.0328, -0.1131, -0.0462, -0.0244,  0.1048,\n",
            "         0.0418, -0.0435,  0.0468,  0.0402, -0.0374,  0.0421,  0.1161, -0.0230,\n",
            "        -0.0187,  0.1099,  0.1576,  0.0957,  0.0331,  0.0708,  0.0436, -0.1325,\n",
            "        -0.2541,  0.0776,  0.0646, -0.0565, -0.0325,  0.1837, -0.0953,  0.1936,\n",
            "         0.0791, -0.0628, -0.0592,  0.1693, -0.0212, -0.0536, -0.0520,  0.0441],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 3.0304e-02,  2.9118e-03,  1.0751e-02,  7.5644e-03, -1.8848e-01,\n",
            "          1.0196e-01, -6.8463e-02,  3.6689e-03, -1.0691e-01,  6.2915e-02,\n",
            "          3.3592e-02,  3.2944e-03, -7.0095e-02,  6.6980e-02,  7.5333e-02,\n",
            "          1.4490e-01,  7.8801e-02,  1.5452e-01,  1.2693e-02,  1.6641e-01,\n",
            "          1.1632e-01,  9.5886e-02,  5.3658e-03, -1.9156e-01, -1.7923e-02,\n",
            "         -2.1751e-01, -9.6325e-03,  9.2521e-03,  4.9209e-03, -1.0099e-01,\n",
            "         -7.1668e-02,  1.1149e-01,  1.8170e-01, -3.3223e-02, -8.0576e-02,\n",
            "         -9.3900e-02, -7.2689e-02, -1.4858e-01, -9.0430e-02,  9.1038e-02,\n",
            "          2.7095e-02,  5.6517e-02,  1.4079e-01, -1.3102e-01,  1.6406e-01,\n",
            "          9.8383e-02, -2.3490e-02,  2.0435e-01],\n",
            "        [ 1.0166e-01, -4.1933e-02,  1.0434e-01, -5.9291e-02, -2.9863e-02,\n",
            "         -8.0517e-02,  1.3675e-01, -6.0932e-02,  8.5338e-02, -7.3927e-02,\n",
            "         -7.8971e-03, -2.1636e-02,  3.8541e-02,  2.5544e-02, -6.2273e-02,\n",
            "         -1.0913e-01, -4.9334e-02, -7.9423e-02, -1.0288e-01, -1.0777e-02,\n",
            "          1.0470e-01,  4.2127e-03,  6.5743e-02, -5.4776e-02,  4.7957e-02,\n",
            "          2.0724e-01,  3.9398e-02,  4.0040e-02, -1.5870e-01,  1.8502e-01,\n",
            "         -1.2568e-02,  2.0840e-01, -1.2834e-01,  1.8038e-01,  8.8422e-03,\n",
            "         -7.7405e-03,  6.4027e-02, -4.1990e-02, -5.1127e-03, -3.0449e-02,\n",
            "          4.9720e-02,  4.9633e-02,  2.5308e-02, -9.8387e-02,  3.5035e-02,\n",
            "          6.0074e-02,  1.6565e-01,  1.7917e-02],\n",
            "        [-8.9137e-03, -7.2711e-02, -2.2894e-01, -5.5245e-02,  2.2160e-01,\n",
            "         -1.6838e-01,  4.3594e-02, -2.6534e-01, -4.1228e-02,  4.1105e-02,\n",
            "         -5.0905e-02,  1.6281e-01, -4.1351e-02, -2.1010e-01, -7.9109e-02,\n",
            "          7.1205e-02,  1.8865e-02,  6.3825e-02, -1.3224e-01, -7.6874e-02,\n",
            "          8.2245e-02,  5.2595e-02, -1.5097e-01,  6.7883e-02,  7.0615e-02,\n",
            "         -7.3829e-02,  1.7626e-02,  1.3823e-01, -1.0425e-01, -2.9474e-02,\n",
            "          2.0366e-01, -7.2091e-02, -3.2825e-02,  1.1886e-01, -6.7654e-02,\n",
            "          4.8415e-02,  1.9917e-02,  2.0153e-02, -2.2627e-02,  2.1064e-02,\n",
            "         -9.4899e-02,  5.8763e-02, -1.4063e-01,  1.0419e-01, -9.4999e-02,\n",
            "          2.0559e-01, -6.3086e-02, -1.5336e-02],\n",
            "        [-5.2378e-02,  1.0423e-01, -2.0079e-01,  1.4087e-02, -3.8244e-02,\n",
            "          1.4390e-01,  5.8587e-02, -1.3478e-01,  9.0397e-02, -2.8107e-02,\n",
            "          7.1619e-02,  6.2770e-02, -8.0400e-03,  1.2006e-01,  2.4948e-02,\n",
            "          6.2235e-02,  1.1729e-02,  1.6115e-01, -1.3156e-01,  3.8489e-02,\n",
            "          6.6062e-03, -4.7262e-03,  4.4516e-02,  3.0894e-02,  7.9363e-02,\n",
            "          8.9242e-02,  1.5102e-02, -1.7523e-02, -9.3804e-02, -7.1092e-03,\n",
            "          1.4000e-01, -9.6914e-02,  5.0309e-02, -1.2269e-02,  2.1009e-01,\n",
            "         -5.6265e-02, -2.4630e-01,  4.4222e-03, -3.6669e-02, -1.5491e-02,\n",
            "          5.2435e-02,  3.3603e-02,  7.2721e-02, -9.3002e-03, -5.4442e-02,\n",
            "         -9.9285e-02, -2.9680e-02, -9.2632e-02],\n",
            "        [-7.2641e-03, -6.0896e-02, -7.0025e-02, -2.7330e-02,  1.8357e-01,\n",
            "          1.8684e-02, -6.7809e-04,  9.9163e-02,  2.1819e-02, -8.1835e-02,\n",
            "         -4.1554e-02, -1.3128e-01,  1.3240e-01, -5.3613e-02, -2.7411e-02,\n",
            "         -8.7063e-02, -3.5259e-03, -1.1493e-02, -6.4067e-02,  1.1833e-01,\n",
            "         -1.0457e-02,  5.1661e-02, -1.9633e-01,  6.6882e-02, -1.2606e-01,\n",
            "          6.1715e-02,  2.3821e-01, -5.5469e-02, -5.7141e-02, -6.9782e-02,\n",
            "         -1.3675e-04, -7.1760e-03, -4.7530e-02,  2.8778e-02,  1.6538e-01,\n",
            "          1.7685e-01, -2.0683e-02,  3.4081e-02,  4.7245e-02, -4.8962e-02,\n",
            "          8.8744e-02, -8.7724e-02, -2.6202e-02, -1.7547e-01, -1.5802e-02,\n",
            "          9.0995e-02, -9.0772e-02,  1.8841e-02]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 0.0499,  0.0192,  0.2279,  0.0865,  0.0618, -0.1291,  0.0661,  0.0989,\n",
            "          0.0929,  0.0660,  0.0205, -0.1368,  0.1953,  0.0235,  0.0496,  0.0131,\n",
            "          0.0886,  0.0094,  0.0324, -0.0110,  0.0696,  0.0625,  0.0701, -0.1681,\n",
            "         -0.0317, -0.0403,  0.0340, -0.1299,  0.0518, -0.0470,  0.0616,  0.1248,\n",
            "         -0.0292, -0.1126,  0.0368, -0.1379, -0.1198, -0.1022,  0.0164, -0.0634,\n",
            "          0.0743, -0.0232,  0.0400, -0.1291, -0.0256,  0.0041, -0.0449,  0.0715],\n",
            "        [-0.0778, -0.0068,  0.0601,  0.1044,  0.0083,  0.0264,  0.0769, -0.0075,\n",
            "         -0.0255, -0.1392, -0.2298,  0.0139,  0.1621, -0.0737, -0.0171,  0.1555,\n",
            "          0.0477, -0.0510,  0.0290, -0.0125,  0.0728, -0.1146, -0.0693,  0.0058,\n",
            "          0.1505,  0.0633,  0.0934, -0.0668, -0.0263,  0.0170, -0.1128, -0.1472,\n",
            "         -0.1296,  0.0217, -0.0354, -0.1071, -0.0510, -0.0226, -0.0047,  0.1358,\n",
            "          0.0723, -0.0113,  0.0780,  0.1284,  0.1205, -0.1387,  0.0248,  0.0481],\n",
            "        [ 0.0369, -0.0669, -0.1135, -0.0709,  0.0169, -0.1846,  0.0506,  0.2378,\n",
            "         -0.0613, -0.0677, -0.0519,  0.1105, -0.1259, -0.0086, -0.0619, -0.1186,\n",
            "          0.2021, -0.0054, -0.0869,  0.0811,  0.0566,  0.0132,  0.1339,  0.1152,\n",
            "         -0.0715,  0.0855, -0.0030,  0.0522,  0.0372,  0.1000,  0.0735, -0.1576,\n",
            "          0.1450,  0.2866, -0.2321, -0.0158,  0.0571, -0.0182, -0.0730, -0.3024,\n",
            "         -0.0805,  0.0460,  0.0292, -0.1609, -0.0742, -0.0619,  0.0850,  0.0391],\n",
            "        [-0.0813, -0.0916, -0.0522,  0.0087,  0.0182, -0.0430,  0.0868, -0.0506,\n",
            "         -0.1307, -0.0951,  0.1123, -0.0560,  0.0034, -0.0551, -0.0072,  0.1550,\n",
            "          0.0511,  0.0530,  0.1050, -0.0527,  0.0892, -0.0900, -0.0012,  0.0636,\n",
            "          0.0866, -0.1256,  0.0941, -0.0126,  0.0597, -0.0329,  0.0466,  0.2216,\n",
            "         -0.0341,  0.1254, -0.0473,  0.0629, -0.0131, -0.0877,  0.0004, -0.0577,\n",
            "          0.1567,  0.2136, -0.0009,  0.0845, -0.0671,  0.0944,  0.0893,  0.1920],\n",
            "        [ 0.0731, -0.0004,  0.0708,  0.0028, -0.0627, -0.0588,  0.2633,  0.0974,\n",
            "         -0.0754, -0.0310, -0.0890,  0.0851, -0.0563,  0.0208, -0.1579, -0.0582,\n",
            "          0.0844,  0.0029,  0.1043, -0.1068, -0.1304, -0.2555, -0.0341, -0.1484,\n",
            "          0.0281,  0.0212, -0.0054, -0.0417,  0.0057,  0.1324,  0.0227, -0.0371,\n",
            "         -0.0535,  0.0926,  0.0224,  0.0878,  0.0275,  0.0722, -0.0458,  0.0827,\n",
            "          0.0068,  0.1608,  0.1362,  0.0786, -0.0329, -0.0098,  0.0248, -0.1674]],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[[[ 1.2680e-01,  1.3223e-02,  8.8561e-02],\n",
            "          [-2.1536e-01,  6.7797e-02,  5.5777e-02],\n",
            "          [ 1.3994e-01,  2.3411e-02,  8.7029e-02]],\n",
            "\n",
            "         [[-9.2546e-03,  9.1180e-02,  8.2252e-02],\n",
            "          [ 7.4659e-02,  1.9647e-02, -6.5078e-03],\n",
            "          [-1.9257e-04,  3.6556e-02, -3.6322e-02]],\n",
            "\n",
            "         [[ 1.7653e-01, -6.0661e-03, -4.7117e-02],\n",
            "          [ 5.5460e-02, -2.1261e-02,  8.0499e-02],\n",
            "          [ 1.3030e-01,  1.9077e-02, -1.6772e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3996e-01,  2.1506e-02,  9.8002e-02],\n",
            "          [ 2.2670e-02,  6.8826e-02,  4.6957e-02],\n",
            "          [-3.6888e-02,  8.4736e-02, -4.4553e-02]],\n",
            "\n",
            "         [[-3.4345e-02,  1.0671e-01, -1.2275e-01],\n",
            "          [-3.0260e-02, -2.6032e-02,  5.8852e-02],\n",
            "          [-1.0190e-01, -1.4729e-03,  3.3139e-02]],\n",
            "\n",
            "         [[ 1.4435e-01,  9.1734e-02, -8.8600e-02],\n",
            "          [-6.3056e-02, -9.5320e-02,  7.3391e-03],\n",
            "          [-4.0904e-02, -2.8177e-02,  1.4864e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.3400e-01, -1.9235e-02, -3.7287e-02],\n",
            "          [-8.6928e-02, -8.8830e-02,  6.6225e-03],\n",
            "          [ 5.3502e-02,  2.5617e-01, -8.5332e-02]],\n",
            "\n",
            "         [[ 3.4987e-02,  3.9076e-02,  9.8279e-03],\n",
            "          [-1.4601e-02,  2.3216e-01, -2.9655e-03],\n",
            "          [ 1.3515e-01,  8.1678e-02, -3.1649e-01]],\n",
            "\n",
            "         [[-2.7513e-02,  8.2682e-02,  3.3391e-02],\n",
            "          [-1.2451e-01, -4.8196e-02, -3.8240e-02],\n",
            "          [-7.2195e-02, -2.2267e-02, -1.0298e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.1279e-02,  1.5803e-01, -2.5163e-02],\n",
            "          [-6.9948e-02,  2.0949e-02, -8.0033e-02],\n",
            "          [-9.2420e-02,  2.4858e-01,  1.8293e-03]],\n",
            "\n",
            "         [[-7.4579e-03, -2.1691e-02,  2.6142e-02],\n",
            "          [-2.5694e-02, -7.5123e-02,  8.0234e-02],\n",
            "          [ 1.1311e-01, -5.4982e-02, -4.1965e-03]],\n",
            "\n",
            "         [[ 1.6003e-01,  7.0639e-02, -1.0003e-01],\n",
            "          [-1.6501e-01, -2.5800e-02,  6.8854e-02],\n",
            "          [-8.6452e-02, -5.0412e-02,  2.1488e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 6.2219e-02, -2.6961e-02, -3.5140e-02],\n",
            "          [-1.0562e-01,  7.6773e-02,  1.1013e-01],\n",
            "          [-6.1763e-02, -1.1708e-02, -8.4743e-02]],\n",
            "\n",
            "         [[-6.2801e-02,  1.2294e-01,  1.4304e-01],\n",
            "          [-5.3206e-02,  2.4705e-01, -3.0761e-02],\n",
            "          [-4.2897e-02, -2.2483e-01, -2.2175e-01]],\n",
            "\n",
            "         [[-1.0123e-02, -1.4404e-01, -8.1671e-02],\n",
            "          [-9.5452e-02, -1.4868e-01, -5.0169e-02],\n",
            "          [ 5.6700e-02,  1.2675e-02,  4.9469e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.7277e-03,  1.0687e-01, -4.5915e-02],\n",
            "          [-1.1268e-01, -2.7491e-02,  6.7164e-02],\n",
            "          [-5.9806e-02, -6.9534e-03, -1.0253e-01]],\n",
            "\n",
            "         [[ 1.9555e-02, -1.0032e-01,  4.6653e-02],\n",
            "          [-6.0243e-02,  1.2843e-01, -7.9533e-02],\n",
            "          [-1.1608e-01,  1.4869e-01, -2.1684e-01]],\n",
            "\n",
            "         [[-5.5787e-02, -1.2425e-03, -5.0765e-02],\n",
            "          [-1.2535e-01, -4.3728e-02, -1.7199e-01],\n",
            "          [-1.8697e-01, -3.1625e-02, -1.0510e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-9.0231e-02,  2.1081e-02, -3.4888e-02],\n",
            "          [-2.7246e-02,  4.6052e-02, -3.5270e-02],\n",
            "          [-1.7107e-02, -9.0336e-02, -6.9928e-02]],\n",
            "\n",
            "         [[-1.7405e-01,  8.4046e-02,  1.5255e-01],\n",
            "          [ 3.5560e-02, -8.1549e-02,  7.8360e-02],\n",
            "          [-1.1025e-01, -2.1358e-02, -3.9513e-02]],\n",
            "\n",
            "         [[ 2.9703e-02,  2.9374e-02, -1.1331e-01],\n",
            "          [-1.0079e-01,  5.3851e-02,  5.8087e-03],\n",
            "          [-5.4410e-02, -1.5190e-01,  4.2282e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4464e-02,  1.4859e-01,  1.0200e-01],\n",
            "          [-6.1974e-02, -3.5108e-02,  7.0570e-02],\n",
            "          [ 2.2906e-02,  2.3773e-02,  6.9560e-02]],\n",
            "\n",
            "         [[ 6.3261e-03, -1.3013e-01,  3.3543e-02],\n",
            "          [ 2.0437e-03,  1.1527e-01,  5.1171e-02],\n",
            "          [-2.3678e-01,  6.9224e-03,  1.9430e-01]],\n",
            "\n",
            "         [[ 1.8835e-01, -6.7815e-02, -7.6381e-02],\n",
            "          [ 1.5742e-01, -5.9078e-02,  6.1036e-02],\n",
            "          [ 6.8553e-02, -1.9374e-01,  1.2907e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.9045e-03, -6.5343e-02,  2.6158e-02],\n",
            "          [-7.6925e-02, -2.4573e-01,  2.6494e-02],\n",
            "          [ 1.2678e-01, -3.2027e-02,  1.1993e-01]],\n",
            "\n",
            "         [[-2.3262e-02,  1.4658e-01,  1.8705e-01],\n",
            "          [ 2.5329e-01,  3.9422e-02, -3.8778e-02],\n",
            "          [ 5.5537e-02,  1.5376e-01,  7.1061e-02]],\n",
            "\n",
            "         [[ 2.3464e-02,  3.5644e-02, -6.3899e-03],\n",
            "          [ 4.1659e-02, -2.9966e-03, -2.9616e-02],\n",
            "          [-6.0168e-02,  5.5254e-02, -6.1876e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7613e-02,  1.1428e-01, -1.0500e-01],\n",
            "          [-1.7408e-01,  1.2595e-02,  4.5825e-02],\n",
            "          [-1.6033e-01,  6.5982e-02,  1.1224e-01]],\n",
            "\n",
            "         [[ 3.4305e-02,  9.2257e-02, -1.9299e-01],\n",
            "          [ 1.6536e-02,  9.2517e-02, -2.7532e-02],\n",
            "          [-8.9014e-03,  9.9377e-03, -3.3046e-02]],\n",
            "\n",
            "         [[-1.2441e-01, -6.7910e-02,  6.8019e-02],\n",
            "          [-2.8445e-02, -1.7901e-02, -2.0560e-02],\n",
            "          [-5.0690e-02,  3.5882e-04, -5.4575e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.7072e-01,  3.9110e-02, -2.6472e-01],\n",
            "          [ 7.9992e-02, -3.3620e-02, -1.2856e-01],\n",
            "          [ 5.0510e-02, -3.5743e-02,  7.2582e-02]],\n",
            "\n",
            "         [[ 2.0571e-01,  1.5257e-01, -1.8326e-01],\n",
            "          [-9.0409e-02, -8.3831e-02, -3.4504e-02],\n",
            "          [-1.0050e-01,  4.7460e-02, -2.3634e-01]],\n",
            "\n",
            "         [[-4.4743e-02,  1.1331e-01,  1.2512e-01],\n",
            "          [-4.0758e-02, -7.7277e-02,  1.5724e-01],\n",
            "          [ 1.1236e-01, -5.4127e-02, -9.7320e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0249e-01, -1.6158e-01,  3.8365e-02],\n",
            "          [-3.2596e-02, -7.2566e-02, -3.8101e-02],\n",
            "          [ 1.5203e-01,  2.4359e-01,  1.0217e-01]],\n",
            "\n",
            "         [[ 8.0655e-02, -1.4672e-01, -7.8237e-02],\n",
            "          [-1.6051e-01, -6.9505e-02,  1.7641e-02],\n",
            "          [ 9.8161e-02,  1.2194e-01,  6.2030e-02]],\n",
            "\n",
            "         [[-1.2889e-02,  8.3097e-02, -2.3348e-02],\n",
            "          [ 2.2877e-02, -9.5211e-02,  4.6850e-02],\n",
            "          [ 1.3582e-01,  5.3995e-02, -1.7156e-01]]]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0103, -0.1613,  0.0441,  0.0428, -0.1097,  0.0955,  0.1899, -0.0504,\n",
            "        -0.0268,  0.1231,  0.0871, -0.0877, -0.0885,  0.2493, -0.0884, -0.0093,\n",
            "        -0.1115,  0.0378,  0.0945, -0.0730, -0.0210, -0.0667, -0.0195, -0.1033,\n",
            "        -0.0135,  0.0339, -0.0878,  0.0352, -0.0148, -0.0306, -0.0315, -0.0562,\n",
            "        -0.0127,  0.0759,  0.1058, -0.0418, -0.0708, -0.0146,  0.0090, -0.0950,\n",
            "        -0.0158,  0.1045,  0.0741,  0.1511,  0.0021, -0.2176, -0.0549, -0.1316],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[-0.0230,  0.1934, -0.1487,  0.1554,  0.0940, -0.0473, -0.0100,  0.1185,\n",
            "          0.0692, -0.0216,  0.1110, -0.0351,  0.0941, -0.0930,  0.1598,  0.0216,\n",
            "         -0.1007, -0.0206, -0.0029,  0.0272,  0.0509,  0.0232,  0.0711,  0.0781,\n",
            "          0.0643, -0.0032,  0.0411, -0.0526,  0.1942, -0.0936, -0.0346, -0.0514,\n",
            "          0.0938,  0.1359, -0.0860,  0.0112, -0.0535, -0.1592,  0.0529,  0.2107,\n",
            "         -0.1725, -0.0736, -0.0427,  0.0352, -0.0047, -0.0302,  0.1365,  0.0142],\n",
            "        [-0.0095, -0.0506,  0.0311,  0.2120, -0.0180, -0.2091, -0.0994,  0.0803,\n",
            "          0.1389,  0.0293, -0.0095, -0.1781,  0.0533,  0.0772,  0.0229,  0.0772,\n",
            "         -0.0698,  0.0815,  0.0937,  0.0881, -0.1765, -0.0431, -0.1082,  0.0800,\n",
            "          0.0981, -0.1668, -0.0048,  0.0637,  0.0230, -0.0137,  0.1131, -0.1009,\n",
            "          0.0022,  0.1256, -0.0495, -0.0613, -0.0046,  0.0870,  0.1821,  0.0638,\n",
            "          0.0747, -0.0955, -0.0676, -0.1320,  0.0467, -0.0365,  0.1519, -0.0178],\n",
            "        [ 0.1194,  0.0712, -0.1275,  0.0322,  0.1541, -0.0707,  0.0190, -0.0398,\n",
            "         -0.0437,  0.0171,  0.0219, -0.0305, -0.1365, -0.1556, -0.0695, -0.1058,\n",
            "          0.0686, -0.0464, -0.0433,  0.0086, -0.0463, -0.0653,  0.0681, -0.0086,\n",
            "          0.0398, -0.2465, -0.0497,  0.0714, -0.1103,  0.1268, -0.0288, -0.1033,\n",
            "         -0.1633, -0.1368, -0.0193,  0.0321,  0.0926, -0.1885,  0.0284, -0.0466,\n",
            "          0.1811,  0.0373, -0.0807,  0.0810,  0.0228, -0.0754, -0.0442, -0.0030],\n",
            "        [-0.0289,  0.0376,  0.0573,  0.0468,  0.1615, -0.1145, -0.0514, -0.0035,\n",
            "          0.0206, -0.0836, -0.0854,  0.0753,  0.0147,  0.1151,  0.0310,  0.0102,\n",
            "         -0.0562,  0.0975,  0.0719, -0.0813, -0.0091,  0.1807, -0.1496, -0.0339,\n",
            "         -0.1485, -0.0299,  0.0724,  0.0117, -0.1994,  0.0670, -0.0778,  0.0791,\n",
            "          0.1511,  0.2079,  0.2107,  0.0197, -0.0614, -0.0174,  0.0718,  0.1780,\n",
            "          0.0199, -0.0190,  0.0297, -0.0576, -0.0026, -0.1476, -0.0068, -0.1317],\n",
            "        [-0.1850,  0.0040,  0.2379,  0.0064, -0.0676,  0.0371, -0.0420, -0.1056,\n",
            "         -0.1885,  0.1130, -0.1101,  0.2387,  0.0415, -0.0044, -0.0021, -0.0552,\n",
            "         -0.0786, -0.0883,  0.0148,  0.0493, -0.0024,  0.0770,  0.0765, -0.0861,\n",
            "          0.1294, -0.0303, -0.0047,  0.0380,  0.2149,  0.1409, -0.0089,  0.0030,\n",
            "         -0.0427, -0.0408,  0.1342, -0.0180, -0.2847, -0.0416,  0.0751,  0.1481,\n",
            "         -0.0319, -0.0686,  0.0125, -0.0342, -0.1214,  0.0445, -0.0087, -0.1246]],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 0.1262,  0.0304,  0.0847,  0.0487,  0.1089, -0.0482, -0.0478,  0.0658,\n",
            "          0.1168, -0.1154, -0.0926,  0.0681,  0.1328,  0.0662,  0.1324,  0.0302,\n",
            "         -0.0490,  0.0259,  0.1407, -0.2921, -0.0190,  0.0252,  0.1085, -0.0987,\n",
            "         -0.0196, -0.0569,  0.0790, -0.0184, -0.0105, -0.1064,  0.1404, -0.0163,\n",
            "         -0.2242, -0.1018, -0.1693,  0.0352, -0.0470,  0.0344, -0.2054, -0.0899,\n",
            "          0.0798,  0.0414,  0.0377,  0.0460, -0.0903, -0.1596, -0.0254,  0.0278],\n",
            "        [-0.0935,  0.1488, -0.0033, -0.1064,  0.0224,  0.2250, -0.0210, -0.2139,\n",
            "         -0.0851, -0.0771,  0.0394,  0.0455, -0.0950,  0.2309, -0.0475, -0.0529,\n",
            "         -0.0744, -0.0501,  0.0684, -0.0779,  0.1046, -0.1086,  0.0074,  0.0012,\n",
            "          0.0719, -0.0024,  0.0364, -0.1431, -0.2571,  0.0282,  0.0794, -0.0784,\n",
            "          0.1469, -0.1786,  0.0379, -0.0091, -0.1226,  0.0708, -0.0078,  0.0112,\n",
            "          0.0838,  0.0385,  0.0061,  0.0186, -0.0196, -0.1638, -0.1466, -0.0456],\n",
            "        [ 0.0482, -0.0542, -0.1046,  0.0099, -0.0838,  0.0034,  0.0908, -0.0856,\n",
            "         -0.0371, -0.1422,  0.0824,  0.0779, -0.1049, -0.2439,  0.0600, -0.1187,\n",
            "          0.1599, -0.0187,  0.1161,  0.0733,  0.0790,  0.2267,  0.1159,  0.0591,\n",
            "         -0.0093,  0.0972, -0.0803, -0.0780,  0.0450,  0.1240,  0.0071,  0.0426,\n",
            "          0.1431,  0.1197, -0.0560, -0.0727,  0.0555,  0.1792,  0.0286,  0.0575,\n",
            "         -0.0813, -0.0106,  0.1370,  0.0604, -0.2004, -0.0809, -0.0816, -0.0127],\n",
            "        [ 0.0452, -0.0552,  0.0228,  0.0192,  0.0217, -0.0533, -0.0038,  0.1293,\n",
            "          0.1312,  0.1027, -0.0551, -0.0277,  0.1583,  0.2025,  0.1811,  0.0754,\n",
            "          0.0053, -0.0893, -0.0928, -0.0271, -0.0383,  0.0673,  0.1008, -0.0430,\n",
            "          0.2122, -0.0830, -0.0978,  0.0051, -0.0652,  0.0150, -0.0735, -0.0296,\n",
            "         -0.0425, -0.0372,  0.0049,  0.0410, -0.0318, -0.1230, -0.0130,  0.0339,\n",
            "          0.0122, -0.0614,  0.0756,  0.0108,  0.0917, -0.1263,  0.0278, -0.0332],\n",
            "        [-0.0640,  0.0719,  0.0886,  0.0107,  0.0702, -0.0130, -0.0080,  0.0077,\n",
            "          0.1312,  0.0050, -0.0655,  0.0216,  0.0809,  0.0376,  0.0330, -0.0230,\n",
            "         -0.1030, -0.0174,  0.0652, -0.0086,  0.0301, -0.0492, -0.0077,  0.1346,\n",
            "         -0.1611,  0.1274,  0.0179, -0.1416,  0.0642, -0.0582, -0.1668, -0.0159,\n",
            "         -0.0767, -0.0078,  0.0050, -0.0930, -0.0966, -0.0409, -0.0085,  0.0309,\n",
            "          0.0169, -0.1657, -0.1841, -0.1152, -0.1042,  0.0482,  0.0520,  0.1099]],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[[[-0.0242, -0.2257,  0.1709],\n",
            "          [ 0.0255, -0.0715,  0.0606],\n",
            "          [ 0.0470,  0.2381,  0.0596]],\n",
            "\n",
            "         [[-0.0326,  0.0027, -0.0874],\n",
            "          [-0.0421, -0.0159,  0.1292],\n",
            "          [ 0.0261,  0.0077,  0.1721]],\n",
            "\n",
            "         [[-0.0413, -0.0386, -0.0608],\n",
            "          [ 0.1616,  0.0262, -0.0716],\n",
            "          [ 0.0927, -0.0637, -0.1346]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0551,  0.0733, -0.0680],\n",
            "          [ 0.0288,  0.0427, -0.0267],\n",
            "          [ 0.1611, -0.0827,  0.1179]],\n",
            "\n",
            "         [[-0.0792, -0.0398, -0.2411],\n",
            "          [ 0.0581, -0.0472,  0.0297],\n",
            "          [-0.0637,  0.2420, -0.0058]],\n",
            "\n",
            "         [[ 0.0190,  0.0083,  0.0284],\n",
            "          [ 0.2018,  0.0912, -0.1129],\n",
            "          [-0.0520,  0.0157,  0.0470]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0899, -0.1068,  0.0573],\n",
            "          [-0.0166, -0.0798,  0.0448],\n",
            "          [-0.0764,  0.0483, -0.0818]],\n",
            "\n",
            "         [[-0.2106, -0.0536, -0.2385],\n",
            "          [-0.1799,  0.1009, -0.0612],\n",
            "          [ 0.1859,  0.1765, -0.0351]],\n",
            "\n",
            "         [[-0.0865,  0.0531, -0.1318],\n",
            "          [-0.2120, -0.0092,  0.0480],\n",
            "          [-0.0814, -0.1429,  0.1656]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0872, -0.0110,  0.0088],\n",
            "          [-0.1979, -0.2702,  0.0499],\n",
            "          [ 0.0137,  0.1954,  0.1250]],\n",
            "\n",
            "         [[ 0.0563, -0.0141, -0.1287],\n",
            "          [-0.0805,  0.2454, -0.1343],\n",
            "          [ 0.0757,  0.0918,  0.1117]],\n",
            "\n",
            "         [[ 0.1792,  0.0258,  0.0259],\n",
            "          [-0.1285, -0.0330,  0.0388],\n",
            "          [-0.0053, -0.0622, -0.0776]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1048, -0.0888,  0.1474],\n",
            "          [ 0.0842, -0.1369, -0.1145],\n",
            "          [ 0.2591,  0.0616, -0.0213]],\n",
            "\n",
            "         [[-0.0267, -0.0866,  0.0254],\n",
            "          [ 0.0658,  0.0909, -0.1070],\n",
            "          [ 0.1216,  0.0878, -0.0590]],\n",
            "\n",
            "         [[-0.0614, -0.1093, -0.0441],\n",
            "          [-0.0016, -0.0871,  0.0834],\n",
            "          [ 0.1636,  0.1113,  0.0583]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0186,  0.1560,  0.1286],\n",
            "          [ 0.0013, -0.1768,  0.1006],\n",
            "          [ 0.0497,  0.0966,  0.0372]],\n",
            "\n",
            "         [[ 0.0242,  0.1610,  0.0089],\n",
            "          [-0.1030,  0.1083,  0.0291],\n",
            "          [-0.0889, -0.0140, -0.0529]],\n",
            "\n",
            "         [[-0.0602, -0.0337,  0.0106],\n",
            "          [ 0.0970, -0.1246,  0.1137],\n",
            "          [-0.0116, -0.1311,  0.0514]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.1545, -0.2471,  0.1099],\n",
            "          [ 0.0357, -0.0165,  0.0896],\n",
            "          [ 0.0098, -0.0448,  0.0426]],\n",
            "\n",
            "         [[-0.0229,  0.0538,  0.1261],\n",
            "          [-0.0787, -0.0236, -0.0410],\n",
            "          [-0.0355, -0.0497,  0.0109]],\n",
            "\n",
            "         [[ 0.1396, -0.0582,  0.2221],\n",
            "          [-0.0272, -0.0627,  0.1262],\n",
            "          [-0.0971,  0.1389, -0.1234]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2090,  0.0472, -0.1242],\n",
            "          [-0.0801, -0.0270, -0.1458],\n",
            "          [ 0.0372,  0.0538, -0.1855]],\n",
            "\n",
            "         [[-0.2004, -0.0080,  0.1871],\n",
            "          [-0.0338,  0.0120,  0.0299],\n",
            "          [ 0.0263, -0.0042,  0.0187]],\n",
            "\n",
            "         [[-0.0104,  0.0524, -0.0249],\n",
            "          [-0.0160,  0.0708, -0.0020],\n",
            "          [-0.0946,  0.0666,  0.0087]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0480, -0.0989,  0.0841],\n",
            "          [ 0.1298, -0.1690,  0.0712],\n",
            "          [ 0.0815,  0.1038, -0.0133]],\n",
            "\n",
            "         [[ 0.0518,  0.0823,  0.0529],\n",
            "          [ 0.0898, -0.0664, -0.0633],\n",
            "          [-0.0690,  0.0405,  0.0734]],\n",
            "\n",
            "         [[-0.0744, -0.0593, -0.0452],\n",
            "          [-0.0489,  0.1223,  0.0492],\n",
            "          [-0.0090, -0.0973, -0.0453]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0557,  0.0146, -0.0406],\n",
            "          [ 0.0042, -0.1293, -0.1614],\n",
            "          [-0.1167, -0.0103,  0.0581]],\n",
            "\n",
            "         [[-0.0360,  0.0597, -0.2047],\n",
            "          [-0.2525,  0.0336, -0.1462],\n",
            "          [-0.0090, -0.0948,  0.0082]],\n",
            "\n",
            "         [[ 0.0669,  0.0536,  0.0797],\n",
            "          [-0.1355, -0.1218,  0.1559],\n",
            "          [ 0.0802, -0.0098, -0.0026]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0335, -0.0620, -0.1035],\n",
            "          [-0.1315, -0.0528,  0.0295],\n",
            "          [ 0.1664,  0.0141,  0.0514]],\n",
            "\n",
            "         [[-0.0396,  0.2136, -0.0352],\n",
            "          [ 0.1589,  0.1012, -0.0677],\n",
            "          [-0.1809,  0.1497, -0.0775]],\n",
            "\n",
            "         [[-0.2418,  0.0557,  0.0249],\n",
            "          [ 0.1245,  0.0425,  0.1021],\n",
            "          [-0.2269,  0.0600,  0.1752]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0333, -0.2342, -0.0374],\n",
            "          [-0.0321, -0.0148,  0.2117],\n",
            "          [-0.0040,  0.1350,  0.1473]],\n",
            "\n",
            "         [[ 0.0602,  0.0091,  0.1631],\n",
            "          [-0.0264,  0.0891, -0.0168],\n",
            "          [ 0.0213,  0.2515, -0.0228]],\n",
            "\n",
            "         [[ 0.0181, -0.0010,  0.0655],\n",
            "          [ 0.1321,  0.1466, -0.0550],\n",
            "          [-0.0799, -0.2208,  0.0334]]]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.1519,  0.0952,  0.0304, -0.0302,  0.0612,  0.0971,  0.0345,  0.1858,\n",
            "         0.0767,  0.0517, -0.0313,  0.2482,  0.1297, -0.1677, -0.1075, -0.0358,\n",
            "         0.0391, -0.0399,  0.0437, -0.1177,  0.0238,  0.2004,  0.0101,  0.1281,\n",
            "         0.0360, -0.0631, -0.0045,  0.1009,  0.0633, -0.1283,  0.0505,  0.0354,\n",
            "        -0.1755,  0.1003, -0.0413,  0.1007, -0.1314,  0.0031, -0.2170, -0.1068,\n",
            "        -0.0509,  0.1158, -0.0600, -0.0394,  0.0355, -0.0090, -0.1831,  0.1068],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[-1.8289e-01, -1.1757e-01, -2.8821e-02, -1.1347e-01,  7.1535e-02,\n",
            "         -7.6396e-02,  1.5928e-02, -1.3671e-01,  2.1809e-01, -5.5418e-02,\n",
            "         -3.4265e-03,  3.1585e-02,  2.1929e-02, -3.3835e-02,  1.7379e-01,\n",
            "         -9.2798e-02,  3.8340e-03,  1.4497e-01, -1.7992e-02, -4.6275e-02,\n",
            "          1.1872e-01,  1.3659e-01, -1.0015e-01, -1.4851e-01, -7.1347e-05,\n",
            "         -4.0334e-02, -5.7949e-02,  9.6463e-02,  1.0854e-01,  1.1844e-03,\n",
            "          1.0409e-01,  5.2626e-02,  4.9521e-03, -2.4454e-02, -1.7247e-01,\n",
            "         -5.4988e-02,  1.5178e-01, -9.5530e-02,  2.4496e-02, -4.3658e-02,\n",
            "         -4.8837e-02,  1.2343e-01,  1.5165e-01,  1.0391e-01, -1.6183e-01,\n",
            "         -4.0409e-02,  1.3479e-01,  7.6922e-02],\n",
            "        [-3.3264e-02,  2.7761e-01,  2.5876e-02, -3.0062e-02,  9.1801e-02,\n",
            "          8.4297e-02,  1.5179e-01, -2.3259e-01, -9.0814e-02,  1.4575e-01,\n",
            "          1.3724e-01,  2.1586e-02, -2.8061e-02, -2.3110e-01, -1.6136e-01,\n",
            "          1.3156e-02,  1.0638e-01,  3.2172e-02, -7.5044e-02,  5.7596e-02,\n",
            "         -6.9835e-02, -5.5666e-02, -1.4711e-01, -4.4263e-02, -1.3608e-01,\n",
            "          4.8302e-03, -5.1721e-02,  1.3066e-01, -3.1975e-02, -5.1868e-02,\n",
            "          1.1730e-01, -3.9104e-02, -1.4911e-01,  1.8226e-01, -1.0200e-02,\n",
            "          3.4755e-02, -2.6562e-02,  1.9639e-01, -7.0164e-02, -3.1651e-02,\n",
            "         -7.4535e-02,  3.3865e-02,  2.0571e-01,  2.1031e-02, -1.1780e-01,\n",
            "         -2.1755e-02, -2.1303e-02,  6.8994e-03],\n",
            "        [-1.0004e-01, -3.5988e-02,  8.0998e-03, -6.1619e-02, -5.2005e-02,\n",
            "         -1.8688e-01, -1.0261e-01, -9.8714e-02,  1.4333e-01, -1.3064e-01,\n",
            "         -8.4756e-02,  1.9791e-02,  1.9341e-01, -9.5464e-02, -1.5690e-01,\n",
            "          6.7803e-02,  4.0272e-02, -3.7256e-02,  6.2756e-02,  2.9095e-02,\n",
            "         -1.5062e-01,  7.5789e-02,  3.8339e-02,  3.4289e-02, -2.0524e-02,\n",
            "         -1.2894e-02,  1.0990e-01, -1.0067e-01, -5.8213e-02, -1.5134e-01,\n",
            "          2.6896e-01,  8.6820e-02,  6.3068e-03, -5.5071e-03,  1.2848e-02,\n",
            "          2.5261e-01,  6.8591e-02,  4.4944e-02, -8.1759e-02,  1.3690e-01,\n",
            "         -8.0418e-02, -5.5601e-02,  9.3056e-02,  1.4095e-01,  5.1051e-02,\n",
            "          4.7255e-02, -4.5880e-02,  4.8299e-02],\n",
            "        [-1.1498e-01, -1.2539e-01, -4.4617e-02,  8.5462e-02, -9.8930e-02,\n",
            "          8.4265e-02, -3.3034e-02, -1.1637e-01,  5.4928e-02, -5.6728e-02,\n",
            "          1.3867e-01, -3.8002e-02,  1.5309e-01,  4.9732e-03,  5.9652e-02,\n",
            "          3.3548e-02, -5.6458e-02, -1.1968e-01,  1.4255e-01, -2.0665e-01,\n",
            "         -8.7554e-02, -1.1687e-01,  4.1810e-03,  6.9031e-02,  2.1860e-01,\n",
            "         -1.5095e-02,  1.3047e-01,  1.2265e-01, -3.1115e-02, -3.3150e-02,\n",
            "         -7.1685e-02, -2.9065e-01, -6.3398e-02, -3.4241e-02,  1.5107e-01,\n",
            "          2.0254e-01, -2.1564e-02,  6.5036e-02, -4.1794e-02, -1.0006e-01,\n",
            "          6.6119e-03, -5.0589e-03,  1.2581e-03, -2.0101e-02,  8.2708e-03,\n",
            "          4.0845e-02, -6.4684e-02,  1.5079e-01],\n",
            "        [ 1.7206e-02,  7.6657e-02, -8.9731e-02,  7.0077e-02,  8.2574e-02,\n",
            "          1.2962e-01, -1.1082e-01, -1.7983e-03, -1.7235e-01,  1.3935e-02,\n",
            "          6.5814e-02,  8.5591e-03, -1.5236e-01,  6.2225e-02, -9.5952e-02,\n",
            "          5.0883e-02,  9.1026e-02,  1.8867e-02,  1.3722e-02,  1.4190e-02,\n",
            "          1.8044e-01,  6.0828e-02, -9.5591e-02,  4.9754e-02,  2.2064e-02,\n",
            "          2.2129e-01, -5.0121e-02, -1.2786e-01, -3.9585e-02, -1.4237e-01,\n",
            "          2.7603e-02, -5.7831e-02,  1.1215e-02,  2.2448e-01,  2.0200e-01,\n",
            "         -5.6999e-02, -2.4561e-01,  1.0968e-01, -1.8966e-01,  1.3993e-01,\n",
            "         -3.9659e-02, -3.8804e-02, -5.9602e-03, -4.6744e-02,  7.2734e-02,\n",
            "         -2.7972e-02, -1.5708e-01, -1.0566e-01]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[ 3.5186e-02,  8.7565e-02,  4.0972e-02, -1.2078e-01,  8.1530e-02,\n",
            "         -1.1147e-01, -7.8868e-02,  1.8730e-01,  4.4809e-03,  3.7696e-02,\n",
            "         -1.7821e-04,  1.1646e-01,  1.4685e-02, -7.5692e-02,  1.3232e-01,\n",
            "          4.8797e-02, -1.5775e-03, -1.3244e-01, -8.5958e-02, -3.4117e-02,\n",
            "         -2.0682e-02, -8.2059e-02, -8.7827e-02, -3.8934e-02,  5.0023e-02,\n",
            "         -2.8453e-02,  1.9044e-01, -1.4975e-01, -2.5768e-02,  1.1077e-02,\n",
            "         -2.9599e-02, -2.0017e-01,  2.1633e-01,  3.6372e-02,  5.1049e-02,\n",
            "          9.2830e-02,  1.3102e-02,  2.0601e-02, -9.2798e-03, -8.0380e-03,\n",
            "         -1.0351e-01,  1.2514e-01, -7.8872e-02, -8.9135e-02,  4.1732e-02,\n",
            "         -1.8188e-02,  1.3560e-01, -7.8524e-02],\n",
            "        [ 2.7718e-02, -1.8232e-01,  1.1389e-02, -5.8458e-02,  2.4482e-01,\n",
            "         -5.6781e-02,  1.2235e-01,  3.1741e-02, -6.2436e-02,  2.8957e-02,\n",
            "         -9.3340e-02, -6.6068e-02,  1.1777e-01,  1.9890e-01, -9.6648e-02,\n",
            "         -7.0001e-02, -1.1238e-01,  2.5250e-02, -7.8101e-02, -6.0590e-02,\n",
            "         -4.1437e-02, -7.8046e-02,  3.5839e-02,  4.4729e-02,  7.3859e-02,\n",
            "          2.0045e-02, -1.4146e-02,  1.1776e-01, -6.9913e-02,  2.2856e-02,\n",
            "         -2.4032e-01, -1.7866e-01, -2.9729e-02, -3.4620e-02,  1.2974e-01,\n",
            "          8.5024e-02,  9.5151e-02,  4.4720e-02, -9.8570e-02,  1.2293e-01,\n",
            "          1.1014e-01,  8.4720e-02,  7.5298e-02,  1.6731e-01,  1.5703e-02,\n",
            "          9.0639e-02, -4.3778e-02, -8.8247e-02],\n",
            "        [-1.1349e-01,  6.4485e-03, -1.6674e-01,  7.8661e-02, -1.3612e-01,\n",
            "         -2.2968e-01,  5.0833e-02,  1.6305e-02,  1.4809e-02, -4.8455e-02,\n",
            "         -9.2067e-02,  6.8845e-02,  1.6539e-01, -1.0523e-01,  2.5614e-02,\n",
            "          1.0537e-01, -3.8143e-02, -3.7079e-02,  9.1133e-03,  8.0542e-02,\n",
            "          3.4814e-02,  1.1315e-01,  1.3464e-01,  1.7223e-01,  8.1463e-02,\n",
            "          6.3524e-02, -4.6893e-02,  7.3272e-03, -6.0719e-02, -1.1833e-02,\n",
            "          4.3744e-02, -2.5026e-02,  1.1885e-01, -1.0457e-02, -9.2189e-02,\n",
            "          8.4430e-02,  1.0153e-01,  3.4198e-04,  2.6214e-01,  3.2020e-02,\n",
            "         -1.7445e-01,  5.9401e-03, -1.8095e-01,  3.5708e-03, -1.7746e-02,\n",
            "          1.6736e-01, -3.6650e-02, -1.8715e-02],\n",
            "        [ 1.9519e-02, -8.0364e-02,  4.7462e-02,  7.4594e-02, -9.7124e-02,\n",
            "         -1.2303e-01,  1.0680e-01, -1.0114e-01, -9.2228e-02,  9.3050e-02,\n",
            "          1.3879e-01, -3.0073e-02,  1.3571e-01, -6.5730e-02, -5.8325e-03,\n",
            "         -1.6186e-01, -3.3859e-03,  1.1733e-01,  1.2742e-01,  6.5908e-02,\n",
            "         -1.3571e-01,  1.5421e-01, -1.1466e-01, -5.3993e-03, -1.1823e-02,\n",
            "         -7.3196e-03, -1.8867e-02, -1.0218e-02, -1.0402e-01,  4.2626e-02,\n",
            "          2.3093e-02, -8.7665e-02, -1.5238e-01,  6.3216e-02,  4.3134e-02,\n",
            "          1.3836e-01, -2.7406e-02,  4.6250e-02,  6.4330e-02,  2.8686e-02,\n",
            "          1.0406e-01,  1.0335e-01,  6.1912e-02,  7.5206e-02,  8.9574e-02,\n",
            "         -1.0460e-01, -8.3580e-02, -1.1506e-01],\n",
            "        [ 1.4840e-01, -5.6033e-02, -1.8245e-02, -6.6504e-02,  4.4681e-02,\n",
            "         -1.8810e-01, -1.0010e-01, -3.8315e-02,  8.2917e-02, -6.6827e-02,\n",
            "         -1.6921e-01, -4.6523e-02, -2.1939e-01,  1.1118e-01, -1.8796e-02,\n",
            "          4.7694e-02, -9.3673e-03, -5.1850e-02, -2.1784e-02, -1.1893e-02,\n",
            "          2.0560e-03, -1.2971e-01,  6.2687e-02, -2.1028e-02,  8.8278e-02,\n",
            "         -7.1923e-02, -6.5180e-02, -6.2804e-03, -1.1069e-01, -4.6959e-02,\n",
            "         -5.4230e-02, -1.1876e-01,  8.4475e-02,  2.3164e-01,  6.9238e-02,\n",
            "          1.3724e-01,  4.3562e-02,  9.5832e-02, -5.2865e-02,  3.1461e-02,\n",
            "          2.1073e-01,  2.0913e-01, -8.3036e-02, -7.3248e-02, -3.8687e-02,\n",
            "          5.5923e-02, -1.4985e-01,  4.5063e-02]], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([[-0.0118, -0.0500,  0.0767,  ...,  0.0083, -0.1180, -0.0575],\n",
            "        [-0.0908,  0.1931, -0.0199,  ..., -0.0316, -0.0677, -0.1200],\n",
            "        [-0.1246,  0.0337,  0.0385,  ...,  0.0417, -0.0683,  0.1537],\n",
            "        [ 0.0166, -0.0264,  0.1385,  ..., -0.1854,  0.1430,  0.0177],\n",
            "        [ 0.0497, -0.2701,  0.1881,  ...,  0.0239,  0.0353, -0.0015]],\n",
            "       device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0116,  0.2291, -0.0212,  0.0672,  0.0589], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0166,  0.1837,  0.0968, -0.1098, -0.0192,  0.0113], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.1042,  0.1051,  0.0988,  0.1091,  0.0661, -0.0450], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0845,  0.1815, -0.2016, -0.0175,  0.0492, -0.0350], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0763, -0.0453, -0.0376,  0.1434, -0.0363,  0.0749], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0259, -0.1164,  0.1801,  0.1823, -0.0011, -0.0633], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0139, -0.1339, -0.0636, -0.0773, -0.0967,  0.0552], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0305,  0.0452,  0.1046, -0.0354,  0.1372, -0.0113], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0532, -0.1741, -0.0399, -0.1537, -0.0664, -0.0689], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([ 0.0194,  0.0761, -0.0077, -0.1037,  0.0740,  0.0555], device='cuda:0') amount of noise :)\n",
            "the grinch added tensor([-0.0871, -0.1730,  0.0622, -0.0069, -0.1575,  0.0576], device='cuda:0') amount of noise :)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   MetaConv2dLayer-1           [-1, 48, 84, 84]           1,344\n",
            "MetaConvNormLayerReLU-2           [-1, 48, 84, 84]               0\n",
            "   MetaConv2dLayer-3           [-1, 48, 42, 42]          20,784\n",
            "MetaConvNormLayerReLU-4           [-1, 48, 42, 42]               0\n",
            "   MetaConv2dLayer-5           [-1, 48, 21, 21]          20,784\n",
            "MetaConvNormLayerReLU-6           [-1, 48, 21, 21]               0\n",
            "   MetaConv2dLayer-7           [-1, 48, 10, 10]          20,784\n",
            "MetaConvNormLayerReLU-8           [-1, 48, 10, 10]               0\n",
            "   MetaLinearLayer-9                    [-1, 5]               5\n",
            "VGGReLUNormNetwork-10                    [-1, 5]               0\n",
            "================================================================\n",
            "Total params: 63,701\n",
            "Trainable params: 63,696\n",
            "Non-trainable params: 5\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.08\n",
            "Forward/backward pass size (MB): 6.86\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 7.18\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZIfE7aOQ2sj"
      },
      "outputs": [],
      "source": [
        "with open('summary_stats_with_space.txt', 'a+') as new_file, open('summary_stats.txt', 'r') as old_file:\n",
        "    for line in old_file:\n",
        "        new_file.write(line.replace('train_loss_mean', '\\ntrain_loss_mean'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99ec4fecb0c94419ae509eb00fad2299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d70c039d2d784bbfaf83dbfb1b74ddb8",
              "IPY_MODEL_f2d70077865d4663b09554b7c5d10e2f",
              "IPY_MODEL_f97fa8112be4450ea8c8f16d799d31ba"
            ],
            "layout": "IPY_MODEL_f230aa795f9e4352a25adb655e13ffee"
          }
        },
        "d70c039d2d784bbfaf83dbfb1b74ddb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b56c614297ec402fb134e3cae567f104",
            "placeholder": "​",
            "style": "IPY_MODEL_20743def1ecb4fa7adb91097dc81fe95",
            "value": "Creating class directories: 100%"
          }
        },
        "f2d70077865d4663b09554b7c5d10e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7011fa5ca6ee4393a4a21d96041f59ae",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2469637a18254489ad46e1dfb40ada7a",
            "value": 64
          }
        },
        "f97fa8112be4450ea8c8f16d799d31ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e239262bed874a7b8dd9983d44b58c7e",
            "placeholder": "​",
            "style": "IPY_MODEL_c71ed2c49bdc4fef94596072b985db40",
            "value": " 64/64 [00:06&lt;00:00,  9.15it/s]"
          }
        },
        "f230aa795f9e4352a25adb655e13ffee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56c614297ec402fb134e3cae567f104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20743def1ecb4fa7adb91097dc81fe95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7011fa5ca6ee4393a4a21d96041f59ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2469637a18254489ad46e1dfb40ada7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e239262bed874a7b8dd9983d44b58c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c71ed2c49bdc4fef94596072b985db40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0eeafc134ec84401a853739dc9c702cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_042f855804474183929114e46a46a6ad",
              "IPY_MODEL_b434f06988ad446996819fa0e1a1df4a",
              "IPY_MODEL_4ab09274d427411ba970caf24c328ca0"
            ],
            "layout": "IPY_MODEL_1bef065127834e2a868b68b9385af84b"
          }
        },
        "042f855804474183929114e46a46a6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78c3ba99e4b944f38c8c45603c447e3d",
            "placeholder": "​",
            "style": "IPY_MODEL_9f2a4aa9cf634589971052659ffee2a3",
            "value": "Creating class directories: 100%"
          }
        },
        "b434f06988ad446996819fa0e1a1df4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b5ae37cabb940cdbcd687869e7df81a",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b24a022fd7064b1dae0dd71cd54e12a9",
            "value": 16
          }
        },
        "4ab09274d427411ba970caf24c328ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_498f7feb070047ebad81c9d0774fe668",
            "placeholder": "​",
            "style": "IPY_MODEL_c5b23f77ec384d3387689525e2004f39",
            "value": " 16/16 [00:01&lt;00:00, 10.19it/s]"
          }
        },
        "1bef065127834e2a868b68b9385af84b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c3ba99e4b944f38c8c45603c447e3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f2a4aa9cf634589971052659ffee2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b5ae37cabb940cdbcd687869e7df81a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b24a022fd7064b1dae0dd71cd54e12a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "498f7feb070047ebad81c9d0774fe668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5b23f77ec384d3387689525e2004f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0baef5ee89cc4d8ebc956b475b11d54d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_541a47f485e440aa9d20944e267edbf1",
              "IPY_MODEL_f5864cbe97404b15be441e55b573471a",
              "IPY_MODEL_8d19c147b6c44113b6f968bd79278a4f"
            ],
            "layout": "IPY_MODEL_ca67930c379b41c39270722302b436ee"
          }
        },
        "541a47f485e440aa9d20944e267edbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06839d3b5c8a4f7fb52a419e6cdb47cb",
            "placeholder": "​",
            "style": "IPY_MODEL_be994381dae4483c97d55280e30e2e79",
            "value": "Creating class directories: 100%"
          }
        },
        "f5864cbe97404b15be441e55b573471a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ccef753661742a887096f56905e253f",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32d9ce4ca8d04de497781b11fd7fd01f",
            "value": 20
          }
        },
        "8d19c147b6c44113b6f968bd79278a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_989610b072fc4d55a60fcc2b0946b08b",
            "placeholder": "​",
            "style": "IPY_MODEL_3b2f7b7c7d3e4c539c1d87577a27061d",
            "value": " 20/20 [00:01&lt;00:00, 10.60it/s]"
          }
        },
        "ca67930c379b41c39270722302b436ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06839d3b5c8a4f7fb52a419e6cdb47cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be994381dae4483c97d55280e30e2e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ccef753661742a887096f56905e253f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32d9ce4ca8d04de497781b11fd7fd01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "989610b072fc4d55a60fcc2b0946b08b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2f7b7c7d3e4c539c1d87577a27061d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}